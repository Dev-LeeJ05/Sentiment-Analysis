{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c84beab-fdd3-4b92-b4ff-53216eb51626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from CustomDataCollatorForSequenceClassification import CustomDataCollatorForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from WordPieceTokenizer import WordPieceTokenizer as Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from CustomBertSequenceClassification import CustomBertSequenceClassification\n",
    "from CustomBert import CustomBertConfig\n",
    "import CustomBert\n",
    "from collections import Counter\n",
    "import os\n",
    "from Model import LSTM\n",
    "from Model import Transformer, PositionalEncoding\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "dataFilePath = 'datasets/'\n",
    "saveFilePath = 'saves/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = Tokenizer(f'{saveFilePath}vocab.txt',do_lower_case=False,strip_accents=False,clean_text=True)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da38b69e-5ccd-4751-a66e-934077285c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>str_len</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>token_type_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 2346 2340 2003 10573 1077 2712 30987 1 3 0 0...</td>\n",
       "      <td>불안</td>\n",
       "      <td>24</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 2085 163 2423 16734 2153 1 3 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>불안</td>\n",
       "      <td>12</td>\n",
       "      <td>1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 2105 4407 1265 18055 1088 5069 1365 1 3 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>14</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 10073 19740 2668 17073 2565 3 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>13</td>\n",
       "      <td>1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 2233 2870 10316 1076 1078 2580 3 0 0 0 0 0 0...</td>\n",
       "      <td>불안</td>\n",
       "      <td>11</td>\n",
       "      <td>1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  발화  감정  str_len  \\\n",
       "0  2 2346 2340 2003 10573 1077 2712 30987 1 3 0 0...  불안       24   \n",
       "1  2 2085 163 2423 16734 2153 1 3 0 0 0 0 0 0 0 0...  불안       12   \n",
       "2  2 2105 4407 1265 18055 1088 5069 1365 1 3 0 0 ...  불안       14   \n",
       "3  2 10073 19740 2668 17073 2565 3 0 0 0 0 0 0 0 ...  불안       13   \n",
       "4  2 2233 2870 10316 1076 1078 2580 3 0 0 0 0 0 0...  불안       11   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "1  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "2  1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "3  1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "4  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "\n",
       "                                      token_type_ids  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{dataFilePath}sentiment_train.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b88c422-88f4-4e80-9f82-53fdcb86b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['감정'] == '불안'),'감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'),'감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'),'감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'),'감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'),'감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'),'감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'),'감정'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3590a8e3-5472-4959-ac1b-e4cd59645e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_classification_dataset(data_frame, tokenizer):\n",
    "    processed_tokens = []\n",
    "    processed_attentions = []\n",
    "    processed_token_type_ids = []\n",
    "\n",
    "    for i in tqdm(range(len(data_frame)), desc=\"데이터 파싱 중\"):\n",
    "        token_str = data_frame.iloc[i, 0]\n",
    "        attention_str = data_frame.iloc[i, 3]\n",
    "        token_type_ids_str = data_frame.iloc[i, 4]\n",
    "\n",
    "        processed_tokens.append([int(t) for t in token_str.split(\" \")])\n",
    "        processed_attentions.append([int(a) for a in attention_str.split(\" \")])\n",
    "        processed_token_type_ids.append([int(t) for t in token_type_ids_str.split(\" \")])\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": processed_tokens,\n",
    "        \"attention_mask\": processed_attentions,\n",
    "        \"token_type_ids\": processed_token_type_ids,\n",
    "        \"labels\": data_frame[\"감정\"].values.tolist()\n",
    "    }\n",
    "        \n",
    "    return dataset_dict\n",
    "def tensor_dataset(dataset_dict):\n",
    "    input_ids = torch.tensor(dataset_dict[\"input_ids\"],dtype=torch.long)\n",
    "    attention_mask = torch.tensor(dataset_dict[\"attention_mask\"],dtype=torch.long)\n",
    "    labels = torch.tensor(dataset_dict[\"labels\"],dtype=torch.long)\n",
    "    tensorDataset = TensorDataset(input_ids,attention_mask,labels)\n",
    "    return tensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5776764d-05d3-46df-909d-2bc90d0f23ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 세트의 크기: 110931 행\n",
      "검증 세트의 크기: 27733 행\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057cc3385b5f44319e6b50fedf4879df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/110931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0148c448ca4760b16b7c1af19d2d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/27733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27733\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df,train_size=0.8,test_size=0.2)\n",
    "\n",
    "print(f\"학습 세트의 크기: {len(train_df)} 행\")\n",
    "print(f\"검증 세트의 크기: {len(val_df)} 행\")\n",
    "\n",
    "train_datasets_dict = prepare_classification_dataset(train_df,tokenizer)\n",
    "train_datasets = tensor_dataset(train_datasets_dict)\n",
    "print(len(train_datasets))\n",
    "val_datasets_dict = prepare_classification_dataset(val_df,tokenizer)\n",
    "val_datasets = tensor_dataset(val_datasets_dict)\n",
    "print(len(val_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d33c59-b51e-43c7-a0a1-c59828780848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "434\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "print(len(train_loader))\n",
    "val_loader = DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1f979-cd2b-403d-adfa-28d9d39a6b92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404c17eb-9dbf-4369-81f5-218c8a2be20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataframe(data_frame, device,batch_size,shuffle=False):\n",
    "#     tensor_x_list = []\n",
    "#     attentions = []\n",
    "#     token_type_ids_ = []\n",
    "#     for i in tqdm(range(len(data_frame))):\n",
    "#         token = data_frame.iloc[i,0]\n",
    "#         token = token.split(\" \")\n",
    "#         token_list = []\n",
    "#         for t in token:\n",
    "#             token_list.append(int(t))\n",
    "#         tensor_x_list.append(token_list)\n",
    "        \n",
    "#         attention = data_frame.iloc[i,3]\n",
    "#         attention = attention.split(\" \")\n",
    "#         attention_list = []\n",
    "#         for a in attention:\n",
    "#             attention_list.append(int(a))\n",
    "#         attentions.append(attention_list)\n",
    "\n",
    "#         token_type_ids = data_frame.iloc[i,4]\n",
    "#         token_type_ids = token_type_ids.split(\" \")\n",
    "#         token_type_ids_list = []\n",
    "#         for t in token_type_ids:\n",
    "#             token_type_ids_list.append(int(t))\n",
    "#         token_type_ids_.append(attention_list)\n",
    "        \n",
    "#     tensor_x = torch.tensor(tensor_x_list, dtype=torch.long, device=device)\n",
    "#     tensor_attention = torch.tensor(attentions, dtype=torch.long, device=device)\n",
    "#     tensor_token_type_ids = torch.tensor(token_type_ids_, dtype=torch.long, device=device)\n",
    "#     tensor_t = torch.tensor(data_frame[\"감정\"].values.tolist(), dtype=torch.long, device=device)\n",
    "\n",
    "#     dataset = TensorDataset(tensor_x,tensor_attention,tensor_t,tensor_token_type_ids)\n",
    "#     loader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True)\n",
    "#     return loader\n",
    "    \n",
    "#     dataset = {\"input_ids\" : tensor_x, \"attention_mask\":tensor_attention,\"token_type_ids\":tensor_token_type_ids,\"labels\":tensor_t}\n",
    "    \n",
    "    \n",
    "\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d065c33-266a-43af-924c-c76f35f47549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Train(epoch,device,train_loader,val_loader,NN,loss_function,optimizer):\n",
    "    acc = 0\n",
    "    prev_acc = 0\n",
    "    cnt = 0\n",
    "    for e in range(epoch):\n",
    "        NN.to(device)\n",
    "        loss_sum = 0\n",
    "        NN.train()\n",
    "        for x, attention,t in train_loader:\n",
    "            y = NN(x,attention)\n",
    "            loss = loss_function(y,t)\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_sum /= len(train_loader)\n",
    "    \n",
    "        NN.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, attention, t in val_loader:\n",
    "                x = x.to(device)\n",
    "                attention = attention.to(device)\n",
    "                t = t.to(device)\n",
    "    \n",
    "                y = NN(x, attention)\n",
    "                correct += (y.argmax(dim=-1) == t).sum().item()\n",
    "                total += len(x)\n",
    "        acc = correct / total\n",
    "    \n",
    "        if acc <= prev_acc:\n",
    "            cnt += 1\n",
    "        else :\n",
    "            torch.save(NN.state_dict(), \"Sentiment.pt\")\n",
    "            cnt = 0\n",
    "            prev_acc = acc\n",
    "        \n",
    "        print(f\"epoch  {e+1}\\t\\tloss {loss_sum:.12f}\\tacc {acc:.4f}\\tcnt {cnt}\")\n",
    "        \n",
    "        if cnt >= 5:\n",
    "            print(\"train halted\")\n",
    "            break\n",
    "            \n",
    "    print(\"---------- 학습 종료 ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a68f657-9e9f-47d8-b477-2cb4ec4601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN = LSTM(vocab_size=vocab_size,embedding_dim=embedding_dim,hidden_dim=64,output_dim=7,n_layers=4,bidirectional=True,dropout_p=0.1)\n",
    "# NN.to(device)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(NN.parameters(),lr=0.001)\n",
    "# epoch = 500\n",
    "# LSTM_Train(epoch,device,train_loader,val_loader,NN,loss_function,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3a0ef-22f3-4ed6-8b5d-4676131d5f34",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b599944c-bab7-4d2e-986e-bf50e02b409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_last_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.finished and self.after_scheduler:\n",
    "            return self.after_scheduler.step(epoch)\n",
    "        else:\n",
    "            return super(GradualWarmupScheduler, self).step(epoch)\n",
    "\n",
    "def Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler,\n",
    "                      warmup_epochs=5, f1_average_mode='weighted', log_dir=\"runs/sentiment_experiment\", save_path=\"saves/models/Sentiment.pt\",multiplier = 1.0):\n",
    "    \n",
    "    writer = SummaryWriter(log_dir) # TensorBoard writer 초기화\n",
    "\n",
    "    combined_scheduler = GradualWarmupScheduler(optimizer, multiplier=multiplier, total_epoch=warmup_epochs, after_scheduler=scheduler)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        NN.to(device)\n",
    "        \n",
    "        train_loss_sum = 0\n",
    "        NN.train()\n",
    "        for x, attention, t in tqdm(train_loader, desc=f\"Epoch {e+1} Training\"):\n",
    "            x = x.to(device)\n",
    "            attention = attention.to(device)\n",
    "            t = t.to(device)\n",
    "\n",
    "            y = NN(x, attention)\n",
    "            loss = loss_function(y, t)\n",
    "            train_loss_sum += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(NN.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        train_loss_sum /= len(train_loader)\n",
    "        \n",
    "        NN.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_all_preds = []\n",
    "        val_all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for x, attention, t in tqdm(val_loader, desc=f\"Epoch {e+1} Validation\", leave=False):\n",
    "                x = x.to(device)\n",
    "                attention = attention.to(device)\n",
    "                t = t.to(device)\n",
    "                \n",
    "                y = NN(x, attention)\n",
    "                \n",
    "                preds = y.argmax(dim=-1)\n",
    "                val_correct += (preds == t).sum().item()\n",
    "                val_total += len(x)\n",
    "\n",
    "                val_all_preds.extend(preds.cpu().numpy())\n",
    "                val_all_targets.extend(t.cpu().numpy())\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        if len(np.unique(val_all_targets)) > 1:\n",
    "            val_f1 = f1_score(val_all_targets, val_all_preds, average=f1_average_mode)\n",
    "        else:\n",
    "            val_f1 = 1.0 if (len(val_all_targets) > 0 and np.all(np.array(val_all_preds) == np.array(val_all_targets))) else 0.0\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(NN.state_dict(), save_path)\n",
    "            print(f\"모델 저장 완료 (Best F1: {best_f1:.4f}).\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # 스케줄러 스텝은 WarmupScheduler를 통해 호출\n",
    "        combined_scheduler.step(val_f1) \n",
    "\n",
    "        # TensorBoard에 로깅\n",
    "        writer.add_scalar('Loss/train', train_loss_sum, e)\n",
    "        writer.add_scalar('Metrics/val_accuracy', val_acc, e)\n",
    "        writer.add_scalar('Metrics/val_f1_score', val_f1, e)\n",
    "        writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], e) # 현재 학습률 로깅\n",
    "        \n",
    "        print(f\"Epoch {e+1}\\tTrain Loss: {train_loss_sum:.6f}\\tVal Acc: {val_acc:.4f}\\tVal F1 ({f1_average_mode}): {val_f1:.4f}\\tNo Improve Epochs: {epochs_no_improve}\")\n",
    "        \n",
    "        if epochs_no_improve >= 10:\n",
    "            print(\"조기 종료: 검증 F1 점수 개선 없음.\")\n",
    "            break\n",
    "            \n",
    "    writer.close() # 학습 종료 후 TensorBoard writer 닫기\n",
    "    print(\"---------- 학습 종료 ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1875b120-9a16-4755-ac20-2d2ffa247ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수: [11703.0, 10593.0, 14507.0, 19764.0, 38841.0, 11043.0, 4480.0]\n",
      "계산된 클래스 가중치: [9.478851318359375, 10.4721040725708, 7.646722316741943, 5.612781047821045, 2.8560283184051514, 10.045368194580078, 24.761383056640625]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb637818010c47bcb796cd9fc6c31414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110e184c1738439ea23859d0f0b473b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.3543).\n",
      "Epoch 1\tTrain Loss: 1.797641\tVal Acc: 0.3753\tVal F1 (weighted): 0.3543\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f9be439e847f5871ea1116d0beb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6b2578216042648578cf5e2bfc7d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4092).\n",
      "Epoch 2\tTrain Loss: 1.685890\tVal Acc: 0.4070\tVal F1 (weighted): 0.4092\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3f6bc058f84ad7a78ba4f3cdebaf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e1afb1ec541e885af74315e8b859f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4347).\n",
      "Epoch 3\tTrain Loss: 1.588381\tVal Acc: 0.4293\tVal F1 (weighted): 0.4347\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34698be83b744aca8cb01990044bccca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16a26049773410986a9da968647853b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4483).\n",
      "Epoch 4\tTrain Loss: 1.514411\tVal Acc: 0.4392\tVal F1 (weighted): 0.4483\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ad57a9fc64e989d339d378bf7d4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e372e99dc3943c58f9ba7e88c885a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4729).\n",
      "Epoch 5\tTrain Loss: 1.467205\tVal Acc: 0.4698\tVal F1 (weighted): 0.4729\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d5dde91f8d4a6ba3c6e841a05647a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7ca47bb7b545e2b0a7a8088dc681d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4760).\n",
      "Epoch 6\tTrain Loss: 1.428439\tVal Acc: 0.4637\tVal F1 (weighted): 0.4760\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47cd471fa2941b791a1df02b5941cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457a91ee38f44c918dd7c20d7233dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.4958).\n",
      "Epoch 7\tTrain Loss: 1.395608\tVal Acc: 0.4967\tVal F1 (weighted): 0.4958\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709435138e23494285382f30df57a194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd21e3a754594839a69cfc852e295c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5075).\n",
      "Epoch 8\tTrain Loss: 1.367409\tVal Acc: 0.5038\tVal F1 (weighted): 0.5075\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f20a6e4f77d4ea58a26fd05cdc49c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85690a29b7442bfa9bfee7e20519eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5109).\n",
      "Epoch 9\tTrain Loss: 1.338979\tVal Acc: 0.5029\tVal F1 (weighted): 0.5109\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5146c2c4f8954ef0bb40058ec9d1f11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc6e0e9bf64ef49410360f00a8fb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5129).\n",
      "Epoch 10\tTrain Loss: 1.314346\tVal Acc: 0.5108\tVal F1 (weighted): 0.5129\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c35bae4ec64a94a060f7003cad8f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236199046f89481a9ea7df69fec767bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5265).\n",
      "Epoch 11\tTrain Loss: 1.290066\tVal Acc: 0.5207\tVal F1 (weighted): 0.5265\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa25266af0342d6a9c0bb8b1a79738d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ce4cd20dd74e56adae11e5855cbee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5294).\n",
      "Epoch 12\tTrain Loss: 1.270868\tVal Acc: 0.5257\tVal F1 (weighted): 0.5294\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d35bf36dad4eb9bffb3329caaf2d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb44d66fc75453cb46c05025a6f39e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5353).\n",
      "Epoch 13\tTrain Loss: 1.249163\tVal Acc: 0.5302\tVal F1 (weighted): 0.5353\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5b8abb726943a5a61faa8665d3e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cfc9efc5084fd2a0c8b501603c6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5395).\n",
      "Epoch 14\tTrain Loss: 1.230707\tVal Acc: 0.5341\tVal F1 (weighted): 0.5395\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e85df9147e40718ca7fea3efbae7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eaec7ed9dbc4712a6153c9f752feff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best F1: 0.5415).\n",
      "Epoch 15\tTrain Loss: 1.211586\tVal Acc: 0.5338\tVal F1 (weighted): 0.5415\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59ac53a7c924aada6b1b4faf70ae449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m SAVE_PATH = \u001b[33m\"\u001b[39m\u001b[33msaves/models/Sentiment_v4.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# f1_average_mode를 'weighted', 'macro', 'micro' 등으로 변경 가능\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# warmup_epochs를 0으로 설정하면 Warm-up 없이 바로 scheduler_plateau가 적용됨\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mTransformer_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 5 에포크 동안 Warm-up\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mf1_average_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns/sentiment_experiment_v4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                  \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mTransformer_Train\u001b[39m\u001b[34m(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler, warmup_epochs, f1_average_mode, log_dir, save_path, multiplier)\u001b[39m\n\u001b[32m     39\u001b[39m train_loss_sum = \u001b[32m0\u001b[39m\n\u001b[32m     40\u001b[39m NN.train()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43me\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m Training\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"saves/models/Sentiment_v4.pt\"\n",
    "NN = Transformer(vocab_size=VOCAB_SIZE,embedding_dim=128,hidden_dim=32,output_dim=7,n_layers=2,n_heads=16,dropout_p=0.1,max_len=128,pad_token_id=0)\n",
    "\n",
    "all_train_labels = train_datasets_dict[\"labels\"]\n",
    "num_classes = NN.output_dim\n",
    "\n",
    "label_counts = np.bincount(all_train_labels, minlength=num_classes)\n",
    "class_counts = torch.tensor(label_counts, dtype=torch.float)\n",
    "class_counts = torch.where(class_counts == 0, torch.tensor(1.0), class_counts)\n",
    "print(f\"클래스별 샘플 수: {class_counts.tolist()}\")\n",
    "\n",
    "class_weights = class_counts.sum() / class_counts\n",
    "class_weights = class_weights.to(device)\n",
    "print(f\"계산된 클래스 가중치: {class_weights.tolist()}\")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(NN.parameters(), lr=5e-5) # Warm-up과 함께 쓸 초기 LR을 여기서 설정\n",
    "scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "epoch = 500\n",
    "\n",
    "# f1_average_mode를 'weighted', 'macro', 'micro' 등으로 변경 가능\n",
    "# warmup_epochs를 0으로 설정하면 Warm-up 없이 바로 scheduler_plateau가 적용됨\n",
    "Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau,\n",
    "                  warmup_epochs=5, # 5 에포크 동안 Warm-up\n",
    "                  f1_average_mode='weighted',\n",
    "                  log_dir=\"runs/sentiment_experiment_v4\",\n",
    "                  save_path=SAVE_PATH,\n",
    "                  multiplier=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8789baa-6fb4-4cac-84bc-84cf7971e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"Sentiment.pt\",weights_only=False)\n",
    "# torch.save(model,f\"{saveFilePath}train_15.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f8250-467c-435f-82c2-1f5712586b2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transfer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ea231-0a55-42ea-9eaf-091ad3f4c2a0",
   "metadata": {},
   "source": [
    "PRETRAIN_MODEL_SAVE_PATH = \"saves/Pretrain.pt\"\n",
    "MODEL_SAVE_PATH = \"Sentiment.pt\"\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "NUM_ATTENTION_HEADS = 12\n",
    "INTERMEDIATE_SIZE = 3072\n",
    "TYPE_VOCAB_SIZE = 2\n",
    "DROPOUT_PROB = 0.1\n",
    "\n",
    "config = CustomBertConfig(\n",
    "    VOCAB_SIZE=VOCAB_SIZE,\n",
    "    HIDDEN_SIZE=HIDDEN_SIZE,\n",
    "    NUM_HIDDEN_LAYERS=NUM_HIDDEN_LAYERS,\n",
    "    NUM_ATTENTION_HEADS=NUM_ATTENTION_HEADS,\n",
    "    INTERMEDIATE_SIZE=INTERMEDIATE_SIZE,\n",
    "    MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,\n",
    "    TYPE_VOCAB_SIZE=TYPE_VOCAB_SIZE,\n",
    "    DROPOUT_PROB=DROPOUT_PROB\n",
    ")\n",
    "\n",
    "model = CustomBertSequenceClassification(config,PRETRAIN_MODEL_SAVE_PATH,7)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(\"모델 가중치 로드 중...\")\n",
    "    # 먼저 CPU에 로드한 후 모델에 로드합니다.\n",
    "    loaded_state_dict = torch.load(MODEL_SAVE_PATH, map_location='cpu')\n",
    "    model.load_state_dict(loaded_state_dict)\n",
    "    print(\"모델 가중치 로드 완료.\")\n",
    "else:\n",
    "    print(\"새로운 모델 초기화 완료. 저장된 가중치를 찾을 수 없습니다.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Custom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : {num_params}')\n",
    "print(f'모델이 담긴 장치 : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943dc6e-66bf-40fb-9ebd-59b3a1d055a8",
   "metadata": {},
   "source": [
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-8\n",
    "WEIGHT_DECAY = 0.1\n",
    "optimizer = AdamW(model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f8ff3-f849-4353-b91b-d7f0280e9002",
   "metadata": {},
   "source": [
    "train_losses = []\n",
    "acc = 0\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "print(f\"\\n<--- 학습 시작 ---> ({EPOCHS} 에폭)\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    loss_sum = 0\n",
    "    progress_bar = tqdm(train_loader,desc=f\"Train Epoch {e+1}\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model.forward(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            token_type_ids=batch[\"token_type_ids\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        progress_bar.set_postfix({'loss':f\"{(loss_sum/(step+1)):.4f}\"})\n",
    "        del outputs, loss\n",
    "        if 'ccuda' in str(device):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = loss_sum / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Train Epoch {e+1} 완료. 평균 학습 손실 : {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_progress = tqdm(val_loader, desc=f\"Validation Epoch {e+1}\")\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_progress):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            y = model.forward(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"]\n",
    "            )\n",
    "            y_logits = y[\"logits\"]\n",
    "            t = batch[\"labels\"]\n",
    "            correct += (y_logits.argmax(dim=-1) == t).sum().item()\n",
    "            total += len(batch[\"input_ids\"])\n",
    "            val_progress.set_postfix({\"acc\" : f\"{((correct/total)*100):.2f}%\"})\n",
    "            \n",
    "    acc = correct / total\n",
    "    \n",
    "    print(f\"Validation Epoch {e+1} 완료. 검증 정확도 : {(acc*100):.2f}%\")\n",
    "    \n",
    "    if acc <= prev_acc:\n",
    "        cnt += 1\n",
    "    else :\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "\n",
    "    if cnt >= 5:\n",
    "        print(\"train halted\")\n",
    "        break\n",
    "       \n",
    "print(\"\\n<--- 학습 완료 --->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cadfc-c327-42f7-ad3c-86197a281f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
