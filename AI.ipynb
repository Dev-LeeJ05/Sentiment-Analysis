{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c84beab-fdd3-4b92-b4ff-53216eb51626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The PyTorch API of nested tensors is in prototype stage and will change in the near future.\")\n",
    "# warnings.filterwarnings(\"ignore\", message=\"The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible.\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from CustomDataCollatorForSequenceClassification import CustomDataCollatorForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from WordPieceTokenizer import WordPieceTokenizer as Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from CustomBertSequenceClassification import CustomBertSequenceClassification\n",
    "from CustomBert import CustomBertConfig\n",
    "import CustomBert\n",
    "from collections import Counter\n",
    "import os\n",
    "from Model import LSTM\n",
    "from Model import Transformer, PositionalEncoding\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import _LRScheduler,ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "dataFilePath = 'datasets/'\n",
    "saveFilePath = 'saves/'\n",
    "# vocab_file_path = f'{dataFilePath}/sentiment_vocab/vocab.txt'\n",
    "vocab_file_path = f'{saveFilePath}vocab.txt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = Tokenizer(vocab_file_path,do_lower_case=False,strip_accents=False,clean_text=True)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da38b69e-5ccd-4751-a66e-934077285c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>str_len</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>token_type_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 17637 15450 2000 11814 1086 3628 29262 3715 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>24</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 2916 188 5325 1233 1125 22353 1 3 0 0 0 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>12</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 2785 1148 1373 1425 1029 6671 2020 1073 1324...</td>\n",
       "      <td>불안</td>\n",
       "      <td>14</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 6916 3859 2747 183 1011 15973 3 0 0 0 0 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>13</td>\n",
       "      <td>1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 5315 4113 1036 1627 1073 1762 3018 3 0 0 0 0...</td>\n",
       "      <td>불안</td>\n",
       "      <td>11</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  발화  감정  str_len  \\\n",
       "0  2 17637 15450 2000 11814 1086 3628 29262 3715 ...  불안       24   \n",
       "1  2 2916 188 5325 1233 1125 22353 1 3 0 0 0 0 0 ...  불안       12   \n",
       "2  2 2785 1148 1373 1425 1029 6671 2020 1073 1324...  불안       14   \n",
       "3  2 6916 3859 2747 183 1011 15973 3 0 0 0 0 0 0 ...  불안       13   \n",
       "4  2 5315 4113 1036 1627 1073 1762 3018 3 0 0 0 0...  불안       11   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "1  1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "2  1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "3  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "4  1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "\n",
       "                                      token_type_ids  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{dataFilePath}sentiment_train.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b88c422-88f4-4e80-9f82-53fdcb86b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['감정'] == '불안'),'감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'),'감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'),'감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'),'감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'),'감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'),'감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'),'감정'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3590a8e3-5472-4959-ac1b-e4cd59645e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_classification_dataset(data_frame, tokenizer):\n",
    "    processed_tokens = []\n",
    "    processed_attentions = []\n",
    "    processed_token_type_ids = []\n",
    "\n",
    "    for i in tqdm(range(len(data_frame)), desc=\"데이터 파싱 중\"):\n",
    "        token_str = data_frame.iloc[i, 0]\n",
    "        attention_str = data_frame.iloc[i, 3]\n",
    "        token_type_ids_str = data_frame.iloc[i, 4]\n",
    "\n",
    "        processed_tokens.append([int(t) for t in token_str.split(\" \")])\n",
    "        processed_attentions.append([int(a) for a in attention_str.split(\" \")])\n",
    "        processed_token_type_ids.append([int(t) for t in token_type_ids_str.split(\" \")])\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": processed_tokens,\n",
    "        \"attention_mask\": processed_attentions,\n",
    "        \"token_type_ids\": processed_token_type_ids,\n",
    "        \"labels\": data_frame[\"감정\"].values.tolist()\n",
    "    }\n",
    "        \n",
    "    return dataset_dict\n",
    "    \n",
    "def tensor_dataset(dataset_dict):\n",
    "    input_ids = torch.tensor(dataset_dict[\"input_ids\"], dtype=torch.long)\n",
    "    attention_mask = torch.tensor(dataset_dict[\"attention_mask\"], dtype=torch.long)\n",
    "    labels = torch.tensor(dataset_dict[\"labels\"], dtype=torch.long)\n",
    "    tensorDataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    return tensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5776764d-05d3-46df-909d-2bc90d0f23ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd69eec9cd15491fb588c5100cb38c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/110931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 110931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801de507b44d45feb1e24fbfcafd67dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/27733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터셋 크기: 27733\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, train_size=0.8, test_size=0.2, stratify=df['감정'])\n",
    "\n",
    "train_datasets_dict = prepare_classification_dataset(train_df, tokenizer)\n",
    "train_datasets = tensor_dataset(train_datasets_dict)\n",
    "print(f\"학습 데이터셋 크기: {len(train_datasets)}\")\n",
    "\n",
    "val_datasets_dict = prepare_classification_dataset(val_df, tokenizer)\n",
    "val_datasets = tensor_dataset(val_datasets_dict)\n",
    "print(f\"검증 데이터셋 크기: {len(val_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d33c59-b51e-43c7-a0a1-c59828780848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 DataLoader 배치 수: 1734\n",
      "검증 DataLoader 배치 수: 434\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"학습 DataLoader 배치 수: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"검증 DataLoader 배치 수: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1f979-cd2b-403d-adfa-28d9d39a6b92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404c17eb-9dbf-4369-81f5-218c8a2be20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataframe(data_frame, device,batch_size,shuffle=False):\n",
    "#     tensor_x_list = []\n",
    "#     attentions = []\n",
    "#     token_type_ids_ = []\n",
    "#     for i in tqdm(range(len(data_frame))):\n",
    "#         token = data_frame.iloc[i,0]\n",
    "#         token = token.split(\" \")\n",
    "#         token_list = []\n",
    "#         for t in token:\n",
    "#             token_list.append(int(t))\n",
    "#         tensor_x_list.append(token_list)\n",
    "        \n",
    "#         attention = data_frame.iloc[i,3]\n",
    "#         attention = attention.split(\" \")\n",
    "#         attention_list = []\n",
    "#         for a in attention:\n",
    "#             attention_list.append(int(a))\n",
    "#         attentions.append(attention_list)\n",
    "\n",
    "#         token_type_ids = data_frame.iloc[i,4]\n",
    "#         token_type_ids = token_type_ids.split(\" \")\n",
    "#         token_type_ids_list = []\n",
    "#         for t in token_type_ids:\n",
    "#             token_type_ids_list.append(int(t))\n",
    "#         token_type_ids_.append(attention_list)\n",
    "        \n",
    "#     tensor_x = torch.tensor(tensor_x_list, dtype=torch.long, device=device)\n",
    "#     tensor_attention = torch.tensor(attentions, dtype=torch.long, device=device)\n",
    "#     tensor_token_type_ids = torch.tensor(token_type_ids_, dtype=torch.long, device=device)\n",
    "#     tensor_t = torch.tensor(data_frame[\"감정\"].values.tolist(), dtype=torch.long, device=device)\n",
    "\n",
    "#     dataset = TensorDataset(tensor_x,tensor_attention,tensor_t,tensor_token_type_ids)\n",
    "#     loader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True)\n",
    "#     return loader\n",
    "    \n",
    "#     dataset = {\"input_ids\" : tensor_x, \"attention_mask\":tensor_attention,\"token_type_ids\":tensor_token_type_ids,\"labels\":tensor_t}\n",
    "    \n",
    "    \n",
    "\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d065c33-266a-43af-924c-c76f35f47549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Train(epoch,device,train_loader,val_loader,NN,loss_function,optimizer):\n",
    "    acc = 0\n",
    "    prev_acc = 0\n",
    "    cnt = 0\n",
    "    for e in range(epoch):\n",
    "        NN.to(device)\n",
    "        loss_sum = 0\n",
    "        NN.train()\n",
    "        for x, attention,t in train_loader:\n",
    "            y = NN(x,attention)\n",
    "            loss = loss_function(y,t)\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_sum /= len(train_loader)\n",
    "    \n",
    "        NN.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, attention, t in val_loader:\n",
    "                x = x.to(device)\n",
    "                attention = attention.to(device)\n",
    "                t = t.to(device)\n",
    "    \n",
    "                y = NN(x, attention)\n",
    "                correct += (y.argmax(dim=-1) == t).sum().item()\n",
    "                total += len(x)\n",
    "        acc = correct / total\n",
    "    \n",
    "        if acc <= prev_acc:\n",
    "            cnt += 1\n",
    "        else :\n",
    "            torch.save(NN.state_dict(), \"Sentiment.pt\")\n",
    "            cnt = 0\n",
    "            prev_acc = acc\n",
    "        \n",
    "        print(f\"epoch  {e+1}\\t\\tloss {loss_sum:.12f}\\tacc {acc:.4f}\\tcnt {cnt}\")\n",
    "        \n",
    "        if cnt >= 5:\n",
    "            print(\"train halted\")\n",
    "            break\n",
    "            \n",
    "    print(\"---------- 학습 종료 ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a68f657-9e9f-47d8-b477-2cb4ec4601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN = LSTM(vocab_size=vocab_size,embedding_dim=embedding_dim,hidden_dim=64,output_dim=7,n_layers=4,bidirectional=True,dropout_p=0.1)\n",
    "# NN.to(device)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(NN.parameters(),lr=0.001)\n",
    "# epoch = 500\n",
    "# LSTM_Train(epoch,device,train_loader,val_loader,NN,loss_function,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3a0ef-22f3-4ed6-8b5d-4676131d5f34",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b599944c-bab7-4d2e-986e-bf50e02b409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = self.base_lrs\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_last_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, metrics=None):\n",
    "        if self.finished and self.after_scheduler:\n",
    "            if isinstance(self.after_scheduler, ReduceLROnPlateau) and metrics is not None:\n",
    "                return self.after_scheduler.step(metrics)\n",
    "            else:\n",
    "                return self.after_scheduler.step() # metrics가 없으면 파라미터 없이 호출 (일반 스케줄러)\n",
    "        else:\n",
    "            return super(GradualWarmupScheduler, self).step() # Warmup 기간에는 파라미터 없이 호출\n",
    "\n",
    "def Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler,\n",
    "                      warmup_epochs=5, f1_average_mode='weighted', log_dir=\"runs/sentiment_experiment\", save_path=\"saves/models/Sentiment.pt\", multiplier=1.0):\n",
    "    \n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    combined_scheduler = GradualWarmupScheduler(optimizer, multiplier=multiplier, total_epoch=warmup_epochs, after_scheduler=scheduler)\n",
    "    \n",
    "    best_f1_weighted = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        NN.to(device)\n",
    "        \n",
    "        train_loss_sum = 0\n",
    "        NN.train()\n",
    "        for x, attention, t in tqdm(train_loader, desc=f\"Epoch {e+1} Training\"):\n",
    "            x = x.to(device)\n",
    "            attention = attention.to(device)\n",
    "            t = t.to(device)\n",
    "\n",
    "            y = NN(x, attention)\n",
    "            loss = loss_function(y, t)\n",
    "            train_loss_sum += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(NN.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        train_loss_sum /= len(train_loader)\n",
    "        \n",
    "        NN.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_all_preds = []\n",
    "        val_all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for x, attention, t in tqdm(val_loader, desc=f\"Epoch {e+1} Validation\", leave=False):\n",
    "                x = x.to(device)\n",
    "                attention = attention.to(device)\n",
    "                t = t.to(device)\n",
    "                \n",
    "                y = NN(x, attention)\n",
    "                \n",
    "                preds = y.argmax(dim=-1)\n",
    "                val_correct += (preds == t).sum().item()\n",
    "                val_total += len(x)\n",
    "\n",
    "                val_all_preds.extend(preds.cpu().numpy())\n",
    "                val_all_targets.extend(t.cpu().numpy())\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        if len(np.unique(val_all_targets)) > 1:\n",
    "            val_f1_weighted = f1_score(val_all_targets, val_all_preds, average='weighted', zero_division=0)\n",
    "            val_f1_macro = f1_score(val_all_targets, val_all_preds, average='macro', zero_division=0)\n",
    "        else:\n",
    "            val_f1_weighted = 1.0 if (len(val_all_targets) > 0 and np.all(np.array(val_all_preds) == np.array(val_all_targets))) else 0.0\n",
    "            val_f1_macro = val_f1_weighted\n",
    "\n",
    "        if val_f1_weighted > best_f1_weighted:\n",
    "            best_f1_weighted = val_f1_weighted\n",
    "            torch.save(NN.state_dict(), save_path)\n",
    "            print(f\"모델 저장 완료 (Best Weighted F1: {best_f1_weighted:.4f}).\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # 스케줄러 스텝: val_f1_weighted 값을 'metrics' 파라미터로 전달\n",
    "        combined_scheduler.step(val_f1_weighted)\n",
    "        \n",
    "        # TensorBoard에 로깅\n",
    "        writer.add_scalar('Loss/train', train_loss_sum, e)\n",
    "        writer.add_scalar('Metrics/val_accuracy', val_acc, e)\n",
    "        writer.add_scalar('Metrics/val_f1_weighted', val_f1_weighted, e)\n",
    "        writer.add_scalar('Metrics/val_f1_macro', val_f1_macro, e)\n",
    "        writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], e)\n",
    "        \n",
    "        print(f\"Epoch {e+1}\\tTrain Loss: {train_loss_sum:.6f}\\tVal Acc: {val_acc:.4f}\\t\\tVal F1 (Weighted): {val_f1_weighted:.4f}\\tVal F1 (Macro): {val_f1_macro:.4f}\\tNo Improve Epochs: {epochs_no_improve}\")\n",
    "        \n",
    "        if epochs_no_improve >= 25:\n",
    "            print(\"조기 종료: 검증 F1 점수 개선 없음.\")\n",
    "            break\n",
    "            \n",
    "    writer.close()\n",
    "    print(\"---------- 학습 종료 ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1875b120-9a16-4755-ac20-2d2ffa247ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 클래스별 샘플 수: [11721.0, 10579.0, 14537.0, 19798.0, 38801.0, 10981.0, 4514.0]\n",
      "학습 데이터 클래스 분포: Counter({4: 38801, 3: 19798, 2: 14537, 0: 11721, 5: 10981, 1: 10579, 6: 4514})\n",
      "계산된 클래스 가중치: [89.57286834716797, 109.95541381835938, 58.23127365112305, 31.39519691467285, 8.173725128173828, 102.0521240234375, 603.9246826171875]\n",
      "첫 배치 레이블 분포: Counter({4: 24, 1: 11, 2: 7, 3: 6, 6: 6, 5: 5, 0: 5})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ecb00a25f94b09a1e6310321ed3a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4c370d2c204537a12294328fbdab6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료 (Best Weighted F1: 0.6558).\n",
      "Epoch 1\tTrain Loss: 1.006563\tVal Acc: 0.6715\t\tVal F1 (Weighted): 0.6558\tVal F1 (Macro): 0.5835\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6480f462cb2f406b945671a0346aa789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e25f6a156dc4072be6be1ed5721a7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Validation:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tTrain Loss: 1.004485\tVal Acc: 0.6714\t\tVal F1 (Weighted): 0.6551\tVal F1 (Macro): 0.5825\tNo Improve Epochs: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b86d2220393405dbf4f7e167b4345e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m data, attention_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m첫 배치 레이블 분포: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mTransformer_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mf1_average_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns/sentiment_experiment_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_number\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36mTransformer_Train\u001b[1;34m(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler, warmup_epochs, f1_average_mode, log_dir, save_path, multiplier)\u001b[0m\n\u001b[0;32m     51\u001b[0m train_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(NN\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_number = 12\n",
    "SAVE_PATH = f\"saves/models/Sentiment_v{train_number}.pt\"\n",
    "NN = Transformer(vocab_size=VOCAB_SIZE,embedding_dim=128,hidden_dim=32,output_dim=7,n_layers=2,n_heads=16,dropout_p=0.1,max_len=128,pad_token_id=0)\n",
    "NN.load_state_dict(torch.load(\"saves/models/Sentiment_v11.pt\"))\n",
    "all_train_labels_original = train_df['감정'].values.astype(int)\n",
    "\n",
    "num_classes = NN.output_dim\n",
    "\n",
    "label_counts_original = np.bincount(all_train_labels_original, minlength=num_classes)\n",
    "class_counts_tensor = torch.tensor(label_counts_original, dtype=torch.float)\n",
    "\n",
    "class_counts_tensor = torch.where(class_counts_tensor == 0, torch.tensor(1.0), class_counts_tensor)\n",
    "\n",
    "print(f\"학습 데이터 클래스별 샘플 수: {class_counts_tensor.tolist()}\")\n",
    "print(f\"학습 데이터 클래스 분포: {Counter(all_train_labels_original)}\")\n",
    "\n",
    "class_weights = (class_counts_tensor.sum() / class_counts_tensor) ** 2\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(f\"계산된 클래스 가중치: {class_weights.tolist()}\")\n",
    "\n",
    "# loss_function = nn.CrossEntropyLoss(weights=class_weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(NN.parameters(), lr=1e-5)\n",
    "scheduler_plateau = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.8, patience=3,min_lr=1e-7)\n",
    "epoch = 10000\n",
    "\n",
    "data, attention_mask, labels = next(iter(train_loader))\n",
    "print(f\"첫 배치 레이블 분포: {Counter(labels.cpu().numpy())}\")\n",
    "\n",
    "Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau,\n",
    "                  warmup_epochs=5,\n",
    "                  f1_average_mode='weighted',\n",
    "                  log_dir=f\"runs/sentiment_experiment_v{train_number}\",\n",
    "                  save_path=SAVE_PATH,\n",
    "                  multiplier=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8789baa-6fb4-4cac-84bc-84cf7971e4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d93f8250-467c-435f-82c2-1f5712586b2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transfer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ea231-0a55-42ea-9eaf-091ad3f4c2a0",
   "metadata": {},
   "source": [
    "PRETRAIN_MODEL_SAVE_PATH = \"saves/Pretrain.pt\"\n",
    "MODEL_SAVE_PATH = \"Sentiment.pt\"\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "NUM_ATTENTION_HEADS = 12\n",
    "INTERMEDIATE_SIZE = 3072\n",
    "TYPE_VOCAB_SIZE = 2\n",
    "DROPOUT_PROB = 0.1\n",
    "\n",
    "config = CustomBertConfig(\n",
    "    VOCAB_SIZE=VOCAB_SIZE,\n",
    "    HIDDEN_SIZE=HIDDEN_SIZE,\n",
    "    NUM_HIDDEN_LAYERS=NUM_HIDDEN_LAYERS,\n",
    "    NUM_ATTENTION_HEADS=NUM_ATTENTION_HEADS,\n",
    "    INTERMEDIATE_SIZE=INTERMEDIATE_SIZE,\n",
    "    MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,\n",
    "    TYPE_VOCAB_SIZE=TYPE_VOCAB_SIZE,\n",
    "    DROPOUT_PROB=DROPOUT_PROB\n",
    ")\n",
    "\n",
    "model = CustomBertSequenceClassification(config,PRETRAIN_MODEL_SAVE_PATH,7)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(\"모델 가중치 로드 중...\")\n",
    "    # 먼저 CPU에 로드한 후 모델에 로드합니다.\n",
    "    loaded_state_dict = torch.load(MODEL_SAVE_PATH, map_location='cpu')\n",
    "    model.load_state_dict(loaded_state_dict)\n",
    "    print(\"모델 가중치 로드 완료.\")\n",
    "else:\n",
    "    print(\"새로운 모델 초기화 완료. 저장된 가중치를 찾을 수 없습니다.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Custom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : {num_params}')\n",
    "print(f'모델이 담긴 장치 : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943dc6e-66bf-40fb-9ebd-59b3a1d055a8",
   "metadata": {},
   "source": [
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-8\n",
    "WEIGHT_DECAY = 0.1\n",
    "optimizer = AdamW(model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f8ff3-f849-4353-b91b-d7f0280e9002",
   "metadata": {},
   "source": [
    "train_losses = []\n",
    "acc = 0\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "print(f\"\\n<--- 학습 시작 ---> ({EPOCHS} 에폭)\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    loss_sum = 0\n",
    "    progress_bar = tqdm(train_loader,desc=f\"Train Epoch {e+1}\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model.forward(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            token_type_ids=batch[\"token_type_ids\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        progress_bar.set_postfix({'loss':f\"{(loss_sum/(step+1)):.4f}\"})\n",
    "        del outputs, loss\n",
    "        if 'ccuda' in str(device):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = loss_sum / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Train Epoch {e+1} 완료. 평균 학습 손실 : {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_progress = tqdm(val_loader, desc=f\"Validation Epoch {e+1}\")\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_progress):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            y = model.forward(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"]\n",
    "            )\n",
    "            y_logits = y[\"logits\"]\n",
    "            t = batch[\"labels\"]\n",
    "            correct += (y_logits.argmax(dim=-1) == t).sum().item()\n",
    "            total += len(batch[\"input_ids\"])\n",
    "            val_progress.set_postfix({\"acc\" : f\"{((correct/total)*100):.2f}%\"})\n",
    "            \n",
    "    acc = correct / total\n",
    "    \n",
    "    print(f\"Validation Epoch {e+1} 완료. 검증 정확도 : {(acc*100):.2f}%\")\n",
    "    \n",
    "    if acc <= prev_acc:\n",
    "        cnt += 1\n",
    "    else :\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "\n",
    "    if cnt >= 5:\n",
    "        print(\"train halted\")\n",
    "        break\n",
    "       \n",
    "print(\"\\n<--- 학습 완료 --->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cadfc-c327-42f7-ad3c-86197a281f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
