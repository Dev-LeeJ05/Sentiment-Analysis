{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30df8e5-48dc-491b-9a3e-07e53075054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "\n",
    "MODEL_SAVE_PATH = \"best_kobert_sentiment_model.pt\" \n",
    "CHECKPOINT_PATH = \"kobert_sentiment_checkpoint.pt\"\n",
    "dataFilePath = 'datasets/'\n",
    "saveFilePath = 'saves/'\n",
    "MODEL_NAME = \"skt/kobert-base-v1\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992baa6a-1883-4b19-9f3c-1bd2b544bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(f\"SentimentDataset initialized with {len(texts)} texts and {len(labels)} labels.\")\n",
    "        # self.labels_count는 main 함수에서 이 객체 생성 후 직접 할당됩니다.\n",
    "        # 예: train_dataset.labels_count = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        if text.lower() == 'nan' or pd.isna(text):\n",
    "            text = \"\"\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        current_token_type_ids = encoding['token_type_ids'].flatten()\n",
    "\n",
    "        current_token_type_ids = torch.clamp(current_token_type_ids, 0, 1)\n",
    "\n",
    "\n",
    "        if hasattr(self, 'labels_count') and not (0 <= label < self.labels_count):\n",
    "             raise ValueError(f\"Label {label} is out of bounds [0, {self.labels_count-1}] at item {item} for text: '{text}'\")\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': current_token_type_ids, # flatten된 텐서 사용\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights=None):\n",
    "        super().__init__(config)\n",
    "        if class_weights is not None:\n",
    "            self.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.loss_fct = nn.CrossEntropyLoss() # 가중치가 없으면 일반 CrossEntropyLoss 사용\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "def train_model(model, data_loader, optimizer, scheduler, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler:\n",
    "            with torch.amp.autocast('cuda'): \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Evaluating\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, predictions), f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, scaler, best_f1_score, id_to_label, unique_labels, patience_counter, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "        'best_f1_score': best_f1_score,\n",
    "        'id_to_label': id_to_label,\n",
    "        'unique_labels': unique_labels,\n",
    "        'model_config': model.config.to_dict(),\n",
    "        'patience_counter': patience_counter # <--- 이 부분 추가\n",
    "    }, path)\n",
    "    print(f\"체크포인트가 '{path}'에 성공적으로 저장되었습니다.\")\n",
    "\n",
    "def load_checkpoint(path, device, class_weights=None):\n",
    "    if not os.path.exists(path):\n",
    "        return False, None\n",
    "\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(checkpoint['unique_labels']))\n",
    "    model = CustomBertForSequenceClassification(config, class_weights=class_weights)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    scaler_state_dict = checkpoint.get('scaler_state_dict', None) \n",
    "\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1_score = checkpoint['best_f1_score']\n",
    "    id_to_label = checkpoint['id_to_label']\n",
    "    unique_labels = checkpoint['unique_labels']\n",
    "    patience_counter = checkpoint.get('patience_counter', 0) # <--- 이 부분 추가 (기본값 0)\n",
    "\n",
    "    print(f\"체크포인트가 '{path}'에서 성공적으로 로드되었습니다. 학습을 에폭 {start_epoch}부터 재개합니다.\")\n",
    "    \n",
    "    loaded_data = {\n",
    "        'start_epoch': start_epoch,\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'scaler_state_dict': scaler_state_dict,\n",
    "        'best_f1_score': best_f1_score,\n",
    "        'id_to_label': id_to_label,\n",
    "        'unique_labels': unique_labels,\n",
    "        'scheduler_state_dict': checkpoint['scheduler_state_dict'],\n",
    "        'patience_counter': patience_counter # <--- 이 부분 추가\n",
    "    }\n",
    "    return True, loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74593447-9602-40ea-84f8-347a2868576a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>str_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>불안</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>불안</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>불안</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>불안</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>불안</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         발화  감정  str_len\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??  불안       24\n",
       "1              그냥 내 느낌일뿐겠지?  불안       12\n",
       "2            아직너무초기라서 그런거죠?  불안       14\n",
       "3             유치원버스 사고 낫다던데  불안       13\n",
       "4               근데 원래이런거맞나요  불안       11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{dataFilePath}sentiment_data.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddd422c-da2a-41b2-95bf-2d2ba620aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['감정'] == '불안'),'감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'),'감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'),'감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'),'감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'),'감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'),'감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'),'감정'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbc6624-6103-4c24-855d-48996c488203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 라벨 매핑: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}\n",
      "총 감정 클래스 수: 7\n",
      "SentimentDataset initialized with 115947 texts and 115947 labels.\n",
      "SentimentDataset initialized with 28987 texts and 28987 labels.\n",
      "시스템의 CPU 코어 수: 20\n",
      "권장 num_workers 시작 값: 4\n"
     ]
    }
   ],
   "source": [
    "texts = df['발화'].tolist()\n",
    "labels = df['감정'].tolist()\n",
    "\n",
    "unique_labels = sorted(list(set(labels)))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "print(f\"감정 라벨 매핑: {label_to_id}\")\n",
    "numeric_labels = [label_to_id[label] for label in labels]\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"총 감정 클래스 수: {num_labels}\")\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(numeric_labels), y=numeric_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# 모든 numeric_labels가 0부터 num_labels-1 사이에 있는지 최종 확인\n",
    "if not all(0 <= l < num_labels for l in numeric_labels):\n",
    "    print(\"오류: numeric_labels에 num_labels 범위를 벗어나는 값이 있습니다. 데이터와 매핑을 확인하세요.\")\n",
    "    problematic_labels = [l for l in numeric_labels if not (0 <= l < num_labels)]\n",
    "    print(f\"문제되는 라벨 값들: {set(problematic_labels)}\")\n",
    "    exit() # 중요한 오류이므로 바로 종료\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, numeric_labels, test_size=0.2, random_state=42, stratify=numeric_labels)\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "max_cpu_cores = os.cpu_count()\n",
    "print(f\"시스템의 CPU 코어 수: {max_cpu_cores}\")\n",
    "\n",
    "recommended_num_workers = min(4, max_cpu_cores if max_cpu_cores else 0)\n",
    "\n",
    "print(f\"권장 num_workers 시작 값: {recommended_num_workers}\")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # 여기에 설정\n",
    "    pin_memory=True # GPU로 데이터를 더 빠르게 전송\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0, # 여기에 설정\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2180ff84-3934-41f5-a837-6be32e78e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트가 'kobert_sentiment_checkpoint.pt'에서 성공적으로 로드되었습니다. 학습을 에폭 23부터 재개합니다.\n",
      "학습을 에폭 23부터 재개합니다. (이전 최고 F1-Score: 0.5322, 인내 카운터: 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11392\\1677975280.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "# 학습 관련 변수들을 기본값으로 초기화\n",
    "PATIENCE = 5\n",
    "NUM_EPOCHS = 30\n",
    "start_epoch = 0\n",
    "best_f1_score = -1.0\n",
    "LEARNING_RATE = 2e-6\n",
    "model = None\n",
    "optimizer = None\n",
    "scaler = None\n",
    "scheduler = None # 스케줄러도 초기화\n",
    "# 체크포인트 로드 시도\n",
    "# load_checkpoint의 첫 번째 반환 값(성공 여부)을 확인\n",
    "load_success, loaded_data = load_checkpoint(CHECKPOINT_PATH, device, class_weights=class_weights_tensor)\n",
    "\n",
    "if load_success: # 체크포인트 로드에 성공한 경우\n",
    "    start_epoch = loaded_data['start_epoch']\n",
    "    model = loaded_data['model']\n",
    "    optimizer = loaded_data['optimizer']\n",
    "    best_f1_score = loaded_data['best_f1_score']\n",
    "    id_to_label = loaded_data['id_to_label']\n",
    "    unique_labels = loaded_data['unique_labels']\n",
    "    num_labels = len(unique_labels)\n",
    "    patience_counter = loaded_data['patience_counter'] # <--- 이 부분 추가\n",
    "    # AMP Scaler 재구성\n",
    "    if loaded_data['scaler_state_dict'] and str(device) == 'cuda':\n",
    "        scaler = GradScaler()\n",
    "        scaler.load_state_dict(loaded_data['scaler_state_dict'])\n",
    "    elif str(device) == 'cuda': \n",
    "        scaler = GradScaler()\n",
    "    else: \n",
    "        scaler = None\n",
    "    print(f\"학습을 에폭 {start_epoch}부터 재개합니다. (이전 최고 F1-Score: {best_f1_score:.4f}, 인내 카운터: {patience_counter})\")\n",
    "else: # 체크포인트 로드에 실패한 경우 (파일 없음) -> 처음부터 학습 시작\n",
    "    print(f\"체크포인트 파일 '{CHECKPOINT_PATH}'을 찾을 수 없습니다. 처음부터 학습을 시작합니다.\")\n",
    "    # 모델 초기화\n",
    "    try:\n",
    "        model = CustomBertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, num_labels=num_labels, class_weights=class_weights_tensor)\n",
    "    except TypeError: # 이전 버전 transformers 호환성\n",
    "        config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "        model = CustomBertForSequenceClassification(config, class_weights=class_weights_tensor)\n",
    "        pretrained_model_state_dict = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).state_dict()\n",
    "        model.load_state_dict(pretrained_model_state_dict, strict=False)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 옵티마이저 생성\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    # 스케일러 생성\n",
    "    if str(device) == 'cuda':\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "        \n",
    "# 스케줄러 재구성/초기화 (옵티마이저가 설정된 후에 수행되어야 함)\n",
    "total_steps = len(train_data_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "# 로드된 스케줄러 상태가 있다면 로드\n",
    "if load_success and loaded_data['scheduler_state_dict'] is not None:\n",
    "    scheduler.load_state_dict(loaded_data['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a9d652-ef9d-4368-807a-401655eb6262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoBERT 모델 학습 시작...\n",
      "--- Epoch 24/30 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11a6ce0cad247d79055b64e325558e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.5021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6966a4a6884c189bd36aae9a3f19db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.4662, F1-Score: 0.4707\n",
      "  F1-Score 개선 없음. 현재 최고 F1-Score: 0.5322 (인내 카운터: 6/5)\n",
      "체크포인트가 'kobert_sentiment_checkpoint.pt'에 성공적으로 저장되었습니다.\n",
      "검증 F1-Score가 5 에폭 동안 개선되지 않아 학습을 조기 종료합니다.\n",
      "KoBERT 모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "print(\"KoBERT 모델 학습 시작...\")\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f'--- Epoch {epoch + 1}/{NUM_EPOCHS} ---')\n",
    "    train_loss = train_model(model, train_data_loader, optimizer, scheduler, device, scaler)\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    val_accuracy, val_f1 = evaluate_model(model, val_data_loader, device)\n",
    "    print(f'  Validation Accuracy: {val_accuracy:.4f}, F1-Score: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1_score:\n",
    "        best_f1_score = val_f1\n",
    "        patience_counter = 0 # <--- F1-Score 개선 시 카운터 초기화\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, scaler, best_f1_score, id_to_label, unique_labels, patience_counter, MODEL_SAVE_PATH) # <--- patience_counter 인자 추가\n",
    "        print(f\"  새로운 최고 F1-Score ({best_f1_score:.4f}) 달성, 모델 저장됨.\")\n",
    "    else:\n",
    "        patience_counter += 1 # <--- F1-Score 개선 없을 시 카운터 증가\n",
    "        print(f\"  F1-Score 개선 없음. 현재 최고 F1-Score: {best_f1_score:.4f} (인내 카운터: {patience_counter}/{PATIENCE})\")\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler, scaler, best_f1_score, id_to_label, unique_labels, patience_counter, CHECKPOINT_PATH) # <--- patience_counter 인자 추가\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter >= PATIENCE: # <--- 이 부분 추가: 인내심 한계 도달 시 학습 중단\n",
    "        print(f\"검증 F1-Score가 {PATIENCE} 에폭 동안 개선되지 않아 학습을 조기 종료합니다.\")\n",
    "        break \n",
    "print(\"KoBERT 모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b40e6-75bb-4a32-9212-af9012327e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
