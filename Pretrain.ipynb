{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813d1cb3-457a-4f89-93fb-306896078ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import unicodedata\n",
    "import re\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import CustomBert\n",
    "from WordPieceTokenizer import WordPieceTokenizer as Tokenizer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from PretrainDataset import TokenizedDataset\n",
    "from PretrainDataset import CustomDataCollatorForMLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f673ef-3409-4822-a649-8b92dd780704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, tokenizer, MAX_SEQUENCE_LENGTH):\n",
    "    concatenated_text = \" \".join(examples[\"text\"])\n",
    "\n",
    "    encoded_output = tokenizer.encode(\n",
    "        concatenated_text,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": [torch.tensor(encoded_output['input_ids'], dtype=torch.long)],\n",
    "        \"attention_mask\": [torch.tensor(encoded_output['attention_mask'], dtype=torch.long)],\n",
    "        \"token_type_ids\": [torch.tensor(encoded_output['token_type_ids'], dtype=torch.long)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863bebd9-614f-41c8-9446-0cf8898c4079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 8개의 텍스트 파일 로드 시작...\n",
      "원시 데이터셋 로드 완료. 총 702964개의 샘플.\n",
      "데이터셋 토큰화 및 청킹 시작...\n",
      "703\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_file_path=\"Pretrained/vocab.txt\",do_lower_case=False,strip_accents=False,clean_text=True)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "datasetsPath = 'datasets/'\n",
    "PREPROCESSED_TEXT_DIR = f'{datasetsPath}preprocess_wiki_text'\n",
    "\n",
    "text_files = [os.path.join(PREPROCESSED_TEXT_DIR, f) for f in os.listdir(PREPROCESSED_TEXT_DIR) if f.endswith('.txt')]\n",
    "\n",
    "if not text_files:\n",
    "    exit()\n",
    "print(f'총 {len(text_files)}개의 텍스트 파일 로드 시작...')\n",
    "raw_dataset = load_dataset(\"text\", data_files={\"train\": text_files}, split=\"train\")\n",
    "print(f\"원시 데이터셋 로드 완료. 총 {len(raw_dataset)}개의 샘플.\")\n",
    "    \n",
    "print('데이터셋 토큰화 및 청킹 시작...')\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    "    fn_kwargs={\"tokenizer\":tokenizer,\"MAX_SEQUENCE_LENGTH\":MAX_SEQUENCE_LENGTH},\n",
    "    desc=f\"맵핑 데이터셋 (토큰화 및 청킹, 최대 길이 {MAX_SEQUENCE_LENGTH})\"\n",
    ")\n",
    "print(len(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7669d0fd-1d29-47d3-ae96-b6eb00d2a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 데이터셋 저장 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a2c7632fff42f09d38178c12aa6e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 데이터셋이 'datasets/tokenized_dataset'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"토큰화된 데이터셋 저장 중...\")\n",
    "tokenized_dataset_path = \"datasets/tokenized_dataset\"\n",
    "tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "print(f\"토큰화된 데이터셋이 '{tokenized_dataset_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3554cc2-7b65-4251-9fb6-1058516ed260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch DataLoader 생성 완료. 배치 크기: 8\n",
      "총 훈련 배치 수: 88\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TokenizedDataset(tokenized_dataset)\n",
    "\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "print(f\"PyTorch DataLoader 생성 완료. 배치 크기: {BATCH_SIZE}\")\n",
    "print(f\"총 훈련 배치 수: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c4dab4-1e38-41a9-99c5-c25c081c1e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     model = CustomBert.CustomBertForMaskedLM(\n\u001b[32m     14\u001b[39m         vocab_size=VOCAB_SIZE,\n\u001b[32m     15\u001b[39m         hidden_size=HIDDEN_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         dropout_prob=DROPOUT_PROB\n\u001b[32m     22\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m     25\u001b[39m num_params = \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCustom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_SAVE_PATH = \"saves/Pretrain.pt\"\n",
    "\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "NUM_ATTENTION_HEADS = 12\n",
    "INTERMEDIATE_SIZE = 3072\n",
    "TYPE_VOCAB_SIZE = 2\n",
    "DROPOUT_PROB = 0.1\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    model = torch.load(MODEL_SAVE_PATH,weights_only=False)\n",
    "else:\n",
    "    model = CustomBert.CustomBertForMaskedLM(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "        num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "        intermediate_size=INTERMEDIATE_SIZE,\n",
    "        max_position_embeddings=MAX_SEQUENCE_LENGTH,\n",
    "        type_vocab_size=TYPE_VOCAB_SIZE,\n",
    "        dropout_prob=DROPOUT_PROB\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Custom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : {num_params}')\n",
    "print(f'모델이 담긴 장치 : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e11127-8e27-444b-b211-584a36e3dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 5000\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ef7de-acda-461d-9bb6-e2c8fa52d1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(f\"\\n<--- 학습 시작 ---> ({EPOCHS} 에폭)\")\n",
    "model.train()\n",
    "\n",
    "CHECKPOINT_DIR = \"pretrain_checkpoints\"\n",
    "\n",
    "prev_loss = 10\n",
    "for e in range(EPOCHS):\n",
    "    loss_sum = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Pre-train Epoch {e+1}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            if torch.isnan(v).any():\n",
    "                print(f\"Warning: NaN found in batch[{k}] at step {step}\")\n",
    "            if torch.isinf(v).any():\n",
    "                print(f\"Warning: Inf found in batch[{k}] at step {step}\")\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            token_type_ids=batch[\"token_type_ids\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        del outputs, loss\n",
    "        if 'cuda' in str(device):\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    avg_train_loss = loss_sum / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss) # 에폭별 평균 손실 저장\n",
    "\n",
    "    print(f\"Pre-train Epoch {e+1} 완료. 평균 학습 손실: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    if avg_train_loss < prev_loss - 0.0001:\n",
    "        torch.save(model.state_dict(),MODEL_SAVE_PATH)\n",
    "        prev_loss = avg_train_loss\n",
    "    \n",
    "    print(f\"모델 가중치 '{MODEL_SAVE_PATH}' 저장 완료.\")\n",
    "\n",
    "print(\"\\n<--- 학습 완료 --->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608baac4-d751-49b3-acad-39f2636c7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', linestyle='-', color='skyblue')\n",
    "plt.title('Pre-training Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Training Loss')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, EPOCHS + 1)) # x축 눈금 에폭 수에 맞춰 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98a5ff-7954-4736-856d-dc40f1873ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
