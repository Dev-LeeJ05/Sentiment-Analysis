{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813d1cb3-457a-4f89-93fb-306896078ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import unicodedata\n",
    "import re\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import CustomBert\n",
    "from CustomBert import CustomBertForMaskedLM\n",
    "from CustomBert import CustomBertConfig\n",
    "from WordPieceTokenizer import WordPieceTokenizer as Tokenizer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from PretrainDataset import TokenizedDataset\n",
    "from PretrainDataset import CustomDataCollatorForMLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f673ef-3409-4822-a649-8b92dd780704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, tokenizer, MAX_SEQUENCE_LENGTH):\n",
    "    import torch\n",
    "    concatenated_text = \" \".join(examples[\"text\"])\n",
    "\n",
    "    encoded_output = tokenizer.encode(\n",
    "        concatenated_text,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": [torch.tensor(encoded_output['input_ids'], dtype=torch.long)],\n",
    "        \"attention_mask\": [torch.tensor(encoded_output['attention_mask'], dtype=torch.long)],\n",
    "        \"token_type_ids\": [torch.tensor(encoded_output['token_type_ids'], dtype=torch.long)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863bebd9-614f-41c8-9446-0cf8898c4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1개의 텍스트 파일 로드 시작...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3f4ed53af94188916b83f76a51f2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원시 데이터셋 로드 완료. 총 3025090개의 샘플.\n",
      "데이터셋 토큰화 및 청킹 시작...\n",
      "3028\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_file_path=\"saves/vocab.txt\",do_lower_case=False,strip_accents=False,clean_text=True)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "num_cpu_cores = os.cpu_count()\n",
    "num_processes_to_use = num_cpu_cores if num_cpu_cores is not None else 1\n",
    "\n",
    "datasetsPath = 'datasets/'\n",
    "PREPROCESSED_TEXT_DIR = f'{datasetsPath}preprocess_wiki_text'\n",
    "\n",
    "text_files = [os.path.join(PREPROCESSED_TEXT_DIR, f) for f in os.listdir(PREPROCESSED_TEXT_DIR) if f.endswith('.txt')]\n",
    "\n",
    "if not text_files:\n",
    "    exit()\n",
    "print(f'총 {len(text_files)}개의 텍스트 파일 로드 시작...')\n",
    "raw_dataset = load_dataset(\"text\", data_files={\"train\": text_files}, split=\"train\")\n",
    "print(f\"원시 데이터셋 로드 완료. 총 {len(raw_dataset)}개의 샘플.\")\n",
    "    \n",
    "print('데이터셋 토큰화 및 청킹 시작...')\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"],\n",
    "    fn_kwargs={\"tokenizer\":tokenizer,\"MAX_SEQUENCE_LENGTH\":MAX_SEQUENCE_LENGTH},\n",
    "    desc=f\"맵핑 데이터셋 (토큰화 및 청킹, 최대 길이 {MAX_SEQUENCE_LENGTH})\"\n",
    ")\n",
    "print(len(tokenized_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7669d0fd-1d29-47d3-ae96-b6eb00d2a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 데이터셋 저장 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6db8bc71654c23a2b4dfaf5e28d363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 데이터셋이 'datasets/tokenized_dataset'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"토큰화된 데이터셋 저장 중...\")\n",
    "tokenized_dataset_path = \"datasets/tokenized_dataset\"\n",
    "tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "print(f\"토큰화된 데이터셋이 '{tokenized_dataset_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3554cc2-7b65-4251-9fb6-1058516ed260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch DataLoader 생성 완료. 배치 크기: 16\n",
      "총 훈련 배치 수: 152\n",
      "총 검증 배치 수: 38\n"
     ]
    }
   ],
   "source": [
    "full_dataset = TokenizedDataset(tokenized_dataset)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size,val_size])\n",
    "\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"PyTorch DataLoader 생성 완료. 배치 크기: {BATCH_SIZE}\")\n",
    "print(f\"총 훈련 배치 수: {len(train_dataloader)}\")\n",
    "print(f\"총 검증 배치 수: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20762d7b-968d-49ef-b5cc-3ebebfbf2539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 데이터셋 토큰화 및 디코딩 확인 ---\n",
      "\n",
      "--- Batch 1 ---\n",
      "\n",
      "Sample 1 Input IDs (Original):\n",
      "[2, 17633, 28309, 2080, 31312, 1175, 2743, 8166, 7, 4, 4716, 12702, 4, 3155, 9451, 11011, 1, 2280, 535, 4, 12205, 1, 3182, 3564, 8076, 2021, 297, 2166, 2135, 2492, 5434, 2870, 31312, 1175, 2743, 8166, 17633, 28309, 2080, 4, 4, 2743, 4, 4, 4, 230, 9, 4, 26920, 12702, 2198, 4734, 4647, 2080, 7, 1062, 4716, 12702, 7280, 3155, 9451, 1, 5671, 5744, 1140, 1, 26920, 12702, 26989, 2749, 5281, 4, 22542, 14356, 1, 983, 1028, 2124, 2266, 2351, 4, 4, 5656, 6779, 1123, 12087, 6108, 4737, 4191, 2000, 3698, 2003, 2198, 4, 1, 7473, 11011, 6978, 4737, 2172, 19427, 2172, 63, 4, 19165, 5415, 26920, 12702, 4, 7331, 4, 2074, 2280, 5626, 1, 7909, 4, 2217, 4737, 30346, 4737, 4191, 7909, 4, 2711, 4, 2080, 3]\n",
      "Decoded Input: [CLS] 시리즈 탄소중립을 위한 바이오산업의 새로운 도전 '[MASK]이트바이오[MASK]협력 협의체 발족[UNK] 개최 생[MASK] 플라스틱[UNK] 규제개선 이루어질 지원 등 개발 연구 협력모델 발굴 바이오산업의 새로운 도전 시리즈 탄소중립을 위한[MASK][MASK] 새로운[MASK][MASK][MASK] 는 .[MASK] 화이트바이오 산업 경쟁력 강화를 위한 '화이트바이오 연대협력 협의체[UNK] 출범시켰음[UNK] 화이트바이오 산업은 기존 화학[MASK] 소재를 미생물[UNK] 효소 등을 활용하거나[MASK][MASK] 재생가능한 자원을 활용하여 바이오기반으로 대체하는 산업[MASK][UNK] 금번 발족식은 바이오기업 석유화학기업 간[MASK] 형성을 목표로 화이트바이오[MASK] 경쟁력을[MASK] 위해 개최되었음[UNK] 원료[MASK] 필요한 바이오기술과 바이오기반 원료[MASK] 제품[MASK] 위한[SEP]\n",
      "Sample 1 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1062, -100, -100, 7280, -100, -100, -100, -100, -100, -100, 26460, -100, -100, 3182, -100, 5925, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 31312, 1175, -100, 8166, 7720, 2436, -100, -100, 1233, -100, -100, -100, -100, -100, -100, 7, -100, 4716, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5412, -100, -100, -100, -100, -100, -100, -100, -100, 6334, 297, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1159, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5174, -100, -100, -100, -100, 7629, -100, 9743, -100, -100, -100, -100, 7909, 20739, -100, 4737, -100, -100, -100, -100, 1175, -100, 2694, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]화[UNK][UNK] 연대[UNK][UNK][UNK][UNK][UNK][UNK]분해[UNK][UNK] 규제[UNK] 인센티브[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 바이오산업의[UNK] 도전 산업통상자원부 장관[UNK][UNK]일[UNK][UNK][UNK][UNK][UNK][UNK] '[UNK]이트[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]산업의[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 식물 등[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]임[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 네트워크[UNK][UNK][UNK][UNK] 산업의[UNK] 강화하기[UNK][UNK][UNK][UNK] 원료 생산에[UNK] 바이오[UNK][UNK][UNK][UNK]의[UNK]화를[UNK][UNK]\n",
      "--- Masked Token Check for Sample 1 ---\n",
      "Pos 9: '[MASK]' was masked, should be '화'\n",
      "Pos 12: '[MASK]' was masked, should be '연대'\n",
      "Pos 19: '[MASK]' was masked, should be '분해'\n",
      "Pos 22: '규제' was masked, should be '규제'\n",
      "Pos 24: '이루어질' was masked, should be '인센티브'\n",
      "Pos 39: '[MASK]' was masked, should be '바이오산업'\n",
      "Pos 40: '[MASK]' was masked, should be '의'\n",
      "Pos 42: '[MASK]' was masked, should be '도전'\n",
      "Pos 43: '[MASK]' was masked, should be '산업통상자원부'\n",
      "Pos 44: '[MASK]' was masked, should be '장관'\n",
      "Pos 47: '[MASK]' was masked, should be '일'\n",
      "Pos 54: ''' was masked, should be '''\n",
      "Pos 56: '이트' was masked, should be '이트'\n",
      "Pos 71: '[MASK]' was masked, should be '산업의'\n",
      "Pos 80: '[MASK]' was masked, should be '식물'\n",
      "Pos 81: '[MASK]' was masked, should be '등'\n",
      "Pos 93: '[MASK]' was masked, should be '임'\n",
      "Pos 103: '[MASK]' was masked, should be '네트워크'\n",
      "Pos 108: '[MASK]' was masked, should be '산업의'\n",
      "Pos 110: '[MASK]' was masked, should be '강화하기'\n",
      "Pos 115: '원료' was masked, should be '원료'\n",
      "Pos 116: '[MASK]' was masked, should be '생산에'\n",
      "Pos 118: '바이오' was masked, should be '바이오'\n",
      "Pos 123: '[MASK]' was masked, should be '의'\n",
      "Pos 125: '[MASK]' was masked, should be '화를'\n",
      "\n",
      "Sample 2 Input IDs (Original):\n",
      "[2, 2095, 1, 15637, 1, 4, 584, 11445, 2197, 4350, 4, 659, 20932, 1023, 14002, 2552, 5518, 18082, 2628, 943, 2036, 4051, 9679, 1, 8276, 13415, 2001, 15503, 2861, 2299, 26638, 4, 4114, 25399, 31250, 1, 4, 1453, 1022, 1, 919, 1012, 4, 1, 10460, 1022, 1, 529, 1316, 1022, 1, 4, 4, 74, 2299, 10685, 2056, 659, 12927, 1011, 1, 707, 760, 4, 1316, 1022, 1, 10460, 1022, 1, 25399, 8905, 6754, 4, 3349, 2144, 3613, 29269, 30047, 29805, 4371, 17355, 27884, 2494, 1, 7, 1712, 4, 1098, 1253, 2148, 4369, 7141, 5313, 1, 6, 6627, 1078, 4, 2642, 141, 540, 1, 4, 12019, 3067, 1, 4, 659, 1573, 27597, 1024, 14358, 7226, 4, 1316, 1573, 4477, 556, 26245, 4, 4917, 8, 10460, 4, 4477, 117, 3]\n",
      "Decoded Input: [CLS] 경기도[UNK] 경기문화재단[UNK][MASK] 시 군이 함께 운영하는[MASK] 옛길에서 속의 일부터 주간 도민을 대상으로 한 교육 프로그램을 진행한다[UNK] 조선시대 한양에서 경기도를 거쳐 주요 지방을[MASK]하던 의주 죽인[UNK][MASK]흥로[UNK] 평해[MASK][UNK] 영남로[UNK] 삼남로[UNK][MASK][MASK] 개 주요 도로가 경기 옛길이다[UNK] 이 중[MASK]남로[UNK] 영남로[UNK] 의주로가 복원[MASK] 역사 문화 생태 자원이 결합된 복합문화체험공간으로 이용되고 있으며[UNK] '팔[MASK]유람 한국지방신문 협회[UNK] \"경기도[MASK] 좋은 길 선[UNK][MASK]되기도 했다[UNK][MASK] 옛길 임명을는 전문가와 함께하는[MASK]남길 구간 소사원[MASK] 평택 , 영남[MASK] 구간 구[SEP]\n",
      "Sample 2 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, 74, -100, -100, -100, -100, 2056, -100, -100, -100, 689, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3542, -100, -100, 1022, -100, 96, -100, -100, -100, 919, -100, 1022, -100, -100, -100, -100, -100, -100, -100, -100, 17757, 297, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 529, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1431, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1712, 1078, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 17563, -100, -100, -100, -100, 2333, -100, -100, -100, 2056, -100, -100, 11057, -100, -100, -100, 529, -100, -100, -100, -100, -100, 1573, -100, -100, -100, 141, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK] 개[UNK][UNK][UNK][UNK] 경기[UNK][UNK][UNK] 월[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 연결[UNK][UNK]로[UNK] 경[UNK][UNK][UNK] 평[UNK]로[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 강화로 등[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 삼[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]돼[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]팔도[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 걷기[UNK][UNK][UNK][UNK] 선정[UNK][UNK][UNK] 경기[UNK][UNK] 아카데미[UNK][UNK][UNK] 삼[UNK][UNK][UNK][UNK][UNK]길[UNK][UNK][UNK] 길[UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 2 ---\n",
      "Pos 5: '[MASK]' was masked, should be '개'\n",
      "Pos 10: '[MASK]' was masked, should be '경기'\n",
      "Pos 14: '속의' was masked, should be '월'\n",
      "Pos 31: '[MASK]' was masked, should be '연결'\n",
      "Pos 34: '죽인' was masked, should be '로'\n",
      "Pos 36: '[MASK]' was masked, should be '경'\n",
      "Pos 40: '평' was masked, should be '평'\n",
      "Pos 42: '[MASK]' was masked, should be '로'\n",
      "Pos 51: '[MASK]' was masked, should be '강화로'\n",
      "Pos 52: '[MASK]' was masked, should be '등'\n",
      "Pos 63: '[MASK]' was masked, should be '삼'\n",
      "Pos 73: '[MASK]' was masked, should be '돼'\n",
      "Pos 86: '팔' was masked, should be '팔'\n",
      "Pos 87: '[MASK]' was masked, should be '도'\n",
      "Pos 98: '[MASK]' was masked, should be '걷기'\n",
      "Pos 103: '[MASK]' was masked, should be '선정'\n",
      "Pos 107: '[MASK]' was masked, should be '경기'\n",
      "Pos 110: '임명을' was masked, should be '아카데미'\n",
      "Pos 114: '[MASK]' was masked, should be '삼'\n",
      "Pos 120: '[MASK]' was masked, should be '길'\n",
      "Pos 124: '[MASK]' was masked, should be '길'\n",
      "\n",
      "Sample 3 Input IDs (Original):\n",
      "[2, 116, 4, 5217, 3183, 27543, 1180, 116, 1007, 1205, 1823, 1134, 4, 4, 4, 4700, 2488, 1175, 7079, 1022, 3669, 7884, 8195, 4, 12715, 4, 31750, 4, 4, 462, 6471, 4, 4, 5631, 2424, 974, 3722, 2490, 2011, 1, 2757, 2024, 298, 1823, 1134, 11852, 1175, 540, 1513, 3134, 1090, 116, 19922, 5217, 22152, 2832, 4, 27543, 12522, 116, 1007, 1205, 1823, 27287, 9743, 706, 19289, 2933, 16112, 3669, 14222, 3267, 3084, 3347, 30710, 31750, 14789, 9433, 1106, 12510, 297, 26904, 2124, 4, 2011, 1, 4, 2021, 28334, 22508, 332, 116, 1007, 1205, 1823, 27287, 5203, 1024, 116, 19922, 5217, 4, 2762, 4, 2000, 5738, 4, 9876, 1023, 5911, 5591, 1043, 1031, 3436, 9739, 9776, 710, 2092, 4, 2102, 4, 1, 710, 4021, 4265, 2228, 9505, 3]\n",
      "Decoded Input: [CLS] 교[MASK]동차보험 봉사단체 교보디렉트[MASK][MASK][MASK] 전개 가장의 교통사고로 인해 힘든 생활을[MASK] 가정의[MASK]녀를[MASK][MASK] 발 빠른[MASK][MASK] 기업이 있어 화제가 되고 있다[UNK] 최근부터 디렉트마케팅의 선두업체인 교보자동차보험에서는[MASK] 봉사단체인 교보디렉트에 강화하기 의 봉사활동을 통해서 사고로 인해 경제가 매우 어려운 가정 유자녀를중에서다니며 장학금 등 생활용품 등을[MASK] 있다[UNK][MASK] 지원행사의 주역 랙 교보디렉트에이드는 교보자동차[MASK] 소속[MASK]으로 구성된[MASK]체로서 특별히 자전거원정대를 구성하여 이달 일 서울[MASK]사를[MASK][UNK] 일 오전 시에 대구센터에[SEP]\n",
      "Sample 3 Labels (Original):\n",
      "[-100, -100, 19922, -100, -100, -100, -100, -100, -100, -100, -100, -100, 10959, 1197, 15647, -100, -100, -100, -100, -100, -100, -100, -100, 2043, -100, 30710, -100, 10149, 2074, -100, -100, 1014, 8761, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 28079, -100, -100, -100, -100, -100, -100, -100, 5203, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2963, -100, -100, -100, -100, -100, -100, 19888, -100, -100, 2151, -100, -100, -100, 1090, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3183, -100, 8938, -100, -100, 27543, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1041, -100, 6221, -100, -100, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK]보자[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 에이드 봉사활동[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 하는[UNK] 유자[UNK] 돕기 위해[UNK][UNK]에 나선[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 자사[UNK][UNK][UNK][UNK][UNK][UNK][UNK]이드[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 찾아[UNK][UNK][UNK][UNK][UNK][UNK] 전달하고[UNK][UNK] 이번[UNK][UNK][UNK]인[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]보험[UNK] 임직원[UNK][UNK] 봉사단[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]본[UNK] 출발[UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 3 ---\n",
      "Pos 2: '[MASK]' was masked, should be '보자'\n",
      "Pos 12: '[MASK]' was masked, should be '에이'\n",
      "Pos 13: '[MASK]' was masked, should be '드'\n",
      "Pos 14: '[MASK]' was masked, should be '봉사활동'\n",
      "Pos 23: '[MASK]' was masked, should be '하는'\n",
      "Pos 25: '[MASK]' was masked, should be '유자'\n",
      "Pos 27: '[MASK]' was masked, should be '돕기'\n",
      "Pos 28: '[MASK]' was masked, should be '위해'\n",
      "Pos 31: '[MASK]' was masked, should be '에'\n",
      "Pos 32: '[MASK]' was masked, should be '나선'\n",
      "Pos 56: '[MASK]' was masked, should be '자사'\n",
      "Pos 64: '강화하기' was masked, should be '이드'\n",
      "Pos 76: '중에서' was masked, should be '찾아'\n",
      "Pos 83: '[MASK]' was masked, should be '전달하고'\n",
      "Pos 86: '[MASK]' was masked, should be '이번'\n",
      "Pos 90: '랙' was masked, should be '인'\n",
      "Pos 101: '[MASK]' was masked, should be '보험'\n",
      "Pos 103: '[MASK]' was masked, should be '임직원'\n",
      "Pos 106: '[MASK]' was masked, should be '봉사단'\n",
      "Pos 118: '[MASK]' was masked, should be '본'\n",
      "Pos 120: '[MASK]' was masked, should be '출발'\n",
      "\n",
      "Sample 4 Input IDs (Original):\n",
      "[2, 2194, 710, 22374, 1024, 526, 1041, 362, 1143, 1318, 3977, 17941, 7, 1641, 1451, 4, 5543, 13751, 4, 4, 1, 4770, 8157, 9373, 5499, 11469, 1048, 10738, 1203, 2824, 1, 3242, 4, 8360, 1032, 10328, 7052, 4, 297, 648, 2584, 2715, 4056, 1, 4, 2670, 4, 1090, 30071, 1175, 6217, 1206, 2899, 2399, 5443, 4, 2561, 1012, 9373, 12909, 26107, 10692, 4, 1046, 5017, 1203, 3067, 1, 4, 4, 4, 2552, 62, 274, 2590, 4, 4, 4, 2301, 4334, 4, 2197, 13751, 2977, 10680, 4, 10407, 1175, 25645, 6440, 11145, 2494, 1, 3289, 2700, 2478, 1, 6101, 1, 3672, 1, 3242, 17628, 2213, 3913, 297, 5499, 732, 5294, 21054, 1106, 10684, 29114, 1438, 2399, 5443, 1206, 7613, 1, 2124, 2981, 6096, 4, 1, 8242, 2349, 31611, 3]\n",
      "Decoded Input: [CLS] 지난 일 군포시는 산본 로데오거리 일원에서 '새봄[MASK] 일제 대청[MASK][MASK][UNK] 행사를 실시하여 깨끗한 관내 도시환경을 조성하고자 하였다[UNK] 이날[MASK] 군포부시장을 비롯해[MASK] 등 여 명이 참석했으며[UNK][MASK] 동안[MASK]인 도로변의 미세먼지와 각종 생활 쓰레기[MASK] 청소해 깨끗한 생활환경 조성으로 않다고[MASK]를 만들고자 했다[UNK][MASK][MASK][MASK] 일부터 각 동별로[MASK][MASK][MASK]단체 회원[MASK] 함께 대청소를 실시해[MASK] 곳곳의 취약지역 환경을 개선하고 있으며[UNK] 오는 일까지 도로[UNK] 도심[UNK] 하천[UNK] 이날 참여자들은 공원 등 관내 전 지역을 돌아다니며 무단투기된 생활 쓰레기와 담배[UNK] 등을 집중 수거[MASK][UNK] 상가지역 생활폐기물[SEP]\n",
      "Sample 4 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 15688, -100, -100, 3995, 180, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 14321, -100, -100, -100, -100, 2268, -100, -100, -100, -100, -100, -100, 4956, -100, 598, -100, -100, -100, -100, -100, -100, -100, -100, 2124, -100, -100, -100, -100, -100, 29850, 22374, -100, -100, -100, -100, -100, 5822, 3550, 2194, -100, -100, -100, -100, 5164, 455, 26837, -100, -100, 4039, -100, -100, -100, -100, 6101, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2004, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]맞이[UNK][UNK]소의 날[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 행사에는[UNK][UNK][UNK][UNK] 공무원[UNK][UNK][UNK][UNK][UNK][UNK] 겨울[UNK] 쌓[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 등을[UNK][UNK][UNK][UNK][UNK] 활기찬 군포시[UNK][UNK][UNK][UNK][UNK] 시는 앞서 지난[UNK][UNK][UNK][UNK] 지역주민 및 직능[UNK][UNK] 등과[UNK][UNK][UNK][UNK] 도심[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]하고[UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 4 ---\n",
      "Pos 15: '[MASK]' was masked, should be '맞이'\n",
      "Pos 18: '[MASK]' was masked, should be '소의'\n",
      "Pos 19: '[MASK]' was masked, should be '날'\n",
      "Pos 32: '[MASK]' was masked, should be '행사에는'\n",
      "Pos 37: '[MASK]' was masked, should be '공무원'\n",
      "Pos 44: '[MASK]' was masked, should be '겨울'\n",
      "Pos 46: '[MASK]' was masked, should be '쌓'\n",
      "Pos 55: '[MASK]' was masked, should be '등을'\n",
      "Pos 61: '않다고' was masked, should be '활기찬'\n",
      "Pos 62: '[MASK]' was masked, should be '군포시'\n",
      "Pos 68: '[MASK]' was masked, should be '시는'\n",
      "Pos 69: '[MASK]' was masked, should be '앞서'\n",
      "Pos 70: '[MASK]' was masked, should be '지난'\n",
      "Pos 75: '[MASK]' was masked, should be '지역주민'\n",
      "Pos 76: '[MASK]' was masked, should be '및'\n",
      "Pos 77: '[MASK]' was masked, should be '직능'\n",
      "Pos 80: '[MASK]' was masked, should be '등과'\n",
      "Pos 85: '[MASK]' was masked, should be '도심'\n",
      "Pos 122: '[MASK]' was masked, should be '하고'\n",
      "\n",
      "Sample 5 Input IDs (Original):\n",
      "[2, 9684, 931, 1207, 1, 2793, 628, 4, 1, 2514, 4, 1175, 28399, 1, 18194, 752, 17640, 713, 1168, 4, 1, 966, 4, 4, 22008, 4, 1152, 1022, 2536, 1, 10085, 13291, 1123, 607, 1022, 943, 2445, 17043, 1, 207, 9388, 4, 24513, 4265, 5877, 2004, 1, 207, 484, 1074, 4, 12485, 11620, 26010, 658, 19974, 1016, 1454, 30367, 4, 1309, 9355, 2861, 1, 2216, 11148, 305, 674, 22299, 1086, 28915, 1064, 2216, 25558, 1123, 978, 1571, 1175, 5101, 23652, 1, 207, 737, 20406, 2216, 13522, 13152, 1175, 4, 4, 8716, 1123, 4, 3132, 10906, 1206, 3548, 142, 1557, 1423, 1175, 872, 1103, 455, 14973, 1048, 968, 1242, 2087, 2997, 4, 4, 1039, 1017, 99, 1452, 15381, 11062, 2006, 21010, 9561, 1, 131, 287, 13139, 752, 4, 3]\n",
      "Decoded Input: [CLS] 아버지는 풍천[UNK] 자는 약[MASK][UNK]인을[MASK]의 증손으로[UNK] 할아버지는 좌승지 임윤[MASK][UNK] 형[MASK][MASK]판서[MASK]국로이며[UNK] 어머니는 청주한 씨로 한원의 딸이다[UNK] 년 선조[MASK] 사마 시에 합격하고[UNK] 년 별시[MASK] 급제에에 급제하여 예문관검열 대교[MASK]교 등의으로 거쳐[UNK] 되어 임진왜란 때 왜적에게 포로가 되어 항복한 황혁의 책임을 주장하였다[UNK] 년 정언이 되어 명나라 장수의[MASK][MASK] 소홀한[MASK]지의 무리와 군수 김백옥의 탐학 및 통역을 호위하지 않은[MASK][MASK]어사 고언백을 탄핵하여 파면시켰다[UNK] 그 뒤 예조 좌[MASK][SEP]\n",
      "Sample 5 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, 1425, -100, 4218, 1705, -100, -100, -100, -100, 752, -100, -100, -100, 2219, -100, -100, 1130, 12513, -100, 713, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 643, -100, -100, -100, -100, -100, -100, -100, -100, 24756, -100, -100, -100, -100, -100, -100, -100, -100, 493, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2216, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 17598, 1014, -100, -100, 583, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1048, -100, -100, -100, -100, 2056, 1158, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1177, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK]초[UNK] 임명필[UNK][UNK][UNK][UNK] 좌[UNK][UNK][UNK]이고[UNK][UNK]은 이조[UNK] 임[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 에[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]문과에[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 봉[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 되어[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 접견에[UNK][UNK] 승[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]을[UNK][UNK][UNK][UNK] 경기방[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]랑[UNK]\n",
      "--- Masked Token Check for Sample 5 ---\n",
      "Pos 7: '[MASK]' was masked, should be '초'\n",
      "Pos 9: '인을' was masked, should be '임명'\n",
      "Pos 10: '[MASK]' was masked, should be '필'\n",
      "Pos 15: '좌' was masked, should be '좌'\n",
      "Pos 19: '[MASK]' was masked, should be '이고'\n",
      "Pos 22: '[MASK]' was masked, should be '은'\n",
      "Pos 23: '[MASK]' was masked, should be '이조'\n",
      "Pos 25: '[MASK]' was masked, should be '임'\n",
      "Pos 41: '[MASK]' was masked, should be '에'\n",
      "Pos 50: '[MASK]' was masked, should be '문과에'\n",
      "Pos 59: '[MASK]' was masked, should be '봉'\n",
      "Pos 72: '되어' was masked, should be '되어'\n",
      "Pos 88: '[MASK]' was masked, should be '접견'\n",
      "Pos 89: '[MASK]' was masked, should be '에'\n",
      "Pos 92: '[MASK]' was masked, should be '승'\n",
      "Pos 105: '을' was masked, should be '을'\n",
      "Pos 110: '[MASK]' was masked, should be '경기'\n",
      "Pos 111: '[MASK]' was masked, should be '방'\n",
      "Pos 126: '[MASK]' was masked, should be '랑'\n",
      "\n",
      "Sample 6 Input IDs (Original):\n",
      "[2, 3218, 3614, 5171, 737, 18481, 1116, 8857, 1114, 12439, 4, 450, 19378, 2006, 2636, 27061, 4319, 3405, 4, 4166, 4, 4, 2257, 18886, 4434, 3183, 891, 1317, 2159, 7451, 9519, 707, 7154, 9937, 2029, 2233, 1, 4949, 2487, 707, 1719, 5903, 4, 4, 3125, 1130, 2636, 3183, 2542, 693, 4434, 3183, 4587, 8816, 2078, 739, 2012, 2338, 431, 2585, 4185, 3614, 2301, 455, 274, 9623, 3218, 3614, 2218, 3466, 9623, 18440, 709, 2090, 3614, 4, 5957, 3895, 10002, 16000, 1048, 12439, 1049, 565, 2008, 3125, 4587, 4, 1, 2203, 16000, 2542, 131, 8786, 462, 26898, 3467, 12439, 3565, 2838, 297, 4, 19101, 2401, 12439, 4, 10400, 3895, 944, 6625, 457, 1, 4, 4, 4, 5797, 406, 2368, 3125, 3614, 11775, 5634, 4, 9088, 1046, 3704, 3798, 3]\n",
      "Decoded Input: [CLS] 최소 가입 인원 정 학자수 미달과 청약[MASK] 미작성하여 단체보험의 재해 사망[MASK]금은[MASK][MASK] 대하여 월간 생명보험 통권 제호 토대로 면밀히 이 사건을 살펴보도록 하겠습니다[UNK] 위원회의 판단 이앗 직장[MASK][MASK] 보험은 단체보험으로서 위 생명보험상품되게 규정 제 사업 방법 몬 에서 정한 가입단체 및 동 단체의 최소 가입 기준 동일 단체의 종업원 인 이상 가입[MASK] 충족되어야 비로소 보험계약을 청약할 수 있는 보험상품[MASK][UNK] 또한 보험계약으로서 그 효력을 발 공감하는 위해서는 청약서의 작성 등[MASK] 실체로서 청약[MASK] 전제되어야 할 것인 바[UNK][MASK][MASK][MASK] 비록 망인이 보험 가입 의사표시로[MASK] 보험료를 모집인에게[SEP]\n",
      "Sample 6 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, 1286, -100, -100, -100, -100, 1023, -100, -100, -100, -100, -100, -100, -100, 3183, -100, 15533, 26468, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 80, -100, 1090, 3988, 3125, -100, -100, -100, -100, -100, -100, -100, -100, 2077, -100, -100, -100, -100, 1023, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 643, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2219, -100, -100, -100, -100, -100, -100, -100, 2053, -100, -100, -100, -100, -100, 9068, -100, -100, -100, 9831, -100, -100, -100, -100, -100, -100, 707, 3072, 2017, -100, -100, -100, -100, -100, -100, -100, 10576, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK]족[UNK][UNK][UNK][UNK]서[UNK][UNK][UNK][UNK][UNK][UNK][UNK]보험[UNK] 면책인지에[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 건[UNK]인 보장 보험[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 관리[UNK][UNK][UNK][UNK]서[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 에[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]이고[UNK][UNK][UNK][UNK][UNK][UNK][UNK]하기[UNK][UNK][UNK][UNK][UNK] 구체적[UNK][UNK][UNK] 행위가[UNK][UNK][UNK][UNK][UNK][UNK] 이 건의 경우[UNK][UNK][UNK][UNK][UNK][UNK][UNK] 본인의[UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 6 ---\n",
      "Pos 5: '학자' was masked, should be '족'\n",
      "Pos 10: '[MASK]' was masked, should be '서'\n",
      "Pos 18: '[MASK]' was masked, should be '보험'\n",
      "Pos 20: '[MASK]' was masked, should be '면책'\n",
      "Pos 21: '[MASK]' was masked, should be '인지에'\n",
      "Pos 40: '앗' was masked, should be '건'\n",
      "Pos 42: '[MASK]' was masked, should be '인'\n",
      "Pos 43: '[MASK]' was masked, should be '보장'\n",
      "Pos 44: '보험' was masked, should be '보험'\n",
      "Pos 53: '되게' was masked, should be '관리'\n",
      "Pos 58: '몬' was masked, should be '서'\n",
      "Pos 75: '[MASK]' was masked, should be '에'\n",
      "Pos 87: '[MASK]' was masked, should be '이고'\n",
      "Pos 95: '공감하는' was masked, should be '하기'\n",
      "Pos 101: '[MASK]' was masked, should be '구체적'\n",
      "Pos 105: '[MASK]' was masked, should be '행위가'\n",
      "Pos 112: '[MASK]' was masked, should be '이'\n",
      "Pos 113: '[MASK]' was masked, should be '건의'\n",
      "Pos 114: '[MASK]' was masked, should be '경우'\n",
      "Pos 122: '[MASK]' was masked, should be '본인의'\n",
      "\n",
      "Sample 7 Input IDs (Original):\n",
      "[2, 584, 14853, 1081, 1, 207, 4, 465, 5936, 10327, 5749, 4, 1136, 1152, 2049, 465, 5936, 10327, 5749, 10999, 1373, 1296, 1022, 14617, 3387, 14853, 2276, 21091, 1136, 5301, 6546, 4, 1, 15976, 5903, 1090, 1, 21091, 1014, 4441, 2045, 5778, 1036, 4, 16034, 2758, 4, 4, 12741, 2011, 1, 21091, 1136, 5552, 4, 4, 24004, 16706, 11691, 3387, 4, 4, 14842, 4328, 1438, 2348, 1, 16700, 26353, 1051, 10327, 3568, 3578, 20339, 5901, 4, 18329, 17753, 1048, 24655, 10896, 557, 1, 5350, 4, 14390, 3107, 3512, 2115, 4148, 3367, 628, 648, 2813, 11081, 15416, 2011, 1, 2237, 400, 6636, 2584, 207, 689, 5311, 21091, 1136, 4, 23694, 5778, 4, 65, 1029, 1, 433, 1367, 1, 16027, 1, 7836, 1164, 1, 936, 30223, 1136, 1, 3046, 3]\n",
      "Decoded Input: [CLS] 시 약사회[UNK] 년[MASK] 밤 시부터 새벽 시까지[MASK]약국 운영 밤 시부터 새벽 시까지 연중무휴로 운영되는 대구시 약사회의 심야약국이 야간[MASK][UNK] 바쁜 직장인[UNK] 심야에 응급하게 의약품이[MASK] 시민들로부터[MASK][MASK] 얻고 있다[UNK] 심야약국은[MASK][MASK] 수성구 황금 모의 대구시[MASK][MASK] 층에 개설된 이후[UNK] 늦은 밤이나 새벽시간대에 갑작스러운[MASK]상황에 약국을 찾지 못해 속[UNK] 했던[MASK] 불편이 크게 해소되어 하루 평균 약 여 명의 환자가 이용하고 있다[UNK] 시민 만 천여 명이 년 월 현재까지 심야약[MASK] 구입한 의약품[MASK] 감기[UNK] 몸살[UNK] 설사[UNK] 소화제[UNK] 피 두부약[UNK] 어린이[SEP]\n",
      "Sample 7 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, 710, -100, -100, -100, -100, 21091, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5301, -100, 31572, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2217, -100, -100, 2642, 17673, -100, -100, -100, -100, -100, -100, 207, 689, -100, -100, 1181, -100, 14853, 5849, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4441, -100, -100, -100, -100, -100, -100, -100, -100, 5573, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5275, -100, -100, 1130, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1159, -100, -100, 3046, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK] 일[UNK][UNK][UNK][UNK] 심야[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]국이[UNK] 근무자[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 필요한[UNK][UNK] 좋은 반응을[UNK][UNK][UNK][UNK][UNK][UNK] 년 월[UNK][UNK]동[UNK] 약사회관[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 응급[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 시민들의[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]국에서[UNK][UNK]은[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]임[UNK][UNK] 어린이[UNK]\n",
      "--- Masked Token Check for Sample 7 ---\n",
      "Pos 6: '[MASK]' was masked, should be '일'\n",
      "Pos 11: '[MASK]' was masked, should be '심야'\n",
      "Pos 29: '국이' was masked, should be '국이'\n",
      "Pos 31: '[MASK]' was masked, should be '근무자'\n",
      "Pos 43: '[MASK]' was masked, should be '필요한'\n",
      "Pos 46: '[MASK]' was masked, should be '좋은'\n",
      "Pos 47: '[MASK]' was masked, should be '반응을'\n",
      "Pos 54: '[MASK]' was masked, should be '년'\n",
      "Pos 55: '[MASK]' was masked, should be '월'\n",
      "Pos 58: '모의' was masked, should be '동'\n",
      "Pos 60: '[MASK]' was masked, should be '약사'\n",
      "Pos 61: '[MASK]' was masked, should be '회관'\n",
      "Pos 75: '[MASK]' was masked, should be '응급'\n",
      "Pos 84: '[MASK]' was masked, should be '시민들의'\n",
      "Pos 107: '[MASK]' was masked, should be '국에서'\n",
      "Pos 110: '[MASK]' was masked, should be '은'\n",
      "Pos 123: '두부' was masked, should be '임'\n",
      "Pos 126: '어린이' was masked, should be '어린이'\n",
      "\n",
      "Sample 8 Input IDs (Original):\n",
      "[2, 15824, 1012, 1345, 1222, 1591, 3549, 8229, 4, 4, 2280, 23043, 689, 2552, 2700, 4457, 4, 10427, 4, 2634, 3954, 7821, 3539, 15524, 13094, 15824, 1175, 2634, 1222, 4, 2015, 2523, 1066, 17293, 19165, 2074, 15824, 1012, 1345, 1222, 1591, 4, 1054, 8229, 385, 7374, 1, 2151, 2914, 1024, 207, 8559, 4, 8704, 31554, 1483, 4, 15824, 1175, 2362, 15292, 7821, 9401, 3285, 2013, 1, 7769, 19602, 3954, 7821, 2626, 8704, 4, 4, 1362, 4, 4, 1090, 556, 1294, 3758, 4, 10068, 2024, 5311, 817, 74, 6624, 1175, 2337, 11769, 3891, 4, 267, 1275, 1, 3689, 1, 2378, 1267, 1203, 2190, 4, 1048, 2122, 2914, 2328, 1, 3607, 1024, 13108, 2263, 25900, 15717, 4487, 6462, 2008, 15824, 4, 2634, 1222, 1591, 2626, 4132, 2066, 2169, 3558, 3]\n",
      "Decoded Input: [CLS] 국군해외파견 기념 전시회[MASK][MASK] 개최 국방부는 월 일부터 일까지 일간[MASK]역에서[MASK] 해외 평화유지 활동을 국민들에게 소개하고 국군의 해외파[MASK] 대한 이해적 공감대 형성을 위해 국군해외파견[MASK]집 전시회 를 개최한다[UNK] 이번 전시는 년 대한민국의[MASK] UN발명병[MASK] 국군의 국제평화유지 역사를 소개한다[UNK] 최초의 국제연합 평화유지활동 UN[MASK][MASK]O[MASK][MASK]인 소말리아[MASK]부대부터 현재까지 총 개 부대의 활동 모습이 사진[MASK] 도표[UNK] 영상[UNK] 홍보책자 등의[MASK]을 통해 전시된다[UNK] 아울러는 국회에 의원입법안으로 상정 장이 있는 국군[MASK] 해외파견활동 지원에 관한 법률 제정[SEP]\n",
      "Sample 8 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, 10989, 1091, -100, -100, -100, -100, -100, -100, 10989, -100, 12604, -100, -100, -100, 3539, -100, -100, -100, -100, -100, -100, 22788, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 896, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19602, -100, 904, -100, 2348, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 26, 1727, -100, 4427, 10068, -100, -100, -100, -100, 1116, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1114, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13590, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2115, -100, -100, 1175, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 용산역[UNK][UNK][UNK][UNK][UNK][UNK] 용산[UNK] 군은[UNK][UNK][UNK] 활동을[UNK][UNK][UNK][UNK][UNK][UNK]견에[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 특[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 국제연합[UNK] 파[UNK] 이후[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] PK[UNK] 파견부대[UNK][UNK][UNK][UNK]수[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]과[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 다채로운[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]되어[UNK][UNK]의[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 8 ---\n",
      "Pos 8: '[MASK]' was masked, should be '용산'\n",
      "Pos 9: '[MASK]' was masked, should be '역'\n",
      "Pos 16: '[MASK]' was masked, should be '용산'\n",
      "Pos 18: '[MASK]' was masked, should be '군은'\n",
      "Pos 22: '활동을' was masked, should be '활동을'\n",
      "Pos 29: '[MASK]' was masked, should be '견에'\n",
      "Pos 41: '[MASK]' was masked, should be '특'\n",
      "Pos 52: '[MASK]' was masked, should be '국제연합'\n",
      "Pos 54: '발명' was masked, should be '파'\n",
      "Pos 56: '[MASK]' was masked, should be '이후'\n",
      "Pos 72: '[MASK]' was masked, should be 'P'\n",
      "Pos 73: '[MASK]' was masked, should be 'K'\n",
      "Pos 75: '[MASK]' was masked, should be '파견'\n",
      "Pos 76: '[MASK]' was masked, should be '부대'\n",
      "Pos 81: '[MASK]' was masked, should be '수'\n",
      "Pos 92: '[MASK]' was masked, should be '과'\n",
      "Pos 102: '[MASK]' was masked, should be '다채로운'\n",
      "Pos 115: '장이' was masked, should be '되어'\n",
      "Pos 118: '[MASK]' was masked, should be '의'\n",
      "\n",
      "Sample 9 Input IDs (Original):\n",
      "[2, 9359, 710, 29784, 7, 16236, 4, 1152, 3775, 1, 3080, 25113, 3964, 29843, 2225, 15840, 4359, 4, 8302, 1, 3242, 3080, 31040, 2014, 17164, 1014, 4, 4359, 4, 3356, 1, 4, 5485, 2636, 4759, 2584, 2715, 4056, 17164, 2580, 2288, 5693, 2079, 2617, 4, 5134, 455, 8773, 2071, 4, 8516, 984, 4, 1014, 2015, 15356, 4, 4, 1, 4, 1, 2854, 3295, 4691, 9492, 13918, 7645, 4, 2080, 7, 4324, 6159, 4743, 1152, 3775, 1, 4, 4690, 3347, 1123, 3646, 4, 2552, 4, 6697, 1, 2854, 3295, 4, 5629, 1046, 4722, 2074, 452, 110, 119, 28017, 2197, 4, 15133, 4743, 1152, 14221, 2186, 6393, 5822, 2606, 1, 22596, 455, 7157, 2197, 4, 7, 1018, 1167, 1377, 28604, 5370, 1, 5589, 18795, 1024, 13241, 1029, 24668, 1036, 3]\n",
      "Decoded Input: [CLS] 지난달 일 광주시는 ' 재난대응[MASK]국훈련[UNK] 사전 관계관 회의를 건설도시국장 주재로 시청[MASK] 개최했다[UNK] 이날 사전 보고회에는 풍수해에[MASK] 시청[MASK] 부서[UNK][MASK] 유관기관 단체 관계자 명이 참석했으며 풍수해 대응 기관 상황에 따른 기능[MASK] 대처 및 임무 추진[MASK] 발표한 후[MASK]에 대한 토의[MASK][MASK][UNK][MASK][UNK] 재난으로부터 시민의 소중한 생명과 재산을[MASK] 위한 '재난대응 안전한국훈련[UNK][MASK] 상황을 가정한 훈련[MASK] 일부터[MASK] 진행된다[UNK] 재난으로부터[MASK] 광주시를 만들기 위해 민 관 군 경이 함께[MASK] 건설업 안전한국훈련을 실시하겠다고 시는 밝혔다[UNK] 독립유공자 및 시민이 함께[MASK] '광복절 경축행사[UNK] 시내 곳곳에는 태극기 물결이[SEP]\n",
      "Sample 9 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, 4743, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 23882, -100, -100, -100, -100, -100, -100, -100, -100, 21555, -100, 74, -100, -100, 74, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1315, -100, -100, -100, -100, 2924, -100, -100, 11963, -100, -100, -100, 1064, 23606, -100, 2554, -100, -100, -100, -100, -100, -100, -100, 11190, -100, -100, -100, -100, -100, -100, -100, -100, 3307, -100, -100, -100, -100, 2000, -100, 4457, -100, -100, -100, -100, 4743, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5128, 16236, 4743, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2043, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 24668, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK] 안전한[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 상황실에서[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 대응한[UNK] 개[UNK][UNK] 개[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]별[UNK][UNK][UNK][UNK] 사항을[UNK][UNK] 개선방안[UNK][UNK][UNK]가 이어졌다[UNK] 한편[UNK][UNK][UNK][UNK][UNK][UNK][UNK] 보호하기[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 실제[UNK][UNK][UNK][UNK]으로[UNK] 일간[UNK][UNK][UNK][UNK] 안전한[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 참여하는 재난대응 안전한[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 하는[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 물결[UNK][UNK]\n",
      "--- Masked Token Check for Sample 9 ---\n",
      "Pos 6: '[MASK]' was masked, should be '안전한'\n",
      "Pos 17: '[MASK]' was masked, should be '상황실에서'\n",
      "Pos 26: '[MASK]' was masked, should be '대응한'\n",
      "Pos 28: '[MASK]' was masked, should be '개'\n",
      "Pos 31: '[MASK]' was masked, should be '개'\n",
      "Pos 44: '[MASK]' was masked, should be '별'\n",
      "Pos 49: '[MASK]' was masked, should be '사항을'\n",
      "Pos 52: '[MASK]' was masked, should be '개선방안'\n",
      "Pos 56: '[MASK]' was masked, should be '가'\n",
      "Pos 57: '[MASK]' was masked, should be '이어졌다'\n",
      "Pos 59: '[MASK]' was masked, should be '한편'\n",
      "Pos 67: '[MASK]' was masked, should be '보호하기'\n",
      "Pos 76: '[MASK]' was masked, should be '실제'\n",
      "Pos 81: '[MASK]' was masked, should be '으로'\n",
      "Pos 83: '[MASK]' was masked, should be '일간'\n",
      "Pos 88: '[MASK]' was masked, should be '안전한'\n",
      "Pos 98: '[MASK]' was masked, should be '참여하는'\n",
      "Pos 99: '건설업' was masked, should be '재난대응'\n",
      "Pos 100: '안전한' was masked, should be '안전한'\n",
      "Pos 112: '[MASK]' was masked, should be '하는'\n",
      "Pos 125: '물결' was masked, should be '물결'\n",
      "\n",
      "Sample 10 Input IDs (Original):\n",
      "[2, 8429, 713, 19965, 1011, 1, 4, 1116, 18455, 2793, 4, 1, 9124, 747, 1116, 1219, 1, 9684, 3548, 4, 1, 10085, 810, 1037, 589, 1487, 4, 23691, 1014, 768, 1185, 1438, 589, 1031, 21773, 303, 1022, 4, 18689, 12445, 1014, 4, 4, 4, 5625, 6086, 2135, 2191, 1, 24304, 4, 9723, 6784, 1175, 707, 2955, 14539, 2053, 4, 3088, 23144, 5138, 2087, 2678, 589, 5116, 4, 1865, 1008, 6535, 3874, 2028, 1, 24688, 29751, 1175, 942, 8079, 212, 1468, 2053, 4, 892, 1194, 16646, 8606, 1017, 1179, 1722, 1031, 11292, 2818, 1132, 1575, 4, 8074, 2031, 547, 305, 2316, 1, 2676, 2944, 20703, 4, 99, 1255, 1043, 2042, 24780, 4, 4, 29751, 1175, 942, 8079, 212, 1468, 2053, 2074, 12322, 4, 4, 2944, 20703, 2015, 99, 3]\n",
      "Decoded Input: [CLS] 본관은 임천이다[UNK][MASK]수재는 자는[MASK][UNK] 호는 졸수재[UNK] 아버지는 군수[MASK][UNK] 어머니는 청송 심씨[MASK] 참의에 증직된 심정양의 딸로[MASK]서부터 학문에[MASK][MASK][MASK]학을 깊이 연구하였다[UNK] 천지[MASK]물과 우주의 이치를 통관하기[MASK] 년간 사람들과 접촉하지 않고 심실에[MASK]앉아 공부했다고 한다[UNK] 이황 이이의 학설을 논변하기[MASK] 퇴율 길에선생사단칠정인도이기설후[MASK] 지은 것이 세 때인데[UNK] 이미 이와 기에[MASK] 고차원적인 정의를[MASK][MASK] 이이의 학설을 논변하기 위해 이기[MASK][MASK] 이와 기에 대한 고[SEP]\n",
      "Sample 10 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, 747, -100, -100, -100, 27308, -100, -100, -100, -100, -100, -100, -100, -100, 2219, -100, -100, -100, -100, -100, -100, 1022, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2407, -100, -100, -100, 12310, 11404, 16785, -100, -100, -100, -100, -100, -100, 1126, -100, -100, -100, -100, -100, -100, -100, 2074, -100, -100, -100, -100, -100, -100, -100, 2149, -100, -100, -100, -100, -100, -100, -100, 29751, -100, -100, -100, -100, -100, -100, 2074, -100, -100, 1119, -100, -100, -100, -100, -100, -100, -100, -100, -100, 12265, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2015, -100, -100, -100, -100, -100, 3190, 24688, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8079, 6335, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK] 졸[UNK][UNK][UNK] 성경[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]이고[UNK][UNK][UNK][UNK][UNK][UNK]로[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 어려[UNK][UNK][UNK] 힘써 일찍이 성리[UNK][UNK][UNK][UNK][UNK][UNK]만[UNK][UNK][UNK][UNK][UNK][UNK][UNK] 위해[UNK][UNK][UNK][UNK][UNK][UNK][UNK] 들어[UNK][UNK][UNK][UNK][UNK][UNK][UNK] 이이[UNK][UNK][UNK][UNK][UNK][UNK] 위해[UNK][UNK]양[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]변을[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 대한[UNK][UNK][UNK][UNK][UNK] 내려 이황[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]설을 지어[UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 10 ---\n",
      "Pos 6: '[MASK]' was masked, should be '졸'\n",
      "Pos 10: '[MASK]' was masked, should be '성경'\n",
      "Pos 19: '[MASK]' was masked, should be '이고'\n",
      "Pos 26: '[MASK]' was masked, should be '로'\n",
      "Pos 37: '[MASK]' was masked, should be '어려'\n",
      "Pos 41: '[MASK]' was masked, should be '힘써'\n",
      "Pos 42: '[MASK]' was masked, should be '일찍이'\n",
      "Pos 43: '[MASK]' was masked, should be '성리'\n",
      "Pos 50: '[MASK]' was masked, should be '만'\n",
      "Pos 58: '[MASK]' was masked, should be '위해'\n",
      "Pos 66: '[MASK]' was masked, should be '들어'\n",
      "Pos 74: '이이' was masked, should be '이이'\n",
      "Pos 81: '[MASK]' was masked, should be '위해'\n",
      "Pos 84: '길에' was masked, should be '양'\n",
      "Pos 94: '[MASK]' was masked, should be '변을'\n",
      "Pos 104: '[MASK]' was masked, should be '대한'\n",
      "Pos 110: '[MASK]' was masked, should be '내려'\n",
      "Pos 111: '[MASK]' was masked, should be '이황'\n",
      "Pos 121: '[MASK]' was masked, should be '설을'\n",
      "Pos 122: '[MASK]' was masked, should be '지어'\n",
      "\n",
      "Sample 11 Input IDs (Original):\n",
      "[2, 4648, 17185, 14234, 7694, 710, 628, 648, 2813, 12353, 14906, 4, 1, 659, 1573, 7954, 4, 1, 12940, 1, 17185, 23993, 7954, 18371, 1024, 2964, 2312, 14181, 17185, 659, 1573, 5547, 2074, 2422, 1438, 11154, 659, 1573, 10541, 1114, 2197, 17185, 1048, 4821, 5159, 2784, 4, 8936, 565, 2011, 1, 710, 11360, 7954, 18371, 2147, 8705, 2592, 7934, 7727, 30661, 1, 4, 1573, 1175, 14366, 3277, 1, 22420, 3424, 7987, 4, 7727, 4475, 19136, 4, 1023, 4, 1573, 1114, 4, 4, 4, 1, 15676, 1, 2765, 3765, 2194, 689, 4012, 1078, 7954, 18371, 4, 4, 2056, 1103, 20672, 6303, 19136, 1, 6227, 952, 1495, 1, 22408, 31314, 1482, 2109, 21641, 1, 659, 4, 3666, 29519, 1427, 3277, 1, 12940, 1, 4648, 689, 4, 693, 1618, 1181, 3]\n",
      "Decoded Input: [CLS] 경기도는 남한산성 세계유산센터에서 일 약 여 명의 도민들이 참여는[MASK][UNK] 옛길 테마[MASK][UNK] 진행했다[UNK] 남한산성 역사는 테마강좌는 지난해 으로 완료된 남한산성 옛길 홍보를 위해 기획된 행사로 옛길 탐방과 함께 남한산성을 주제로 국내외 전문가[MASK] 들을 수 있다[UNK] 일 진행된 테마강좌에서는 서울대 국토문제 연구소 박사는[UNK][MASK]길의 지리적 이야기[UNK] 경희대학교 민속[MASK] 연구소 선임연구원의[MASK]서[MASK]길과[MASK][MASK][MASK][UNK] 열렸다[UNK] 이어 도는 지난 월 일에도 테마강좌[MASK][MASK] 경기학연구센터 수석연구원의[UNK] 나의 행궁[UNK] 국사 수출에찬위원회 위원에[UNK] 옛[MASK] 보는 병자호란 이야기[UNK] 진행했다[UNK] 경기도는 월[MASK] 위례동[SEP]\n",
      "Sample 11 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2770, -100, -100, -100, -100, 18371, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 25304, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 659, -100, -100, -100, -100, -100, -100, 3424, -100, 1103, -100, -100, -100, 2095, -100, 659, -100, -100, 2144, 2775, 3277, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7954, -100, 1046, 12261, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1331, -100, -100, -100, -100, -100, 26842, -100, -100, -100, -100, -100, -100, -100, -100, -100, 710, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 가운데[UNK][UNK][UNK][UNK]강좌[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 강연을[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 옛[UNK][UNK][UNK][UNK][UNK][UNK]대학교[UNK]학[UNK][UNK][UNK] 경기도[UNK] 옛[UNK][UNK] 문화자원 이야기[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 테마[UNK]를 진행해[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]편[UNK][UNK][UNK][UNK][UNK]길로[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 일[UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 11 ---\n",
      "Pos 11: '[MASK]' was masked, should be '가운데'\n",
      "Pos 16: '[MASK]' was masked, should be '강좌'\n",
      "Pos 46: '[MASK]' was masked, should be '강연을'\n",
      "Pos 62: '[MASK]' was masked, should be '옛'\n",
      "Pos 69: '대학교' was masked, should be '대학교'\n",
      "Pos 71: '[MASK]' was masked, should be '학'\n",
      "Pos 75: '[MASK]' was masked, should be '경기도'\n",
      "Pos 77: '[MASK]' was masked, should be '옛'\n",
      "Pos 80: '[MASK]' was masked, should be '문화'\n",
      "Pos 81: '[MASK]' was masked, should be '자원'\n",
      "Pos 82: '[MASK]' was masked, should be '이야기'\n",
      "Pos 92: '테마' was masked, should be '테마'\n",
      "Pos 94: '[MASK]' was masked, should be '를'\n",
      "Pos 95: '[MASK]' was masked, should be '진행해'\n",
      "Pos 107: '수출에' was masked, should be '편'\n",
      "Pos 113: '[MASK]' was masked, should be '길로'\n",
      "Pos 123: '[MASK]' was masked, should be '일'\n",
      "\n",
      "Sample 12 Input IDs (Original):\n",
      "[2, 8719, 2436, 1, 3140, 6802, 1206, 11680, 9189, 22074, 2470, 3155, 3052, 3020, 4047, 455, 10813, 14101, 25914, 22074, 2470, 3155, 3052, 3020, 8660, 3883, 689, 710, 4, 3574, 1, 2472, 8335, 4, 247, 1092, 1131, 15653, 1, 10420, 1048, 7351, 2004, 1, 4, 2470, 3155, 8538, 2340, 2996, 5588, 3199, 1, 737, 3883, 7028, 1206, 24769, 4962, 2839, 2391, 18253, 7682, 732, 13021, 10813, 12545, 23193, 10813, 14101, 1046, 2080, 15990, 15839, 23013, 3849, 21630, 1, 4047, 25914, 10813, 14101, 1046, 4, 4, 1048, 2197, 20110, 11322, 7579, 1, 2203, 2757, 3140, 29055, 7, 27695, 1175, 180, 1, 3558, 2284, 1, 12262, 7833, 2794, 18250, 1014, 2015, 4, 7549, 2008, 3591, 1, 22074, 585, 2415, 26925, 4, 5660, 13885, 1573, 3360, 4, 1, 7028, 1206, 3]\n",
      "Decoded Input: [CLS] 농식품부 장관[UNK] 미국 대사와 양자 면담 양국의 농업협력 방안 논의 기후 및 식량안보 대응과 양국의 농업협력 방안 논의 농림축산식품부 장관은 월 일[MASK] 오후[UNK] 서울시 중구[MASK] 달개비 콘퍼런스[UNK] 부임을 축하하고[UNK][MASK] 농업협력 방안에 대해 의견을 교환하였음[UNK] 정 장관은 기후변화와 우크라이나 전쟁 등으로 어느 때보다 심각한 전 세계적 식량위기 하에서 식량안보를 위한 국제사회 협력이 중요하다는 점을 강조하며[UNK] 기후 대응과 식량안보를[MASK][MASK]을 함께 해나가자고 했음[UNK] 또한 최근 미국 각지에서 '김치의 날[UNK] 제정되고[UNK] 한류 문화를 통한 한식에 대한[MASK] 증가하고 있는 만큼[UNK] 양국의 식문화 교류와[MASK] 활발해지길 희망[MASK][UNK] 기후변화와[SEP]\n",
      "Sample 12 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 710, 974, -100, -100, -100, -100, 3075, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6410, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 12545, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2080, 8591, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6406, -100, -100, -100, -100, -100, -100, -100, -100, 15839, -100, -100, -100, -100, 3090, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 일 화[UNK][UNK][UNK][UNK] 소재[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 양국[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]위기[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 위한 국제협력[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 관심이[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 협력이[UNK][UNK][UNK][UNK]했음[UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 12 ---\n",
      "Pos 27: '일' was masked, should be '일'\n",
      "Pos 28: '[MASK]' was masked, should be '화'\n",
      "Pos 33: '[MASK]' was masked, should be '소재'\n",
      "Pos 44: '[MASK]' was masked, should be '양국'\n",
      "Pos 66: '위기' was masked, should be '위기'\n",
      "Pos 83: '[MASK]' was masked, should be '위한'\n",
      "Pos 84: '[MASK]' was masked, should be '국제협력'\n",
      "Pos 109: '[MASK]' was masked, should be '관심이'\n",
      "Pos 118: '[MASK]' was masked, should be '협력이'\n",
      "Pos 123: '[MASK]' was masked, should be '했음'\n",
      "\n",
      "Sample 13 Input IDs (Original):\n",
      "[2, 8429, 952, 1107, 1, 2892, 139, 1409, 1027, 2219, 1, 9684, 656, 4, 15219, 7997, 1684, 2536, 1, 10085, 750, 1065, 29644, 1043, 4, 23064, 706, 17043, 1, 15772, 2218, 89, 2219, 1, 15450, 1036, 139, 1334, 4, 139, 1183, 19790, 1011, 1, 4, 643, 703, 16201, 9760, 29557, 1036, 2490, 1, 7595, 20625, 10538, 8995, 1022, 15846, 6736, 872, 15327, 589, 4, 2255, 4223, 4, 4, 1048, 5080, 1, 207, 10127, 1202, 11082, 19031, 1036, 525, 1585, 2284, 15577, 13701, 7595, 707, 1472, 1175, 179, 1036, 22204, 188, 1409, 1049, 4, 2011, 4, 25441, 4, 1, 8429, 13291, 1, 2793, 112, 1007, 1, 27076, 4567, 28399, 1, 18194, 943, 496, 2219, 1, 9684, 943, 1482, 28400, 1011, 1, 207, 20615, 8, 98, 1245, 2954, 23286, 3]\n",
      "Decoded Input: [CLS] 본관은 행주[UNK] 대법 기응세이고[UNK] 아버지는 영[MASK]부사 기자헌이며[UNK] 어머니는 종실 입지를원[MASK] 이정 의 딸이다[UNK] 형이 기준 격이고[UNK] 동생이 기신[MASK] 기민격이다[UNK][MASK] 에 음보로 해남 현감이 되고[UNK] 이듬해 비수도권 장흥 부사로 전직되었으나 탐학이 심[MASK] 하여 정리[MASK][MASK]을 받았다[UNK] 년 인조반정으로 관직이 삭탈되고 유배되었다가 이듬해 이괄의 난이 일어나자 내응할[MASK] 있다[MASK] 사사[MASK][UNK] 본관은 청주[UNK] 자는 광보[UNK] 한수성의 증손으로[UNK] 할아버지는 한 부이고[UNK] 아버지는 한찬 남이다[UNK] 년 광해군 , 계축 월에 진사[SEP]\n",
      "Sample 13 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, 18194, -100, -100, -100, -100, -100, -100, -100, 28595, -100, -100, -100, -100, -100, -100, -100, -100, 941, -100, 1052, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1089, -100, -100, -100, -100, -100, 20615, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1014, -100, -100, -100, -100, -100, 872, -100, -100, 2330, -100, -100, 2460, 11062, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4769, -100, 2255, -100, 2458, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 18194, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK] 할아버지는[UNK][UNK][UNK][UNK][UNK][UNK][UNK]중추[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 하[UNK]군[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]격[UNK][UNK][UNK][UNK][UNK] 광해군[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]에[UNK][UNK][UNK][UNK][UNK] 탐[UNK][UNK]하다[UNK][UNK]사의 탄핵[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 우려가[UNK] 하여[UNK]되었다[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 할아버지는[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 13 ---\n",
      "Pos 5: '대법' was masked, should be '할아버지는'\n",
      "Pos 13: '[MASK]' was masked, should be '중추'\n",
      "Pos 22: '입지를' was masked, should be '하'\n",
      "Pos 24: '[MASK]' was masked, should be '군'\n",
      "Pos 38: '[MASK]' was masked, should be '격'\n",
      "Pos 44: '[MASK]' was masked, should be '광해군'\n",
      "Pos 54: '비수도권' was masked, should be '에'\n",
      "Pos 60: '탐' was masked, should be '탐'\n",
      "Pos 63: '[MASK]' was masked, should be '하다'\n",
      "Pos 66: '[MASK]' was masked, should be '사의'\n",
      "Pos 67: '[MASK]' was masked, should be '탄핵'\n",
      "Pos 92: '[MASK]' was masked, should be '우려가'\n",
      "Pos 94: '[MASK]' was masked, should be '하여'\n",
      "Pos 96: '[MASK]' was masked, should be '되었다'\n",
      "Pos 109: '할아버지는' was masked, should be '할아버지는'\n",
      "\n",
      "Sample 14 Input IDs (Original):\n",
      "[2, 4, 974, 4, 4, 1183, 1200, 139, 5671, 2422, 2373, 1, 3503, 11736, 26330, 1, 8204, 3024, 139, 5671, 1048, 8297, 31049, 20088, 2074, 2194, 689, 2552, 9203, 662, 29008, 139, 5671, 4, 2373, 2375, 4, 8, 61, 689, 710, 974, 4021, 4, 4, 20483, 3503, 3964, 9357, 4, 3539, 4612, 2013, 1, 29008, 139, 23910, 4, 2373, 230, 3262, 4, 30881, 1014, 13873, 4, 2074, 4, 2016, 4, 2006, 373, 2435, 4, 4, 139, 4, 1014, 2015, 4, 1066, 23829, 297, 4691, 3476, 1114, 10966, 8164, 2239, 14288, 1023, 385, 12207, 1, 3242, 3503, 14781, 62, 7101, 2236, 427, 707, 16959, 2337, 3411, 10748, 1, 2337, 2804, 25427, 2239, 14288, 2513, 4, 4, 4, 2013, 1, 4, 5671, 2422, 4794, 4, 3019, 17324, 4, 7867, 3]\n",
      "Decoded Input: [CLS][MASK] 화[MASK][MASK]민선 기 출범 기획 위원회[UNK] 최종 회의가 개최되었다[UNK] 대구광역시장의 기 출범을 앞두고 각계각층의 의견수렴을 위해 지난 월 일부터 운영해 온 민선 기 출범[MASK] 위원회 공동[MASK] , 가 월 일 화 오전[MASK][MASK] 대회의실에서 최종 회의를 개최하고[MASK] 활동을 마무리한다[UNK] 민선 기공연을[MASK] 위원회 는 그동안[MASK] 염원에 부응[MASK] 위해[MASK]적으로[MASK]하여 룩 다양한[MASK][MASK] 기[MASK]에 대한[MASK]적 제언 등 시민의 바람과 희망을 담은 정책제안서 를 만들었다[UNK] 이날 최종 회의에서는 각 분과 위원장 명 이 그동안의 활동 결과를 설명하고[UNK] 활동 내용을 정리한 정책제안서를[MASK][MASK][MASK]한다[UNK][MASK] 출범 기획 위원회는[MASK] 외의 급격한[MASK] 변화에[SEP]\n",
      "Sample 14 Labels (Original):\n",
      "[-100, 9, -100, 20483, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2422, -100, -100, 4241, -100, -100, -100, 710, -100, -100, 584, 4359, -100, -100, -100, -100, 17783, -100, -100, -100, -100, -100, -100, 5671, 2422, -100, -100, -100, 5573, -100, -100, -100, 2053, -100, 21933, -100, 2337, -100, 31049, -100, 2996, 22694, -100, 8046, -100, -100, 2319, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2239, -100, -100, 3387, 9550, 3752, -100, -100, 139, -100, -100, -100, 2423, -100, -100, 2173, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK] .[UNK] 대회의실에서 '[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 기획[UNK][UNK]위원장[UNK][UNK][UNK] 일[UNK][UNK] 시 시청[UNK][UNK][UNK][UNK] 그간의[UNK][UNK][UNK][UNK][UNK][UNK] 출범 기획[UNK][UNK][UNK] 시민들의[UNK][UNK][UNK]하기[UNK] 열정[UNK] 활동[UNK] 각계각층의[UNK] 의견을 수렴하고[UNK] 공약[UNK][UNK] 발전[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 정책[UNK][UNK] 대구시장에게 전달[UNK][UNK] 기[UNK][UNK][UNK] 국내[UNK][UNK] 환경[UNK][UNK]\n",
      "--- Masked Token Check for Sample 14 ---\n",
      "Pos 1: '[MASK]' was masked, should be '.'\n",
      "Pos 3: '[MASK]' was masked, should be '대회의실에서'\n",
      "Pos 4: '[MASK]' was masked, should be '''\n",
      "Pos 33: '[MASK]' was masked, should be '기획'\n",
      "Pos 36: '[MASK]' was masked, should be '위원장'\n",
      "Pos 40: '일' was masked, should be '일'\n",
      "Pos 43: '[MASK]' was masked, should be '시'\n",
      "Pos 44: '[MASK]' was masked, should be '시청'\n",
      "Pos 49: '[MASK]' was masked, should be '그간의'\n",
      "Pos 56: '공연을' was masked, should be '출범'\n",
      "Pos 57: '[MASK]' was masked, should be '기획'\n",
      "Pos 61: '[MASK]' was masked, should be '시민들의'\n",
      "Pos 65: '[MASK]' was masked, should be '하기'\n",
      "Pos 67: '[MASK]' was masked, should be '열정'\n",
      "Pos 69: '[MASK]' was masked, should be '활동'\n",
      "Pos 71: '룩' was masked, should be '각계각층의'\n",
      "Pos 73: '[MASK]' was masked, should be '의견을'\n",
      "Pos 74: '[MASK]' was masked, should be '수렴하고'\n",
      "Pos 76: '[MASK]' was masked, should be '공약'\n",
      "Pos 79: '[MASK]' was masked, should be '발전'\n",
      "Pos 110: '정책' was masked, should be '정책'\n",
      "Pos 113: '[MASK]' was masked, should be '대구시'\n",
      "Pos 114: '[MASK]' was masked, should be '장에게'\n",
      "Pos 115: '[MASK]' was masked, should be '전달'\n",
      "Pos 118: '[MASK]' was masked, should be '기'\n",
      "Pos 122: '[MASK]' was masked, should be '국내'\n",
      "Pos 125: '[MASK]' was masked, should be '환경'\n",
      "\n",
      "Sample 15 Input IDs (Original):\n",
      "[2, 1, 3192, 21684, 12335, 29669, 1017, 9775, 1, 4, 1, 1, 4, 1, 1, 1, 1, 8, 1, 1, 4, 1, 1, 2808, 8155, 3962, 18998, 5408, 455, 2336, 8155, 4890, 16287, 707, 1, 8831, 4, 4216, 1954, 2031, 2522, 21904, 2330, 1, 8932, 1, 3294, 1, 1, 2085, 1, 1, 528, 1, 1, 4, 1055, 3122, 1, 1, 803, 1334, 4, 1, 1, 4720, 2330, 1, 3677, 6411, 475, 5380, 4941, 7027, 3208, 92, 1557, 16851, 2089, 1, 8123, 61, 10765, 4643, 4592, 5343, 85, 10922, 1, 8123, 177, 3208, 23861, 1, 10301, 1064, 2490, 4, 2364, 1, 4, 8096, 12335, 29669, 1017, 9775, 254, 1591, 2330, 4, 8096, 12335, 29669, 1017, 2266, 254, 1591, 4, 254, 1591, 4, 254, 1591, 1012, 254, 1591, 1012, 3]\n",
      "Decoded Input: [CLS][UNK] 구분 대차 품사 형용사 발음[UNK][MASK][UNK][UNK][MASK][UNK][UNK][UNK][UNK] ,[UNK][UNK][MASK][UNK][UNK] 의미 범주 인간 태도 주제 및 상황 범주 심리 문형 이[UNK] 뜻풀이[MASK] 거리낄 것이 없이 당당하다[UNK] 용례[UNK] 마음[UNK][UNK] 사람[UNK][UNK] 삶[UNK][UNK][MASK]면 되지[UNK][UNK] 처신[MASK][UNK][UNK] 행동하다[UNK] 사람들이 나를 범인으로 의심했지만 나는 결백하였기 때문에[UNK] 대화 가 너는 항상 열심히 사는 것 같아[UNK] 대화 나 나는 아이들에게[UNK] 엄마가 되고[MASK]거든[UNK][MASK] 단어 품사 형용사 발음 대견하다[MASK] 단어 품사 형용사 활용 대견[MASK] 대견[MASK] 대견해 대견해[SEP]\n",
      "Sample 15 Labels (Original):\n",
      "[-100, -100, -100, 8096, -100, -100, -100, -100, -100, 2266, -100, -100, 8, -100, -100, -100, -100, 8, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 20426, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 527, -100, -100, -100, -100, -100, -100, 2330, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 592, -100, -100, 3192, -100, -100, -100, -100, -100, -100, -100, -100, 3192, 8096, -100, -100, -100, -100, -100, -100, 1012, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK] 단어[UNK][UNK][UNK][UNK][UNK] 활용[UNK][UNK] ,[UNK][UNK][UNK][UNK] ,[UNK][UNK] ,[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 조금도[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 살[UNK][UNK][UNK][UNK][UNK][UNK]하다[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 싶[UNK][UNK] 구분[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 구분 단어[UNK][UNK][UNK][UNK][UNK][UNK]해[UNK][UNK]해[UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 15 ---\n",
      "Pos 3: '대차' was masked, should be '단어'\n",
      "Pos 9: '[MASK]' was masked, should be '활용'\n",
      "Pos 12: '[MASK]' was masked, should be ','\n",
      "Pos 17: ',' was masked, should be ','\n",
      "Pos 20: '[MASK]' was masked, should be ','\n",
      "Pos 36: '[MASK]' was masked, should be '조금도'\n",
      "Pos 55: '[MASK]' was masked, should be '살'\n",
      "Pos 62: '[MASK]' was masked, should be '하다'\n",
      "Pos 97: '[MASK]' was masked, should be '싶'\n",
      "Pos 100: '[MASK]' was masked, should be '구분'\n",
      "Pos 109: '[MASK]' was masked, should be '구분'\n",
      "Pos 110: '단어' was masked, should be '단어'\n",
      "Pos 117: '[MASK]' was masked, should be '해'\n",
      "Pos 120: '[MASK]' was masked, should be '해'\n",
      "\n",
      "Sample 16 Input IDs (Original):\n",
      "[2, 9978, 5695, 7153, 15170, 1, 4, 24776, 3636, 7642, 2186, 7635, 1, 6642, 297, 74, 4, 119, 8299, 455, 452, 1099, 1200, 7593, 8664, 9978, 5695, 7153, 15170, 4585, 230, 7118, 9727, 17102, 24776, 4, 1313, 4, 4022, 29, 1548, 15795, 16, 16577, 4, 706, 4, 8266, 1022, 9, 9, 9, 4, 710, 6642, 675, 74, 12186, 1048, 4, 4, 3636, 7642, 1048, 20815, 1, 2757, 1, 15754, 1180, 13535, 2001, 19737, 24776, 707, 4, 710, 6805, 2168, 18091, 974, 1207, 80, 1, 6642, 80, 1, 7635, 4, 1, 20930, 80, 297, 2359, 6273, 1106, 2168, 4, 4, 1008, 17015, 4, 17102, 3675, 17064, 5636, 1, 2039, 2555, 13926, 1, 4, 4, 9978, 4, 7153, 15170, 675, 74, 2925, 7153, 15170, 1024, 4, 4, 16557, 455, 3]\n",
      "Decoded Input: [CLS] 함양산림항공관리소[UNK][MASK] ASF 항공방역 실시 파주[UNK] 연천 등 개[MASK] 군 DMZ 및 민통선 일원 산림청 함양산림항공관리소 소장 는 아프리카 돼지열병 ASF[MASK]f[MASK]an Swine Fev[MASK] 의[MASK] 검출로 . . .[MASK] 일 연천 외 개 읍면동을[MASK][MASK] 항공방역을 실시함[UNK] 최근[UNK] 폐사체 개체에서 아프리카돼지열병 ASF 이[MASK] 일 되었고 발생 실적은 화천 건[UNK] 연천 건[UNK] 파주[MASK][UNK] 철원 건 등 모두 건이며 발생[MASK][MASK]아프리카[MASK]열병 표준행동지침[UNK] 따라 처리됐음[UNK][MASK][MASK] 함양[MASK]항공관리소 외 개 산림항공관리소는[MASK][MASK]지대 및[SEP]\n",
      "Sample 16 Labels (Original):\n",
      "[-100, -100, -100, -100, -100, -100, 19737, -100, -100, -100, -100, -100, -100, -100, -100, -100, 584, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11, -100, 29455, -100, -100, -100, -100, -100, -100, 3413, -100, 4965, -100, -100, -100, -100, -100, 9, -100, -100, -100, -100, -100, -100, 2628, 3506, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19737, -100, -100, 8266, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 80, -100, -100, -100, -100, -100, -100, -100, -100, 18091, 6, -100, -100, 11169, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2531, 2039, -100, 5695, -100, -100, -100, 74, -100, -100, -100, -100, 506, 29185, -100, -100, -100]\n",
      "Decoded Labels (masked tokens only): [UNK][UNK][UNK][UNK][UNK][UNK] 아프리카돼지열병[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 시[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] A[UNK]ric[UNK][UNK][UNK][UNK][UNK][UNK]er[UNK] 지속적인[UNK][UNK][UNK][UNK][UNK] .[UNK][UNK][UNK][UNK][UNK][UNK] 대상으로 긴급[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 아프리카돼지열병[UNK][UNK] 검출[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 건[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 실적은 \"[UNK][UNK]돼지[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK] 이에 따라[UNK]산림[UNK][UNK][UNK] 개[UNK][UNK][UNK][UNK] 비무장[UNK][UNK][UNK]\n",
      "--- Masked Token Check for Sample 16 ---\n",
      "Pos 6: '[MASK]' was masked, should be '아프리카돼지열병'\n",
      "Pos 16: '[MASK]' was masked, should be '시'\n",
      "Pos 35: '[MASK]' was masked, should be 'A'\n",
      "Pos 37: '[MASK]' was masked, should be 'ric'\n",
      "Pos 44: '[MASK]' was masked, should be 'er'\n",
      "Pos 46: '[MASK]' was masked, should be '지속적인'\n",
      "Pos 52: '[MASK]' was masked, should be '.'\n",
      "Pos 59: '[MASK]' was masked, should be '대상으로'\n",
      "Pos 60: '[MASK]' was masked, should be '긴급'\n",
      "Pos 72: '아프리카돼지열병' was masked, should be '아프리카돼지열병'\n",
      "Pos 75: '[MASK]' was masked, should be '검출'\n",
      "Pos 88: '[MASK]' was masked, should be '건'\n",
      "Pos 97: '[MASK]' was masked, should be '실적은'\n",
      "Pos 98: '[MASK]' was masked, should be '\"'\n",
      "Pos 101: '[MASK]' was masked, should be '돼지'\n",
      "Pos 111: '[MASK]' was masked, should be '이에'\n",
      "Pos 112: '[MASK]' was masked, should be '따라'\n",
      "Pos 114: '[MASK]' was masked, should be '산림'\n",
      "Pos 118: '개' was masked, should be '개'\n",
      "Pos 123: '[MASK]' was masked, should be '비'\n",
      "Pos 124: '[MASK]' was masked, should be '무장'\n",
      "\n",
      "--- 디코딩 확인 완료 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 데이터셋 토큰화 및 디코딩 확인 ---\")\n",
    "\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    for j in range(input_ids.shape[0]):\n",
    "        print(f\"\\nSample {j+1} Input IDs (Original):\")\n",
    "        print(input_ids[j].tolist())\n",
    "        decoded_input = tokenizer.decode(input_ids[j].tolist())\n",
    "        print(f\"Decoded Input: {decoded_input}\")\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "        print(f\"Sample {j+1} Labels (Original):\")\n",
    "        print(labels[j].tolist())\n",
    "        \n",
    "        decoded_label = tokenizer.decode(labels[j].tolist())\n",
    "        print(f\"Decoded Labels (masked tokens only): {decoded_label}\")\n",
    "        \n",
    "        print(f\"--- Masked Token Check for Sample {j+1} ---\")\n",
    "        input_ids_list = input_ids[j].tolist()\n",
    "        labels_list = labels[j].tolist()\n",
    "        \n",
    "        masked_positions_and_labels = []\n",
    "        for k, (input_id, label_id) in enumerate(zip(input_ids_list, labels_list)):\n",
    "            if label_id != -100:\n",
    "                masked_token = tokenizer.decode([input_id])\n",
    "                original_token = tokenizer.decode([label_id])\n",
    "                masked_positions_and_labels.append(f\"Pos {k}: '{masked_token}' was masked, should be '{original_token}'\")\n",
    "        \n",
    "        if masked_positions_and_labels:\n",
    "            for item in masked_positions_and_labels:\n",
    "                print(item)\n",
    "        else:\n",
    "            print(\"No masked tokens with labels found in this sample (all -100).\")\n",
    "\n",
    "print(\"\\n--- 디코딩 확인 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c4dab4-1e38-41a9-99c5-c25c081c1e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치 로드 중...\n",
      "모델 가중치 로드 완료.\n",
      "Custom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : 110946560\n",
      "모델이 담긴 장치 : cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_SAVE_PATH = \"saves/Pretrain.pt\"\n",
    "\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_HIDDEN_LAYERS = 12\n",
    "NUM_ATTENTION_HEADS = 12\n",
    "INTERMEDIATE_SIZE = 3072\n",
    "TYPE_VOCAB_SIZE = 2\n",
    "DROPOUT_PROB = 0.1\n",
    "\n",
    "config = CustomBertConfig(\n",
    "    VOCAB_SIZE=VOCAB_SIZE,\n",
    "    HIDDEN_SIZE=HIDDEN_SIZE,\n",
    "    NUM_HIDDEN_LAYERS=NUM_HIDDEN_LAYERS,\n",
    "    NUM_ATTENTION_HEADS=NUM_ATTENTION_HEADS,\n",
    "    INTERMEDIATE_SIZE=INTERMEDIATE_SIZE,\n",
    "    MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH,\n",
    "    TYPE_VOCAB_SIZE=TYPE_VOCAB_SIZE,\n",
    "    DROPOUT_PROB=DROPOUT_PROB\n",
    ")\n",
    "\n",
    "model = CustomBertForMaskedLM(config)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(\"모델 가중치 로드 중...\")\n",
    "    # 먼저 CPU에 로드한 후 모델에 로드합니다.\n",
    "    loaded_state_dict = torch.load(MODEL_SAVE_PATH, map_location='cpu')\n",
    "    model.load_state_dict(loaded_state_dict)\n",
    "    print(\"모델 가중치 로드 완료.\")\n",
    "else:\n",
    "    print(\"새로운 모델 초기화 완료. 저장된 가중치를 찾을 수 없습니다.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Custom Bert 모델 초기화 완료. 총 학습 가능 파라미터 수 : {num_params}')\n",
    "print(f'모델이 담긴 장치 : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e11127-8e27-444b-b211-584a36e3dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 학습 스텝 수: 7600\n",
      "워밍업 스텝 수: 760\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.1\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "WARMUP_STEPS = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,num_training_steps=total_steps)\n",
    "print(f\"총 학습 스텝 수: {total_steps}\")\n",
    "print(f\"워밍업 스텝 수: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "555ef7de-acda-461d-9bb6-e2c8fa52d1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 최고 검증 정확도: 0.0137 불러옴.\n",
      "\n",
      "<--- 학습 시작 ---> (50 에폭)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5328\\2665029642.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f026ffa6d55843958401585a898ac767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-train Epoch 1:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5328\\2665029642.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     45\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m scaler.update()\n\u001b[32m     49\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "LOSS_PATH = f\"saves/Pretrain_loss.txt\"\n",
    "BEST_ACCURACY_PATH = f\"saves/Pretrain_best_accuracy.txt\"\n",
    "best_val_accuracy = 0.0\n",
    "if os.path.exists(BEST_ACCURACY_PATH):\n",
    "    with open(BEST_ACCURACY_PATH, 'r') as f:\n",
    "        content = f.read()\n",
    "        if content.strip():\n",
    "            best_val_accuracy = float(content.strip())\n",
    "            print(f\"이전 최고 검증 정확도: {best_val_accuracy:.4f} 불러옴.\")\n",
    "\n",
    "if os.path.exists(LOSS_PATH):\n",
    "    with open(LOSS_PATH,'r') as f:\n",
    "        content = f.read()\n",
    "        if content.strip():\n",
    "            prev_loss = float(content.strip())\n",
    "else:\n",
    "    prev_loss = 100\n",
    "\n",
    "train_losses = []\n",
    "train_accuarcy = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(f\"\\n<--- 학습 시작 ---> ({EPOCHS} 에폭)\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Pre-train Epoch {e+1}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f\"{(loss_sum/(step+1)):.4f}\"})\n",
    "\n",
    "        del outputs, loss\n",
    "        if 'cuda' in str(device):\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    avg_train_loss = loss_sum / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Pre-train Epoch {e+1} 완료. 평균 학습 손실: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    val_progressbar = tqdm(val_dataloader, desc=f\"Pre-train Epoch {e+1} Valid\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_progressbar):        \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    token_type_ids=batch[\"token_type_ids\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                loss = outputs[\"loss\"]\n",
    "                logits = outputs[\"logits\"]\n",
    "                \n",
    "            total_val_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            active_labels = batch[\"labels\"].view(-1)\n",
    "            active_predictions = predictions.view(-1)\n",
    "\n",
    "            mask = (active_labels != -100)\n",
    "\n",
    "            correct_predictions += (active_predictions[mask] == active_labels[mask]).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "\n",
    "            val_progressbar.set_postfix({'val_loss': f\"{(total_val_loss/(step+1)):.4f}\"})\n",
    "\n",
    "            del outputs, loss, logits, predictions, active_labels, active_predictions, mask # 메모리 해제\n",
    "            if 'cuda' in str(device):\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "        if step % 100 == 0: \n",
    "            print(f\"\\n--- Batch {step} Debug Info ---\")\n",
    "            print(\"predictions sample (masked):\", predictions.view(-1)[mask][:10]) # 예측값 샘플\n",
    "            print(\"active_labels sample (masked):\", active_labels[mask][:10])   # 실제 레이블 샘플\n",
    "            print(\"total_predictions for this batch:\", mask.sum().item()) # 이 배치에서 계산에 포함된 토큰 수\n",
    "            print(\"---------------------------------\")\n",
    "        train_accuarcy.append(val_accuracy)\n",
    "        print(f\"Pre-train Epoch {e+1} 완료. 평균 검증 손실: {avg_val_loss:.4f}, 검증 정확도: {(val_accuracy*100):.2f}%\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(f\"새로운 최고 검증 정확도 {(best_val_accuracy*100):.2f}% 달성! 모델 가중치 '{MODEL_SAVE_PATH}' 저장 완료.\")\n",
    "            \n",
    "            with open(BEST_ACCURACY_PATH, 'w') as f:\n",
    "                f.write(str(best_val_accuracy))\n",
    "            print(f\"최고 검증 정확도 '{BEST_ACCURACY_PATH}' 저장 완료.\")\n",
    "        else:\n",
    "            print(f\"현재 검증 정확도 ({(val_accuracy*100):.2f}%)는 최고 정확도 ({(best_val_accuracy*100):.2f}%)보다 낮습니다. 모델을 저장하지 않습니다.\")\n",
    "\n",
    "print(\"\\n<--- 학습 완료 --->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608baac4-d751-49b3-acad-39f2636c7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Pre-training Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Training Loss')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, EPOCHS + 1)) # x축 눈금 에폭 수에 맞춰 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851cb92a-fcfa-4bd9-82cd-08fce423ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, EPOCHS + 1), train_accuarcy, marker='o', linestyle='-', color='b')\n",
    "plt.title('Pre-training Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Training Accuarcy')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, EPOCHS + 1)) # x축 눈금 에폭 수에 맞춰 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98a5ff-7954-4736-856d-dc40f1873ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19d72d-cdc9-47c8-a594-c72ff993c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab15d5e-a71b-47df-ba60-34b68c1ca6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
