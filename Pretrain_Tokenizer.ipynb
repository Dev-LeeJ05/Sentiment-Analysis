{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c601ae3-68e0-4999-ada1-5a05b028ef3f",
   "metadata": {},
   "source": [
    "# 한국어 위키백과 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e7768c-fb54-4365-857d-fcf1745dc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import logging\n",
    "\n",
    "datasetsFilePath = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33de0c63-b04c-4d4d-8ba5-94f6b48b3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess_kor_for_wordpiece(text : str) :\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    target = r\"[^가-힣0-9a-zA-Z.,!?'\\\" ]\"\n",
    "    text = re.sub(target, repl=\" \", string=text)\n",
    "\n",
    "    text = re.sub(r'([.,!?\"\\'])(\\1{1,})', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1+', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", repl=\" \", string=text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fc6096-097b-4a46-8eca-4cf8c31fe657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 12:24:22,732 : INFO : 'datasets/extracted_wiki_text_gensim' 디렉토리에서 텍스트 전처리 시작\n",
      "2025-06-25 12:24:22,733 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00000.txt' 파일 전처리 중\n",
      "100000it [00:24, 4110.02it/s]\n",
      "2025-06-25 12:24:47,105 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00000.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:24:47,105 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00001.txt' 파일 전처리 중\n",
      "100000it [00:15, 6534.24it/s]\n",
      "2025-06-25 12:25:02,435 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00001.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:02,435 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00002.txt' 파일 전처리 중\n",
      "100000it [00:11, 8658.31it/s]\n",
      "2025-06-25 12:25:14,005 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00002.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:14,005 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00003.txt' 파일 전처리 중\n",
      "100000it [00:09, 10175.91it/s]\n",
      "2025-06-25 12:25:23,846 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00003.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:23,846 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00004.txt' 파일 전처리 중\n",
      "100000it [00:09, 10059.00it/s]\n",
      "2025-06-25 12:25:33,806 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00004.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:33,806 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00005.txt' 파일 전처리 중\n",
      "100000it [00:09, 10184.47it/s]\n",
      "2025-06-25 12:25:43,641 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00005.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:43,642 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00006.txt' 파일 전처리 중\n",
      "100000it [00:09, 10517.25it/s]\n",
      "2025-06-25 12:25:53,163 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00006.txt' 전처리 완료. 100000줄 저장됨.\n",
      "2025-06-25 12:25:53,164 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00007.txt' 파일 전처리 중\n",
      "2964it [00:00, 13804.02it/s]\n",
      "2025-06-25 12:25:53,380 : INFO : 'datasets/extracted_wiki_text_gensim\\wiki_text_00007.txt' 전처리 완료. 2964줄 저장됨.\n",
      "2025-06-25 12:25:53,381 : INFO : \n",
      "--- 텍스트 전처리 완료 ---\n",
      "2025-06-25 12:25:53,381 : INFO : 총 8개 파일에서 702964줄이 전처리되어 'datasets/preprocess_wiki_text'에 저장됨.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "input_exracted_dir = f'{datasetsFilePath}extracted_wiki_text_gensim'\n",
    "output_preprocessed_dir = f'{datasetsFilePath}preprocess_wiki_text'\n",
    "\n",
    "if not os.path.exists(output_preprocessed_dir):\n",
    "    os.makedirs(output_preprocessed_dir)\n",
    "\n",
    "logging.info(f\"'{input_exracted_dir}' 디렉토리에서 텍스트 전처리 시작\")\n",
    "\n",
    "total_processed_files = 0\n",
    "total_processed_lines = 0\n",
    "\n",
    "for root, _, files in os.walk(input_exracted_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            input_file_path = os.path.join(root,filename)\n",
    "            output_file_path = os.path.join(output_preprocessed_dir,filename)\n",
    "\n",
    "            logging.info(f\"'{input_file_path}' 파일 전처리 중\")\n",
    "            processed_lines_in_file = 0\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "                 open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                for line in tqdm(infile):\n",
    "                    preprocessed_line = text_preprocess_kor_for_wordpiece(line.strip())\n",
    "                    if preprocessed_line:\n",
    "                        outfile.write(preprocessed_line+'\\n')\n",
    "                        processed_lines_in_file += 1\n",
    "            total_processed_files += 1\n",
    "            total_processed_lines += processed_lines_in_file\n",
    "            logging.info(f\"'{input_file_path}' 전처리 완료. {processed_lines_in_file}줄 저장됨.\")\n",
    "logging.info(f\"\\n--- 텍스트 전처리 완료 ---\")\n",
    "logging.info(f\"총 {total_processed_files}개 파일에서 {total_processed_lines}줄이 전처리되어 '{output_preprocessed_dir}'에 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a455931-08d2-4367-af0e-c15c876c90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 12:25:53,390 : INFO : 총 8개의 전처리된 파일을 사용하여 WordPiece 어휘집 학습 시작...\n",
      "2025-06-25 12:29:16,004 : INFO : WordPiece 어휘집 학습 완료. 'datasets/custom_tokenizer_output/vocab.txt'에 저장되었습니다.\n",
      "2025-06-25 12:29:16,009 : INFO : 생성된 어휘집 크기: 32000\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "preprocessed_text_dir = f'{datasetsFilePath}preprocessed_wiki_text'\n",
    "output_tokenizer_dir = f'{datasetsFilePath}custom_tokenizer_output' \n",
    "\n",
    "if not os.path.exists(output_tokenizer_dir):\n",
    "    os.makedirs(output_tokenizer_dir)\n",
    "\n",
    "files = [os.path.join(preprocessed_text_dir, f) for f in os.listdir(preprocessed_text_dir) if f.endswith('.txt')]\n",
    "\n",
    "if not files:\n",
    "    logging.error(f\"'{preprocessed_text_dir}' 디렉토리에 전처리된 텍스트 파일이 없습니다. 1단계 전처리 단계를 먼저 완료하세요.\")\n",
    "else:\n",
    "    logging.info(f\"총 {len(files)}개의 전처리된 파일을 사용하여 WordPiece 어휘집 학습 시작...\")\n",
    "\n",
    "    tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=False,\n",
    "        handle_chinese_chars=False,\n",
    "        strip_accents=False,\n",
    "        lowercase=False\n",
    "    )\n",
    "\n",
    "    tokenizer.train(\n",
    "        files,\n",
    "        vocab_size=32000,\n",
    "        min_frequency=5,\n",
    "        show_progress=True,\n",
    "        special_tokens=[\n",
    "            \"[PAD]\",\n",
    "            \"[UNK]\",\n",
    "            \"[CLS]\", \n",
    "            \"[SEP]\",\n",
    "            \"[MASK]\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tokenizer.save_model(output_tokenizer_dir)\n",
    "\n",
    "    logging.info(f\"WordPiece 어휘집 학습 완료. '{output_tokenizer_dir}/vocab.txt'에 저장되었습니다.\")\n",
    "    logging.info(f\"생성된 어휘집 크기: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef5226-ed2d-4a40-8c4b-5c13ec1df1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
