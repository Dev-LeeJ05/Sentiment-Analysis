{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c601ae3-68e0-4999-ada1-5a05b028ef3f",
   "metadata": {},
   "source": [
    "# 한국어 위키백과 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e7768c-fb54-4365-857d-fcf1745dc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import logging\n",
    "\n",
    "datasetsFilePath = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33de0c63-b04c-4d4d-8ba5-94f6b48b3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess_kor_for_wordpiece(text : str) :\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    target = r\"[^가-힣0-9a-zA-Z.,!?'\\\" ]\"\n",
    "    text = re.sub(target, repl=\" \", string=text)\n",
    "\n",
    "    text = re.sub(r'([.,!?\"\\'])(\\1{1,})', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1+', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", repl=\" \", string=text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fc6096-097b-4a46-8eca-4cf8c31fe657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 22:52:18,779 : INFO : 'datasets/extracted_wiki_text_gensim' 디렉토리에서 텍스트 전처리 시작\n",
      "2025-07-04 22:52:18,781 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_액티벤처.txt' 파일 전처리 중\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7f49547fb34e25bc9cd648e868de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 22:52:18,910 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_액티벤처.txt' 전처리 완료. 9881줄 저장됨.\n",
      "2025-07-04 22:52:18,910 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_엘지유플러스.txt' 파일 전처리 중\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710c5295f45443f2a87b6e8b5f06e12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 22:52:20,001 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_엘지유플러스.txt' 전처리 완료. 134189줄 저장됨.\n",
      "2025-07-04 22:52:20,001 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_하나카드.txt' 파일 전처리 중\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f76a9c33e2b4f50842082038c66c7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 22:52:21,759 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_하나카드.txt' 전처리 완료. 216609줄 저장됨.\n",
      "2025-07-04 22:52:21,759 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_한국어말뭉치데이터_구어체.txt' 파일 전처리 중\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79d4cadbc154ed2bb8737fcb764c53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 23:00:14,733 : INFO : 'datasets/extracted_wiki_text_gensim\\TS_한국어말뭉치데이터_구어체.txt' 전처리 완료. 28730474줄 저장됨.\n",
      "2025-07-04 23:00:14,733 : INFO : \n",
      "--- 텍스트 전처리 완료 ---\n",
      "2025-07-04 23:00:14,733 : INFO : 총 4개 파일에서 29091153줄이 전처리되어 'datasets/preprocess_wiki_text'에 저장됨.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "input_exracted_dir = f'{datasetsFilePath}extracted_wiki_text_gensim'\n",
    "output_preprocessed_dir = f'{datasetsFilePath}preprocess_wiki_text'\n",
    "\n",
    "if not os.path.exists(output_preprocessed_dir):\n",
    "    os.makedirs(output_preprocessed_dir)\n",
    "\n",
    "logging.info(f\"'{input_exracted_dir}' 디렉토리에서 텍스트 전처리 시작\")\n",
    "\n",
    "total_processed_files = 0\n",
    "total_processed_lines = 0\n",
    "\n",
    "for root, _, files in os.walk(input_exracted_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            input_file_path = os.path.join(root,filename)\n",
    "            output_file_path = os.path.join(output_preprocessed_dir,filename)\n",
    "\n",
    "            logging.info(f\"'{input_file_path}' 파일 전처리 중\")\n",
    "            processed_lines_in_file = 0\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "                 open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                progress_bar = tqdm(infile)\n",
    "                for line in progress_bar:\n",
    "                    preprocessed_line = text_preprocess_kor_for_wordpiece(line.strip())\n",
    "                    if preprocessed_line:\n",
    "                        outfile.write(preprocessed_line+'\\n')\n",
    "                        processed_lines_in_file += 1\n",
    "            total_processed_files += 1\n",
    "            total_processed_lines += processed_lines_in_file\n",
    "            logging.info(f\"'{input_file_path}' 전처리 완료. {processed_lines_in_file}줄 저장됨.\")\n",
    "logging.info(f\"\\n--- 텍스트 전처리 완료 ---\")\n",
    "logging.info(f\"총 {total_processed_files}개 파일에서 {total_processed_lines}줄이 전처리되어 '{output_preprocessed_dir}'에 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a455931-08d2-4367-af0e-c15c876c90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 23:01:26,188 : INFO : 총 4개의 전처리된 파일을 사용하여 WordPiece 어휘집 학습 시작...\n",
      "2025-07-04 23:04:07,029 : INFO : WordPiece 어휘집 학습 완료. 'datasets/custom_tokenizer_output/vocab.txt'에 저장되었습니다.\n",
      "2025-07-04 23:04:07,035 : INFO : 생성된 어휘집 크기: 32000\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "preprocessed_text_dir = f'{datasetsFilePath}preprocess_wiki_text'\n",
    "output_tokenizer_dir = f'{datasetsFilePath}custom_tokenizer_output' \n",
    "\n",
    "if not os.path.exists(output_tokenizer_dir):\n",
    "    os.makedirs(output_tokenizer_dir)\n",
    "\n",
    "files = [os.path.join(preprocessed_text_dir, f) for f in os.listdir(preprocessed_text_dir) if f.endswith('.txt')]\n",
    "\n",
    "if not files:\n",
    "    logging.error(f\"'{preprocessed_text_dir}' 디렉토리에 전처리된 텍스트 파일이 없습니다. 1단계 전처리 단계를 먼저 완료하세요.\")\n",
    "else:\n",
    "    logging.info(f\"총 {len(files)}개의 전처리된 파일을 사용하여 WordPiece 어휘집 학습 시작...\")\n",
    "\n",
    "    tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=False,\n",
    "        strip_accents=False,\n",
    "        lowercase=False\n",
    "    )\n",
    "\n",
    "    tokenizer.train(\n",
    "        files,\n",
    "        vocab_size=32000,\n",
    "        min_frequency=5,\n",
    "        show_progress=True,\n",
    "        special_tokens=[\n",
    "            \"[PAD]\",\n",
    "            \"[UNK]\",\n",
    "            \"[CLS]\", \n",
    "            \"[SEP]\",\n",
    "            \"[MASK]\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tokenizer.save_model(output_tokenizer_dir)\n",
    "\n",
    "    logging.info(f\"WordPiece 어휘집 학습 완료. '{output_tokenizer_dir}/vocab.txt'에 저장되었습니다.\")\n",
    "    logging.info(f\"생성된 어휘집 크기: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef5226-ed2d-4a40-8c4b-5c13ec1df1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68ad33-7346-4b4f-b860-06c39815a1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
