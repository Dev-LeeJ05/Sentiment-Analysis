{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86d9f44-871e-4803-87d5-d2100f9ca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import VocabBuilder\n",
    "from WordPieceTokenizer import WordPieceTokenizer\n",
    "\n",
    "datasetsFilePath = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760c2143-7e1a-4a08-bfd6-16de644a5488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>str_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>불안</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>불안</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>불안</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>불안</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>불안</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         발화  감정  str_len\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??  불안       24\n",
       "1              그냥 내 느낌일뿐겠지?  불안       12\n",
       "2            아직너무초기라서 그런거죠?  불안       14\n",
       "3             유치원버스 사고 낫다던데  불안       13\n",
       "4               근데 원래이런거맞나요  불안       11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{datasetsFilePath}sentiment_data.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3470f15-571f-425c-b595-ea01272c1e04",
   "metadata": {},
   "source": [
    "# 한글 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad04804-8055-42aa-b29a-cf4fcf55d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess_kor_for_wordpiece(text : str) :\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    target = r\"[^가-힣0-9a-zA-Z.,!?'\\\" ]\"\n",
    "    text = re.sub(target, repl=\" \", string=text)\n",
    "\n",
    "    text = re.sub(r'([.,!?\"\\'])(\\1{1,})', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1+', r'\\1', text)\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", repl=\" \", string=text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11fbd47-203d-41ef-9cd6-578726b15e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d7107887ff4e5baefb4dcfaba9abe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m text = df.iloc[i,\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m text = text_preprocess_kor_for_wordpiece(text)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    910\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1939\u001b[39m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[32m   1940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[32m   1941\u001b[39m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_single_block(indexer, value, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\indexing.py:2035\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_split_path\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   2032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2033\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m   2034\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[32m-> \u001b[39m\u001b[32m2035\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\indexing.py:2175\u001b[39m, in \u001b[36m_iLocIndexer._setitem_single_column\u001b[39m\u001b[34m(self, loc, value, plane_indexer)\u001b[39m\n\u001b[32m   2165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype == np.void:\n\u001b[32m   2166\u001b[39m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[32m   2167\u001b[39m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2170\u001b[39m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[32m   2171\u001b[39m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28mself\u001b[39m.obj.iloc[:, loc] = construct_1d_array_from_inferred_fill_value(\n\u001b[32m   2173\u001b[39m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   2174\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2175\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[38;5;28mself\u001b[39m.obj._clear_item_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1338\u001b[39m, in \u001b[36mBlockManager.column_setitem\u001b[39m\u001b[34m(self, loc, idx, value, inplace_only)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1337\u001b[39m     new_mgr = col_mgr.setitem((idx,), value)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_block\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_to_warn:\n\u001b[32m   1341\u001b[39m     warnings.warn(\n\u001b[32m   1342\u001b[39m         COW_WARNING_GENERAL_MSG,\n\u001b[32m   1343\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1344\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1345\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1149\u001b[39m, in \u001b[36mBlockManager.iset\u001b[39m\u001b[34m(self, loc, value, inplace, refs)\u001b[39m\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1148\u001b[39m         blk.set_inplace(blk_locs, value_getitem(val_locs))\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df))):\n",
    "    text = df.iloc[i,0]\n",
    "    text = text_preprocess_kor_for_wordpiece(text)\n",
    "    df.iloc[i,0] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b5ea0-8218-4a1e-91c2-b561767a3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'{datasetsFilePath}sentiment_data.txt'\n",
    "VocabBuilder.SaveDataFrameTextsTo_txt(df,\"발화\",file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a0279-3f21-40d7-aafc-44770133f5f9",
   "metadata": {},
   "source": [
    "# BertWordPieceTokenizer를 사용하여 vocab.txt 생성 후 사용\n",
    "'datasets/sentiment_vocab/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf3f98-d785-42da-8271-e8d33993cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file]\n",
    "vocab_size = 32000\n",
    "min_frequency = 5\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "output_dir = f'{datasetsFilePath}sentiment_vocab'\n",
    "VocabBuilder.GenerateVocab(files,vocab_size,min_frequency,special_tokens,output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a8317-9e7f-4e7b-b3e9-a872658712b7",
   "metadata": {},
   "source": [
    "## Pretrain에서 사용한 vocab.txt 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4aad3a4-ff59-4b85-b6cc-c33482f68060",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file_path = f'saves/vocab.txt'\n",
    "tokenizer = WordPieceTokenizer(vocab_file_path, do_lower_case=False, strip_accents=False,clean_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e2a48b-7162-4165-a401-94523605ac79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d2152709c74f66bc98b8b87361ddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>str_len</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>token_type_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 17637 15450 2000 11814 1086 3628 29262 3715 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>24</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 2916 188 5325 1233 1125 22353 1 3 0 0 0 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>12</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 2785 1148 1373 1425 1029 6671 2020 1073 1324...</td>\n",
       "      <td>불안</td>\n",
       "      <td>14</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 6916 3859 2747 183 1011 15973 3 0 0 0 0 0 0 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>13</td>\n",
       "      <td>1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 5315 4113 1036 1627 1073 1762 3018 3 0 0 0 0...</td>\n",
       "      <td>불안</td>\n",
       "      <td>11</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  발화  감정  str_len  \\\n",
       "0  2 17637 15450 2000 11814 1086 3628 29262 3715 ...  불안       24   \n",
       "1  2 2916 188 5325 1233 1125 22353 1 3 0 0 0 0 0 ...  불안       12   \n",
       "2  2 2785 1148 1373 1425 1029 6671 2020 1073 1324...  불안       14   \n",
       "3  2 6916 3859 2747 183 1011 15973 3 0 0 0 0 0 0 ...  불안       13   \n",
       "4  2 5315 4113 1036 1627 1073 1762 3018 3 0 0 0 0...  불안       11   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "1  1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "2  1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "3  1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "4  1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "\n",
       "                                      token_type_ids  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 128\n",
    "attention_masks = []\n",
    "token_type_ids_ = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    text_to_encode = df.iloc[i,0]\n",
    "    encoded_result = tokenizer.encode(text_to_encode, max_length=max_length)\n",
    "    input_ids = \" \".join(map(str,encoded_result['input_ids']))\n",
    "    attention_mask = \" \".join(map(str,encoded_result['attention_mask']))\n",
    "    token_type_ids = \" \".join(map(str,encoded_result['token_type_ids']))\n",
    "    df.iloc[i,0] = input_ids\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids_.append(token_type_ids)\n",
    "\n",
    "df['attention_mask'] = attention_masks\n",
    "df['token_type_ids'] = token_type_ids_ \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13543c69-8693-43d2-91cf-11af0f258151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{datasetsFilePath}sentiment_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5019af9-b1ac-46ce-a043-2f17a4bd5561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
