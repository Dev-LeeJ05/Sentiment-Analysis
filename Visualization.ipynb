{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286b1739-55b8-4f72-863f-953dde0f0157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TracingCheckError",
     "evalue": "Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.Model.Transformer,\n\t\t        %input_ids : Tensor,\n\t\t        %attention_mask : Tensor):\n\t\t    %fc_out : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t    %dropout : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%self.1)\n\t\t    %transformer_encoder : __torch__.torch.nn.modules.transformer.TransformerEncoder = prim::GetAttr[name=\"transformer_encoder\"](%self.1)\n\t\t    %pos_encoder : __torch__.Model.PositionalEncoding = prim::GetAttr[name=\"pos_encoder\"](%self.1)\n\t\t    %embedding : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"embedding\"](%self.1)\n\t\t    %28 : bool = prim::Constant[value=0](), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t    %29 : int = prim::Constant[value=0](), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t-   %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%embedding)\n\t\t?           ^^\n\t\t+   %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%embedding)\n\t\t?           ^\n\t\t-   %x : Tensor = aten::embedding(%weight.19, %input_ids, %29, %28, %28), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t?                                         ^^\n\t\t+   %x : Tensor = aten::embedding(%weight.3, %input_ids, %29, %28, %28), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t?                                         ^\n\t\t    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t    %33 : bool = prim::Constant[value=0](), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t    %34 : int = prim::Constant[value=2](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %35 : int = prim::Constant[value=9223372036854775807](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %36 : int = prim::Constant[value=0](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %37 : int = prim::Constant[value=1](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %pe : Tensor = prim::GetAttr[name=\"pe\"](%pos_encoder)\n\t\t    %39 : int = aten::size(%x, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %40 : Tensor = aten::slice(%pe, %36, %36, %35, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %41 : Tensor = aten::slice(%40, %37, %36, %39, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %42 : Tensor = aten::slice(%41, %34, %36, %35, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %input.1 : Tensor = aten::add(%x, %42, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t-   %src : Tensor = aten::dropout(%input.1, %32, %33), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?       ^^\n\t\t+   %src.1 : Tensor = aten::dropout(%input.1, %32, %33), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?       ^^^^\n\t\t    %10 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:48:0\n\t\t    %mask : Tensor = aten::eq(%attention_mask, %10) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:48:0\n\t\t+   %45 : float = prim::Constant[value=0.](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %46 : int = prim::Constant[value=2](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %47 : int = prim::Constant[value=1](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %48 : int = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %49 : int = prim::Constant[value=256](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %50 : int = prim::Constant[value=8](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %51 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t-   %45 : float = prim::Constant[value=0.](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %46 : int = prim::Constant[value=8](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %47 : int = prim::Constant[value=-2](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %48 : int = prim::Constant[value=3](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %49 : int = prim::Constant[value=-1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %50 : str = prim::Constant[value=\"trunc\"](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %51 : Tensor = prim::Constant[value={8}](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %52 : int = prim::Constant[value=2](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %53 : int = prim::Constant[value=0](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %54 : int = prim::Constant[value=1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %55 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %56 : bool = prim::Constant[value=1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %57 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %58 : int = prim::Constant[value=256](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %59 : float = prim::Constant[value=-inf](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?     ^\n\t\t+   %52 : float = prim::Constant[value=-inf](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?     ^\n\t\t-   %60 : bool = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t+   %53 : bool = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t-   %61 : NoneType = prim::Constant(), scope: __module.transformer_encoder\n\t\t?    ^^\n\t\t+   %54 : NoneType = prim::Constant(), scope: __module.transformer_encoder\n\t\t?    ^^\n\t\t-   %62 : int = prim::Constant[value=6](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t+   %55 : int = prim::Constant[value=6](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t    %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_3 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"3\"](%layers)\n\t\t    %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_2 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"2\"](%layers.5)\n\t\t    %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_1 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"1\"](%layers.3)\n\t\t    %layers.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_0 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"0\"](%layers.1)\n\t\t-   %71 : Tensor = aten::zeros_like(%mask, %62, %61, %61, %60, %61), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^                                     ^^   ^^   ^^   ^^   ^^\n\t\t+   %64 : Tensor = aten::zeros_like(%mask, %55, %54, %54, %53, %54), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^                                     ^^   ^^   ^^   ^^   ^^\n\t\t-   %src_key_padding_mask : Tensor = aten::masked_fill_(%71, %mask, %59), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?                                                        ^^           ^\n\t\t+   %src_key_padding_mask : Tensor = aten::masked_fill_(%64, %mask, %52), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?                                                        ^^           ^\n\t\t+   %66 : Tensor = aten::logical_not(%src_key_padding_mask), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:506:0\n\t\t+   %src.3 : Tensor = aten::_nested_tensor_from_mask(%src.1, %66, %53), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505:0\n\t\t+   %linear2.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t+   %bias.9 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.3)\n\t\t+   %linear2.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t+   %weight.13 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.1)\n\t\t+   %linear1.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t+   %bias.7 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.3)\n\t\t+   %linear1.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t+   %weight.11 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.1)\n\t\t+   %norm2.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_0)\n\t\t+   %bias.5 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.3)\n\t\t    %norm2.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_0)\n\t\t+   %weight.9 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.1)\n\t\t-   %dropout2.1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_0)\n\t\t-   %linear2.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t-   %dropout.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_0)\n\t\t-   %linear1.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm1.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_0)\n\t\t?     ^ +  ^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %bias.3 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.3)\n\t\t    %norm1.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_0)\n\t\t-   %dropout1.1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_0)\n\t\t+   %weight.7 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.1)\n\t\t+   %self_attn.7 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.7)\n\t\t+   %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n\t\t+   %self_attn.5 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t+   %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n\t\t+   %self_attn.3 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.3)\n\t\t    %self_attn.1 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t-   %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.1)\n\t\t-   %bias.17 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n\t\t-   %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.1)\n\t\t-   %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n\t\t-   %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.1)\n\t\t    %in_proj_weight.1 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.1)\n\t\t+   %src.5 : Tensor = aten::_transformer_encoder_layer_fwd(%src.3, %49, %50, %in_proj_weight.1, %in_proj_bias.1, %weight.5, %bias.1, %53, %53, %51, %weight.7, %bias.3, %weight.9, %bias.5, %weight.11, %bias.7, %weight.13, %bias.9, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2.7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t-   %query.1 : Tensor = aten::transpose(%src, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %88 : int = aten::size(%query.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.1 : Tensor = prim::NumToTensor(%88), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %90 : int = aten::size(%query.1, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.1 : Tensor = prim::NumToTensor(%90), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %92 : int = aten::size(%query.1, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.1 : Tensor = prim::NumToTensor(%92), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %head_dim.1 : Tensor = aten::div(%embed_dim.1, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %95 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %96 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %97 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %98 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %99 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %100 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %101 : int = aten::size(%query.1, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %102 : Tensor = aten::linear(%query.1, %in_proj_weight.1, %in_proj_bias.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %103 : int[] = prim::ListConstruct(%48, %101), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %104 : Tensor = aten::unflatten(%102, %49, %103), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %105 : Tensor = aten::unsqueeze(%104, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %106 : Tensor = aten::transpose(%105, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %107 : Tensor = aten::squeeze(%106, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.1 : Tensor = aten::contiguous(%107, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.1 : Tensor = aten::select(%proj.1, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.1 : Tensor = aten::select(%proj.1, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.1 : Tensor = aten::select(%proj.1, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %112 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %113 : int = aten::Int(%112), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %114 : int[] = prim::ListConstruct(%88, %113, %100), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %115 : Tensor = aten::view(%q.1, %114), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.3 : Tensor = aten::transpose(%115, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %117 : int = aten::size(%k.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %118 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %119 : int = aten::Int(%118), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %120 : int[] = prim::ListConstruct(%117, %119, %99), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %121 : Tensor = aten::view(%k.1, %120), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.3 : Tensor = aten::transpose(%121, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %123 : int = aten::size(%v.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %124 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %125 : int = aten::Int(%124), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %126 : int[] = prim::ListConstruct(%123, %125, %98), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %127 : Tensor = aten::view(%v.1, %126), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.3 : Tensor = aten::transpose(%127, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %129 : int = aten::size(%k.3, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %130 : int[] = prim::ListConstruct(%90, %54, %54, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %131 : Tensor = aten::view(%src_key_padding_mask, %130), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %132 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %133 : Tensor = aten::expand(%131, %132, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %134 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %135 : int = aten::Int(%134), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %136 : int[] = prim::ListConstruct(%135, %54, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %key_padding_mask.1 : Tensor = aten::reshape(%133, %136), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %138 : int[] = prim::ListConstruct(%90, %46, %49, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_mask.1 : Tensor = aten::view(%key_padding_mask.1, %138), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %140 : int[] = prim::ListConstruct(%90, %46, %88, %97), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %q.5 : Tensor = aten::view(%q.3, %140), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %142 : int[] = prim::ListConstruct(%90, %46, %129, %96), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %k.5 : Tensor = aten::view(%k.3, %142), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %144 : int[] = prim::ListConstruct(%90, %46, %129, %95), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %v.5 : Tensor = aten::view(%v.3, %144), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.1 : Tensor = aten::scaled_dot_product_attention(%q.5, %k.5, %v.5, %attn_mask.1, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %147 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %148 : Tensor = aten::permute(%attn_output.1, %147), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %149 : Tensor = aten::contiguous(%148, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %150 : Tensor = aten::mul(%bsz.1, %tgt_len.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %151 : int = aten::Int(%150), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %152 : int[] = prim::ListConstruct(%151, %92), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_output.3 : Tensor = aten::view(%149, %152), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.5 : Tensor = aten::linear(%attn_output.3, %weight.21, %bias.17), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %155 : int = aten::size(%attn_output.5, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %156 : int[] = prim::ListConstruct(%88, %90, %155), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_output.7 : Tensor = aten::view(%attn_output.5, %156), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.3 : Tensor = aten::transpose(%attn_output.7, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %159 : Tensor = aten::dropout(%input.3, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.5 : Tensor = aten::add(%src, %159, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.1)\n\t\t-   %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.1)\n\t\t-   %163 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1\n\t\t-   %input.7 : Tensor = aten::layer_norm(%input.5, %163, %weight.23, %bias.19, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.1)\n\t\t?         -                                               ^ ^\n\t\t+   %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.7)\n\t\t?          +                                              ^ ^\n\t\t+   %linear2.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t-   %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.1)\n\t\t-   %input.9 : Tensor = aten::linear(%input.7, %weight.25, %bias.21), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.11 : Tensor = aten::relu(%input.9), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.13 : Tensor = aten::dropout(%input.11, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.1)\n\t\t-   %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.1)\n\t\t?            ^                                                  ^\n\t\t+   %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.5)\n\t\t?            ^                                                  ^\n\t\t+   %linear1.7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %input.15 : Tensor = aten::linear(%input.13, %weight.27, %bias.23), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %173 : Tensor = aten::dropout(%input.15, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.17 : Tensor = aten::add(%input.7, %173, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t-   %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.1)\n\t\t?         ^^                                         ^ ^^ ^\n\t\t+   %bias.17 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.7)\n\t\t?         ^^                                        ++ ^^ ^ ^\n\t\t+   %linear1.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.1)\n\t\t?            ^                                           ^ ^^ ^\n\t\t+   %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.5)\n\t\t?            ^                                          ++ ^^ ^ ^\n\t\t-   %177 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm2\n\t\t-   %query.3 : Tensor = aten::layer_norm(%input.17, %177, %weight.29, %bias.25, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %norm2.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?          ^\n\t\t+   %norm2.7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?          ^\n\t\t-   %dropout2.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_1)\n\t\t+   %bias.15 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.7)\n\t\t-   %linear2.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm2.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?     ^ +  ^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.5)\n\t\t-   %dropout.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_1)\n\t\t-   %linear1.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %norm1.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t?          ^\n\t\t+   %norm1.7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t?          ^\n\t\t-   %dropout1.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_1)\n\t\t+   %bias.13 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.7)\n\t\t+   %norm1.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t+   %weight.17 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.5)\n\t\t+   %self_attn.15 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t+   %out_proj.7 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.15)\n\t\t+   %bias.11 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.7)\n\t\t-   %self_attn.3 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t+   %self_attn.13 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t?              +\n\t\t-   %out_proj.7 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.3)\n\t\t-   %bias.27 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.7)\n\t\t-   %out_proj.5 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.3)\n\t\t+   %out_proj.5 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.13)\n\t\t?                                                                                                                               +\n\t\t-   %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n\t\t?           -\n\t\t+   %weight.15 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n\t\t?            +\n\t\t+   %self_attn.11 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t-   %in_proj_bias.3 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.3)\n\t\t?                                                                            ^\n\t\t+   %in_proj_bias.3 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.11)\n\t\t?                                                                            ^^\n\t\t+   %self_attn.9 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t-   %in_proj_weight.3 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.3)\n\t\t?                                                                                ^\n\t\t+   %in_proj_weight.3 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.9)\n\t\t?                                                                                ^\n\t\t+   %src.7 : Tensor = aten::_transformer_encoder_layer_fwd(%src.5, %49, %50, %in_proj_weight.3, %in_proj_bias.3, %weight.15, %bias.11, %53, %53, %51, %weight.17, %bias.13, %weight.19, %bias.15, %weight.21, %bias.17, %weight.23, %bias.19, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2.11 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %query.5 : Tensor = aten::transpose(%query.3, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %194 : int = aten::size(%query.5, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.3 : Tensor = prim::NumToTensor(%194), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %196 : int = aten::size(%query.5, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.3 : Tensor = prim::NumToTensor(%196), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %198 : int = aten::size(%query.5, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.3 : Tensor = prim::NumToTensor(%198), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %head_dim.3 : Tensor = aten::div(%embed_dim.3, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %201 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %202 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %203 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %204 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %205 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %206 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %207 : int = aten::size(%query.5, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %208 : Tensor = aten::linear(%query.5, %in_proj_weight.3, %in_proj_bias.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %209 : int[] = prim::ListConstruct(%48, %207), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %210 : Tensor = aten::unflatten(%208, %49, %209), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %211 : Tensor = aten::unsqueeze(%210, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %212 : Tensor = aten::transpose(%211, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %213 : Tensor = aten::squeeze(%212, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.3 : Tensor = aten::contiguous(%213, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.7 : Tensor = aten::select(%proj.3, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.7 : Tensor = aten::select(%proj.3, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.7 : Tensor = aten::select(%proj.3, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %218 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %219 : int = aten::Int(%218), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %220 : int[] = prim::ListConstruct(%194, %219, %206), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %221 : Tensor = aten::view(%q.7, %220), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.9 : Tensor = aten::transpose(%221, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %223 : int = aten::size(%k.7, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %224 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %225 : int = aten::Int(%224), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %226 : int[] = prim::ListConstruct(%223, %225, %205), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %227 : Tensor = aten::view(%k.7, %226), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.9 : Tensor = aten::transpose(%227, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %229 : int = aten::size(%v.7, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %230 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %231 : int = aten::Int(%230), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %232 : int[] = prim::ListConstruct(%229, %231, %204), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %233 : Tensor = aten::view(%v.7, %232), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.9 : Tensor = aten::transpose(%233, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %235 : int = aten::size(%k.9, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %236 : int[] = prim::ListConstruct(%196, %54, %54, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %237 : Tensor = aten::view(%src_key_padding_mask, %236), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %238 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %239 : Tensor = aten::expand(%237, %238, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %240 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %241 : int = aten::Int(%240), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %242 : int[] = prim::ListConstruct(%241, %54, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %key_padding_mask.3 : Tensor = aten::reshape(%239, %242), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %244 : int[] = prim::ListConstruct(%196, %46, %49, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_mask.3 : Tensor = aten::view(%key_padding_mask.3, %244), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %246 : int[] = prim::ListConstruct(%196, %46, %194, %203), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %q.11 : Tensor = aten::view(%q.9, %246), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %248 : int[] = prim::ListConstruct(%196, %46, %235, %202), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %k.11 : Tensor = aten::view(%k.9, %248), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %250 : int[] = prim::ListConstruct(%196, %46, %235, %201), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %v.11 : Tensor = aten::view(%v.9, %250), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.9 : Tensor = aten::scaled_dot_product_attention(%q.11, %k.11, %v.11, %attn_mask.3, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %253 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %254 : Tensor = aten::permute(%attn_output.9, %253), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %255 : Tensor = aten::contiguous(%254, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %256 : Tensor = aten::mul(%bsz.3, %tgt_len.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %257 : int = aten::Int(%256), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %258 : int[] = prim::ListConstruct(%257, %198), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_output.11 : Tensor = aten::view(%255, %258), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.13 : Tensor = aten::linear(%attn_output.11, %weight.31, %bias.27), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %261 : int = aten::size(%attn_output.13, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %262 : int[] = prim::ListConstruct(%194, %196, %261), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_output.15 : Tensor = aten::view(%attn_output.13, %262), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.19 : Tensor = aten::transpose(%attn_output.15, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %265 : Tensor = aten::dropout(%input.19, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.21 : Tensor = aten::add(%query.3, %265, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.29 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.3)\n\t\t-   %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.3)\n\t\t-   %269 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm1\n\t\t-   %input.23 : Tensor = aten::layer_norm(%input.21, %269, %weight.33, %bias.29, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.31 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.3)\n\t\t-   %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.3)\n\t\t-   %input.25 : Tensor = aten::linear(%input.23, %weight.35, %bias.31), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.27 : Tensor = aten::relu(%input.25), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.29 : Tensor = aten::dropout(%input.27, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.33 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.3)\n\t\t?         ^^                                                ^\n\t\t+   %bias.29 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.11)\n\t\t?         ^^                                                ^^\n\t\t+   %linear2.9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.3)\n\t\t?            ^                                                  ^\n\t\t+   %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.9)\n\t\t?            ^                                                  ^\n\t\t-   %input.31 : Tensor = aten::linear(%input.29, %weight.37, %bias.33), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %279 : Tensor = aten::dropout(%input.31, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.33 : Tensor = aten::add(%input.23, %279, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t+   %linear1.11 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t+   %bias.27 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.11)\n\t\t+   %linear1.9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t+   %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.9)\n\t\t+   %norm2.11 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t-   %bias.35 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.3)\n\t\t?         ^                                               ^\n\t\t+   %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.11)\n\t\t?         ^                                               ^^\n\t\t-   %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.3)\n\t\t-   %283 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm2\n\t\t-   %query.7 : Tensor = aten::layer_norm(%input.33, %283, %weight.39, %bias.35, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %norm2.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t?          ^\n\t\t+   %norm2.9 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t?          ^\n\t\t+   %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.9)\n\t\t-   %dropout2.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_2)\n\t\t-   %linear2.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %dropout.7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_2)\n\t\t-   %linear1.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm1.11 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?     ^ +  ^^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.11)\n\t\t-   %norm1.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?          ^\n\t\t+   %norm1.9 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?          ^\n\t\t-   %dropout1.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_2)\n\t\t+   %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.9)\n\t\t-   %self_attn.5 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t?              ^\n\t\t+   %self_attn.23 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t?              ^^\n\t\t-   %out_proj.11 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t?                                                                                                                                ^\n\t\t+   %out_proj.11 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.23)\n\t\t?                                                                                                                                ^^\n\t\t-   %bias.37 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.11)\n\t\t?         ^^\n\t\t+   %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.11)\n\t\t?         ^^\n\t\t+   %self_attn.21 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %out_proj.9 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t?                                                                                                                               ^\n\t\t+   %out_proj.9 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.21)\n\t\t?                                                                                                                               ^^\n\t\t-   %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.9)\n\t\t?           ^^\n\t\t+   %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.9)\n\t\t?           ^^\n\t\t+   %self_attn.19 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %in_proj_bias.5 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.5)\n\t\t?                                                                            ^\n\t\t+   %in_proj_bias.5 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.19)\n\t\t?                                                                            ^^\n\t\t+   %self_attn.17 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %in_proj_weight.5 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.5)\n\t\t?                                                                                ^\n\t\t+   %in_proj_weight.5 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.17)\n\t\t?                                                                                ^^\n\t\t+   %src : Tensor = aten::_transformer_encoder_layer_fwd(%src.7, %49, %50, %in_proj_weight.5, %in_proj_bias.5, %weight.25, %bias.21, %53, %53, %51, %weight.27, %bias.23, %weight.29, %bias.25, %weight.31, %bias.27, %weight.33, %bias.29, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t-   %query.9 : Tensor = aten::transpose(%query.7, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %300 : int = aten::size(%query.9, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.5 : Tensor = prim::NumToTensor(%300), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %302 : int = aten::size(%query.9, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.5 : Tensor = prim::NumToTensor(%302), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %304 : int = aten::size(%query.9, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.5 : Tensor = prim::NumToTensor(%304), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %head_dim.5 : Tensor = aten::div(%embed_dim.5, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %307 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %308 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %309 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %310 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %311 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %312 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %313 : int = aten::size(%query.9, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %314 : Tensor = aten::linear(%query.9, %in_proj_weight.5, %in_proj_bias.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %315 : int[] = prim::ListConstruct(%48, %313), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %316 : Tensor = aten::unflatten(%314, %49, %315), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %317 : Tensor = aten::unsqueeze(%316, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %318 : Tensor = aten::transpose(%317, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %319 : Tensor = aten::squeeze(%318, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.5 : Tensor = aten::contiguous(%319, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.13 : Tensor = aten::select(%proj.5, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.13 : Tensor = aten::select(%proj.5, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.13 : Tensor = aten::select(%proj.5, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %324 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %325 : int = aten::Int(%324), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %326 : int[] = prim::ListConstruct(%300, %325, %312), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %327 : Tensor = aten::view(%q.13, %326), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.15 : Tensor = aten::transpose(%327, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %329 : int = aten::size(%k.13, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %330 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %331 : int = aten::Int(%330), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %332 : int[] = prim::ListConstruct(%329, %331, %311), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %333 : Tensor = aten::view(%k.13, %332), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.15 : Tensor = aten::transpose(%333, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %335 : int = aten::size(%v.13, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %336 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %337 : int = aten::Int(%336), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %338 : int[] = prim::ListConstruct(%335, %337, %310), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %339 : Tensor = aten::view(%v.13, %338), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.15 : Tensor = aten::transpose(%339, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %341 : int = aten::size(%k.15, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %342 : int[] = prim::ListConstruct(%302, %54, %54, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %343 : Tensor = aten::view(%src_key_padding_mask, %342), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %344 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %345 : Tensor = aten::expand(%343, %344, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %346 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %347 : int = aten::Int(%346), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %348 : int[] = prim::ListConstruct(%347, %54, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %key_padding_mask.5 : Tensor = aten::reshape(%345, %348), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %350 : int[] = prim::ListConstruct(%302, %46, %49, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_mask.5 : Tensor = aten::view(%key_padding_mask.5, %350), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %352 : int[] = prim::ListConstruct(%302, %46, %300, %309), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %q.17 : Tensor = aten::view(%q.15, %352), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %354 : int[] = prim::ListConstruct(%302, %46, %341, %308), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %k.17 : Tensor = aten::view(%k.15, %354), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %356 : int[] = prim::ListConstruct(%302, %46, %341, %307), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %v.17 : Tensor = aten::view(%v.15, %356), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.17 : Tensor = aten::scaled_dot_product_attention(%q.17, %k.17, %v.17, %attn_mask.5, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %359 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %360 : Tensor = aten::permute(%attn_output.17, %359), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %361 : Tensor = aten::contiguous(%360, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %362 : Tensor = aten::mul(%bsz.5, %tgt_len.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %363 : int = aten::Int(%362), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %364 : int[] = prim::ListConstruct(%363, %304), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_output.19 : Tensor = aten::view(%361, %364), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.21 : Tensor = aten::linear(%attn_output.19, %weight.41, %bias.37), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %367 : int = aten::size(%attn_output.21, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %368 : int[] = prim::ListConstruct(%300, %302, %367), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_output.23 : Tensor = aten::view(%attn_output.21, %368), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.35 : Tensor = aten::transpose(%attn_output.23, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %371 : Tensor = aten::dropout(%input.35, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.37 : Tensor = aten::add(%query.7, %371, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.39 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.5)\n\t\t-   %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.5)\n\t\t-   %375 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm1\n\t\t-   %input.39 : Tensor = aten::layer_norm(%input.37, %375, %weight.43, %bias.39, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.41 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.5)\n\t\t-   %weight.45 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.5)\n\t\t-   %input.41 : Tensor = aten::linear(%input.39, %weight.45, %bias.41), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.43 : Tensor = aten::relu(%input.41), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.45 : Tensor = aten::dropout(%input.43, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.43 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.5)\n\t\t?         -                                                --\n\t\t+   %bias.39 : Tensor = prim::GetAttr[name=\"bias\"](%linear2)\n\t\t?          +\n\t\t+   %linear2.13 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t-   %weight.47 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.5)\n\t\t?            ^                                                  ^\n\t\t+   %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.13)\n\t\t?            ^                                                  ^^\n\t\t+   %linear1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t-   %input.47 : Tensor = aten::linear(%input.45, %weight.47, %bias.43), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %385 : Tensor = aten::dropout(%input.47, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.49 : Tensor = aten::add(%input.39, %385, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t-   %bias.45 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.5)\n\t\t?         ^^                                         ^ ^^^^\n\t\t+   %bias.37 : Tensor = prim::GetAttr[name=\"bias\"](%linear1)\n\t\t?         ^^                                        ++ ^^ ^\n\t\t+   %linear1.13 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t-   %weight.49 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.5)\n\t\t?            ^                                           ^ ^^ ^\n\t\t+   %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.13)\n\t\t?            ^                                          ++ ^^ ^ ^^\n\t\t-   %389 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm2\n\t\t-   %query.11 : Tensor = aten::layer_norm(%input.49, %389, %weight.49, %bias.45, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t    %norm2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_3)\n\t\t-   %dropout2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_3)\n\t\t+   %bias.35 : Tensor = prim::GetAttr[name=\"bias\"](%norm2)\n\t\t-   %linear2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t?    -- ^^                                   ---  ^^ -                        -- ^^\n\t\t+   %norm2.13 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_3)\n\t\t?     ^ + +++                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.13)\n\t\t-   %dropout.9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_3)\n\t\t-   %linear1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t    %norm1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_3)\n\t\t-   %dropout1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_3)\n\t\t+   %bias.33 : Tensor = prim::GetAttr[name=\"bias\"](%norm1)\n\t\t+   %norm1.13 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_3)\n\t\t+   %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.13)\n\t\t    %self_attn : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t    %out_proj : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn)\n\t\t-   %bias.47 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n\t\t?         ^^\n\t\t+   %bias.31 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n\t\t?         ^^\n\t\t+   %self_attn.29 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %out_proj.13 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn)\n\t\t+   %out_proj.13 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.29)\n\t\t?                                                                                                                               +++\n\t\t-   %weight.51 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.13)\n\t\t?            -\n\t\t+   %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.13)\n\t\t?           +\n\t\t+   %self_attn.27 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn)\n\t\t+   %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.27)\n\t\t?                                                                         +++\n\t\t+   %self_attn.25 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn)\n\t\t+   %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.25)\n\t\t?                                                                             +++\n\t\t+   %output : Tensor = aten::_transformer_encoder_layer_fwd(%src, %49, %50, %in_proj_weight, %in_proj_bias, %weight.35, %bias.31, %53, %53, %51, %weight.37, %bias.33, %weight.39, %bias.35, %weight.41, %bias.37, %weight.43, %bias.39, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %176 : int = aten::size(%src.1, %48), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t-   %query : Tensor = aten::transpose(%query.11, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %406 : int = aten::size(%query, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len : Tensor = prim::NumToTensor(%406), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %408 : int = aten::size(%query, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz : Tensor = prim::NumToTensor(%408), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %410 : int = aten::size(%query, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim : Tensor = prim::NumToTensor(%410), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %head_dim : Tensor = aten::div(%embed_dim, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %413 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %414 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %415 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %416 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %417 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %418 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %419 : int = aten::size(%query, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %420 : Tensor = aten::linear(%query, %in_proj_weight, %in_proj_bias), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %421 : int[] = prim::ListConstruct(%48, %419), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %422 : Tensor = aten::unflatten(%420, %49, %421), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %423 : Tensor = aten::unsqueeze(%422, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %424 : Tensor = aten::transpose(%423, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %425 : Tensor = aten::squeeze(%424, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj : Tensor = aten::contiguous(%425, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.19 : Tensor = aten::select(%proj, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.19 : Tensor = aten::select(%proj, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.19 : Tensor = aten::select(%proj, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %430 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %431 : int = aten::Int(%430), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %432 : int[] = prim::ListConstruct(%406, %431, %418), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %433 : Tensor = aten::view(%q.19, %432), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.21 : Tensor = aten::transpose(%433, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %435 : int = aten::size(%k.19, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %436 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %437 : int = aten::Int(%436), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %438 : int[] = prim::ListConstruct(%435, %437, %417), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %439 : Tensor = aten::view(%k.19, %438), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.21 : Tensor = aten::transpose(%439, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %441 : int = aten::size(%v.19, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %442 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %443 : int = aten::Int(%442), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %444 : int[] = prim::ListConstruct(%441, %443, %416), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %445 : Tensor = aten::view(%v.19, %444), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.21 : Tensor = aten::transpose(%445, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %447 : int = aten::size(%k.21, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %448 : int[] = prim::ListConstruct(%408, %54, %54, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %449 : Tensor = aten::view(%src_key_padding_mask, %448), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %450 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %451 : Tensor = aten::expand(%449, %450, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %452 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %453 : int = aten::Int(%452), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %454 : int[] = prim::ListConstruct(%453, %54, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %key_padding_mask : Tensor = aten::reshape(%451, %454), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %456 : int[] = prim::ListConstruct(%408, %46, %49, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_mask : Tensor = aten::view(%key_padding_mask, %456), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %458 : int[] = prim::ListConstruct(%408, %46, %406, %415), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %q : Tensor = aten::view(%q.21, %458), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %460 : int[] = prim::ListConstruct(%408, %46, %447, %414), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %k : Tensor = aten::view(%k.21, %460), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %462 : int[] = prim::ListConstruct(%408, %46, %447, %413), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %v : Tensor = aten::view(%v.21, %462), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.25 : Tensor = aten::scaled_dot_product_attention(%q, %k, %v, %attn_mask, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %465 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %466 : Tensor = aten::permute(%attn_output.25, %465), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %467 : Tensor = aten::contiguous(%466, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %468 : Tensor = aten::mul(%bsz, %tgt_len), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %469 : int = aten::Int(%468), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %470 : int[] = prim::ListConstruct(%469, %410), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_output.27 : Tensor = aten::view(%467, %470), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.29 : Tensor = aten::linear(%attn_output.27, %weight.51, %bias.47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %473 : int = aten::size(%attn_output.29, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %474 : int[] = prim::ListConstruct(%406, %408, %473), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_output : Tensor = aten::view(%attn_output.29, %474), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.51 : Tensor = aten::transpose(%attn_output, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %477 : Tensor = aten::dropout(%input.51, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.53 : Tensor = aten::add(%query.11, %477, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t?      -- ------------         ^^^  ^^^ ^  -     ------                                      --------------------------------------                                                                                                  ^^^\n\t\t+   %177 : int = aten::size(%src.1, %47), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t?    ++++++            ^^^^  ^ ^                                                                                                                                               ^^^\n\t\t-   %bias.49 : Tensor = prim::GetAttr[name=\"bias\"](%norm1)\n\t\t-   %weight.53 : Tensor = prim::GetAttr[name=\"weight\"](%norm1)\n\t\t-   %481 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm1\n\t\t-   %input.55 : Tensor = aten::layer_norm(%input.53, %481, %weight.53, %bias.49, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.51 : Tensor = prim::GetAttr[name=\"bias\"](%linear1)\n\t\t-   %weight.55 : Tensor = prim::GetAttr[name=\"weight\"](%linear1)\n\t\t-   %input.57 : Tensor = aten::linear(%input.55, %weight.55, %bias.51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.59 : Tensor = aten::relu(%input.57), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.61 : Tensor = aten::dropout(%input.59, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.53 : Tensor = prim::GetAttr[name=\"bias\"](%linear2)\n\t\t-   %weight.57 : Tensor = prim::GetAttr[name=\"weight\"](%linear2)\n\t\t-   %input.63 : Tensor = aten::linear(%input.61, %weight.57, %bias.53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %491 : Tensor = aten::dropout(%input.63, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.65 : Tensor = aten::add(%input.55, %491, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t?      -- ------------         ^^^  ^^^^^ ^^    ^^^^^^^                                      --------------------------------------                                                                                                  ^^^\n\t\t+   %178 : int = aten::size(%src.1, %46), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t?    ++++++            ^^^^  ^^^ ^    ^                                                                                                                                        ^^^\n\t\t+   %179 : int[] = prim::ListConstruct(%176, %177, %178), scope: __module.transformer_encoder\n\t\t+   %transformer_output : Tensor = aten::to_padded_tensor(%output, %45, %179), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t-   %bias.55 : Tensor = prim::GetAttr[name=\"bias\"](%norm2)\n\t\t-   %weight.59 : Tensor = prim::GetAttr[name=\"weight\"](%norm2)\n\t\t-   %495 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm2\n\t\t-   %transformer_output : Tensor = aten::layer_norm(%input.65, %495, %weight.59, %bias.55, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t    %13 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %14 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %15 : int = prim::Constant[value=9223372036854775807]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %16 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %17 : Tensor = aten::slice(%transformer_output, %13, %14, %15, %16) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %18 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %19 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %20 : Tensor = aten::select(%17, %18, %19) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %21 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %22 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %23 : int = prim::Constant[value=9223372036854775807]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %24 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t-   %input.67 : Tensor = aten::slice(%20, %21, %22, %23, %24) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t?          ^^\n\t\t+   %input.3 : Tensor = aten::slice(%20, %21, %22, %23, %24) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t?          ^\n\t\t-   %497 : bool = prim::Constant[value=0](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^^\n\t\t+   %181 : bool = prim::Constant[value=0](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^^\n\t\t-   %498 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^\n\t\t+   %182 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^ +\n\t\t-   %input : Tensor = aten::dropout(%input.67, %498, %497), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?                                          ^^   ^^    ^^^\n\t\t+   %input : Tensor = aten::dropout(%input.3, %182, %181), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?                                          ^   ^ +   ^^^\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%fc_out)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%fc_out)\n\t\t-   %502 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.fc_out # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t?    ^^^\n\t\t+   %186 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.fc_out # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t?    ^^^\n\t\t-   return (%502)\n\t\t?            ^^^\n\t\t+   return (%186)\n\t\t?            ^^^\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %fc_out : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t?                                                             ^^\n\t\t+ %fc_out : __torch__.torch.nn.modules.linear.___torch_mangle_82.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t?                                                             ^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTracingCheckError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m writer = SummaryWriter(\u001b[33m'\u001b[39m\u001b[33monnx/transformer_model_graph\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# writer.add_graph를 사용하여 모델의 연산 그래프를 TensorBoard에 추가\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 모델 객체와 모델의 forward 메서드에 전달될 더미 입력 튜플을 제공합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_attention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# writer를 닫아 로그 파일 작성을 완료합니다.\u001b[39;00m\n\u001b[32m     32\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:841\u001b[39m, in \u001b[36mSummaryWriter.add_graph\u001b[39m\u001b[34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[39m\n\u001b[32m    838\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mtensorboard.logging.add_graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[32m    840\u001b[39m \u001b[38;5;28mself\u001b[39m._get_file_writer().add_graph(\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\tensorboard\\_pytorch_graph.py:326\u001b[39m, in \u001b[36mgraph\u001b[39m\u001b[34m(model, args, verbose, use_strict_trace)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         trace = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m         graph = trace.graph\n\u001b[32m    328\u001b[39m         torch._C._jit_pass_inline(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\jit\\_trace.py:1002\u001b[39m, in \u001b[36mtrace\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    989\u001b[39m     warnings.warn(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`optimize` is deprecated and has no effect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `with torch.jit.optimized_execution()` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    992\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    993\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    994\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    997\u001b[39m     check_if_torch_exportable,\n\u001b[32m    998\u001b[39m     log_torch_jit_trace_exportability,\n\u001b[32m    999\u001b[39m     log_torchscript_usage,\n\u001b[32m   1000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m traced_func = \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m log_torchscript_usage(\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m, model_id=_get_model_id(traced_func))\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\jit\\_trace.py:696\u001b[39m, in \u001b[36m_trace_impl\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    694\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    695\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mexample_kwarg_inputs should be a dict\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    710\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__self__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    711\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func.\u001b[34m__self__\u001b[39m, torch.nn.Module)\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    713\u001b[39m ):\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\jit\\_trace.py:1307\u001b[39m, in \u001b[36mtrace_module\u001b[39m\u001b[34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[39m\n\u001b[32m   1295\u001b[39m                 _check_trace(\n\u001b[32m   1296\u001b[39m                     check_inputs,\n\u001b[32m   1297\u001b[39m                     func,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1304\u001b[39m                     example_inputs_is_kwarg=example_inputs_is_kwarg,\n\u001b[32m   1305\u001b[39m                 )\n\u001b[32m   1306\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m                 \u001b[43m_check_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcheck_trace_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1319\u001b[39m     torch.jit._trace._trace_module_map = old_module_map\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\jit\\_trace.py:590\u001b[39m, in \u001b[36m_check_trace\u001b[39m\u001b[34m(check_inputs, func, traced_func, check_tolerance, strict, force_outplace, is_trace_module, _module_class, example_inputs_is_kwarg)\u001b[39m\n\u001b[32m    588\u001b[39m diag_info = graph_diagnostic_info()\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m diag_info):\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TracingCheckError(*diag_info)\n",
      "\u001b[31mTracingCheckError\u001b[39m: Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.Model.Transformer,\n\t\t        %input_ids : Tensor,\n\t\t        %attention_mask : Tensor):\n\t\t    %fc_out : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t    %dropout : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%self.1)\n\t\t    %transformer_encoder : __torch__.torch.nn.modules.transformer.TransformerEncoder = prim::GetAttr[name=\"transformer_encoder\"](%self.1)\n\t\t    %pos_encoder : __torch__.Model.PositionalEncoding = prim::GetAttr[name=\"pos_encoder\"](%self.1)\n\t\t    %embedding : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"embedding\"](%self.1)\n\t\t    %28 : bool = prim::Constant[value=0](), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t    %29 : int = prim::Constant[value=0](), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t-   %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%embedding)\n\t\t?           ^^\n\t\t+   %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%embedding)\n\t\t?           ^\n\t\t-   %x : Tensor = aten::embedding(%weight.19, %input_ids, %29, %28, %28), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t?                                         ^^\n\t\t+   %x : Tensor = aten::embedding(%weight.3, %input_ids, %29, %28, %28), scope: __module.embedding # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2551:0\n\t\t?                                         ^\n\t\t    %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t    %33 : bool = prim::Constant[value=0](), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t    %34 : int = prim::Constant[value=2](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %35 : int = prim::Constant[value=9223372036854775807](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %36 : int = prim::Constant[value=0](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %37 : int = prim::Constant[value=1](), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %pe : Tensor = prim::GetAttr[name=\"pe\"](%pos_encoder)\n\t\t    %39 : int = aten::size(%x, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %40 : Tensor = aten::slice(%pe, %36, %36, %35, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %41 : Tensor = aten::slice(%40, %37, %36, %39, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %42 : Tensor = aten::slice(%41, %34, %36, %35, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t    %input.1 : Tensor = aten::add(%x, %42, %37), scope: __module.pos_encoder # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:20:0\n\t\t-   %src : Tensor = aten::dropout(%input.1, %32, %33), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?       ^^\n\t\t+   %src.1 : Tensor = aten::dropout(%input.1, %32, %33), scope: __module.pos_encoder/__module.pos_encoder.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?       ^^^^\n\t\t    %10 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:48:0\n\t\t    %mask : Tensor = aten::eq(%attention_mask, %10) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:48:0\n\t\t+   %45 : float = prim::Constant[value=0.](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %46 : int = prim::Constant[value=2](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %47 : int = prim::Constant[value=1](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %48 : int = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t+   %49 : int = prim::Constant[value=256](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %50 : int = prim::Constant[value=8](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %51 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t-   %45 : float = prim::Constant[value=0.](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %46 : int = prim::Constant[value=8](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %47 : int = prim::Constant[value=-2](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %48 : int = prim::Constant[value=3](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %49 : int = prim::Constant[value=-1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %50 : str = prim::Constant[value=\"trunc\"](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %51 : Tensor = prim::Constant[value={8}](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %52 : int = prim::Constant[value=2](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %53 : int = prim::Constant[value=0](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %54 : int = prim::Constant[value=1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %55 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %56 : bool = prim::Constant[value=1](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %57 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %58 : int = prim::Constant[value=256](), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %59 : float = prim::Constant[value=-inf](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?     ^\n\t\t+   %52 : float = prim::Constant[value=-inf](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?     ^\n\t\t-   %60 : bool = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t+   %53 : bool = prim::Constant[value=0](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t-   %61 : NoneType = prim::Constant(), scope: __module.transformer_encoder\n\t\t?    ^^\n\t\t+   %54 : NoneType = prim::Constant(), scope: __module.transformer_encoder\n\t\t?    ^^\n\t\t-   %62 : int = prim::Constant[value=6](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t+   %55 : int = prim::Constant[value=6](), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^\n\t\t    %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_3 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"3\"](%layers)\n\t\t    %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_2 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"2\"](%layers.5)\n\t\t    %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_1 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"1\"](%layers.3)\n\t\t    %layers.1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%transformer_encoder)\n\t\t    %_0 : __torch__.torch.nn.modules.transformer.TransformerEncoderLayer = prim::GetAttr[name=\"0\"](%layers.1)\n\t\t-   %71 : Tensor = aten::zeros_like(%mask, %62, %61, %61, %60, %61), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^                                     ^^   ^^   ^^   ^^   ^^\n\t\t+   %64 : Tensor = aten::zeros_like(%mask, %55, %54, %54, %53, %54), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?    ^^                                     ^^   ^^   ^^   ^^   ^^\n\t\t-   %src_key_padding_mask : Tensor = aten::masked_fill_(%71, %mask, %59), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?                                                        ^^           ^\n\t\t+   %src_key_padding_mask : Tensor = aten::masked_fill_(%64, %mask, %52), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5967:0\n\t\t?                                                        ^^           ^\n\t\t+   %66 : Tensor = aten::logical_not(%src_key_padding_mask), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:506:0\n\t\t+   %src.3 : Tensor = aten::_nested_tensor_from_mask(%src.1, %66, %53), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505:0\n\t\t+   %linear2.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t+   %bias.9 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.3)\n\t\t+   %linear2.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t+   %weight.13 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.1)\n\t\t+   %linear1.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t+   %bias.7 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.3)\n\t\t+   %linear1.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t+   %weight.11 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.1)\n\t\t+   %norm2.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_0)\n\t\t+   %bias.5 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.3)\n\t\t    %norm2.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_0)\n\t\t+   %weight.9 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.1)\n\t\t-   %dropout2.1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_0)\n\t\t-   %linear2.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_0)\n\t\t-   %dropout.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_0)\n\t\t-   %linear1.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_0)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm1.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_0)\n\t\t?     ^ +  ^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %bias.3 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.3)\n\t\t    %norm1.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_0)\n\t\t-   %dropout1.1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_0)\n\t\t+   %weight.7 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.1)\n\t\t+   %self_attn.7 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.7)\n\t\t+   %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n\t\t+   %self_attn.5 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t+   %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n\t\t+   %self_attn.3 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t+   %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.3)\n\t\t    %self_attn.1 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n\t\t-   %out_proj.3 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.1)\n\t\t-   %bias.17 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.3)\n\t\t-   %out_proj.1 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.1)\n\t\t-   %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.1)\n\t\t-   %in_proj_bias.1 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.1)\n\t\t    %in_proj_weight.1 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.1)\n\t\t+   %src.5 : Tensor = aten::_transformer_encoder_layer_fwd(%src.3, %49, %50, %in_proj_weight.1, %in_proj_bias.1, %weight.5, %bias.1, %53, %53, %51, %weight.7, %bias.3, %weight.9, %bias.5, %weight.11, %bias.7, %weight.13, %bias.9, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2.7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t-   %query.1 : Tensor = aten::transpose(%src, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %88 : int = aten::size(%query.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.1 : Tensor = prim::NumToTensor(%88), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %90 : int = aten::size(%query.1, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.1 : Tensor = prim::NumToTensor(%90), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %92 : int = aten::size(%query.1, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.1 : Tensor = prim::NumToTensor(%92), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %head_dim.1 : Tensor = aten::div(%embed_dim.1, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %95 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %96 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %97 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %98 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %99 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %100 : int = aten::Int(%head_dim.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %101 : int = aten::size(%query.1, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %102 : Tensor = aten::linear(%query.1, %in_proj_weight.1, %in_proj_bias.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %103 : int[] = prim::ListConstruct(%48, %101), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %104 : Tensor = aten::unflatten(%102, %49, %103), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %105 : Tensor = aten::unsqueeze(%104, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %106 : Tensor = aten::transpose(%105, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %107 : Tensor = aten::squeeze(%106, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.1 : Tensor = aten::contiguous(%107, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.1 : Tensor = aten::select(%proj.1, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.1 : Tensor = aten::select(%proj.1, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.1 : Tensor = aten::select(%proj.1, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %112 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %113 : int = aten::Int(%112), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %114 : int[] = prim::ListConstruct(%88, %113, %100), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %115 : Tensor = aten::view(%q.1, %114), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.3 : Tensor = aten::transpose(%115, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %117 : int = aten::size(%k.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %118 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %119 : int = aten::Int(%118), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %120 : int[] = prim::ListConstruct(%117, %119, %99), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %121 : Tensor = aten::view(%k.1, %120), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.3 : Tensor = aten::transpose(%121, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %123 : int = aten::size(%v.1, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %124 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %125 : int = aten::Int(%124), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %126 : int[] = prim::ListConstruct(%123, %125, %98), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %127 : Tensor = aten::view(%v.1, %126), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.3 : Tensor = aten::transpose(%127, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %129 : int = aten::size(%k.3, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %130 : int[] = prim::ListConstruct(%90, %54, %54, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %131 : Tensor = aten::view(%src_key_padding_mask, %130), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %132 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %133 : Tensor = aten::expand(%131, %132, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %134 : Tensor = aten::mul(%bsz.1, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %135 : int = aten::Int(%134), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %136 : int[] = prim::ListConstruct(%135, %54, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %key_padding_mask.1 : Tensor = aten::reshape(%133, %136), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %138 : int[] = prim::ListConstruct(%90, %46, %49, %129), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_mask.1 : Tensor = aten::view(%key_padding_mask.1, %138), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %140 : int[] = prim::ListConstruct(%90, %46, %88, %97), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %q.5 : Tensor = aten::view(%q.3, %140), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %142 : int[] = prim::ListConstruct(%90, %46, %129, %96), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %k.5 : Tensor = aten::view(%k.3, %142), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %144 : int[] = prim::ListConstruct(%90, %46, %129, %95), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %v.5 : Tensor = aten::view(%v.3, %144), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.1 : Tensor = aten::scaled_dot_product_attention(%q.5, %k.5, %v.5, %attn_mask.1, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %147 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %148 : Tensor = aten::permute(%attn_output.1, %147), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %149 : Tensor = aten::contiguous(%148, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %150 : Tensor = aten::mul(%bsz.1, %tgt_len.1), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %151 : int = aten::Int(%150), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %152 : int[] = prim::ListConstruct(%151, %92), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_output.3 : Tensor = aten::view(%149, %152), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.5 : Tensor = aten::linear(%attn_output.3, %weight.21, %bias.17), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %155 : int = aten::size(%attn_output.5, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %156 : int[] = prim::ListConstruct(%88, %90, %155), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn\n\t\t-   %attn_output.7 : Tensor = aten::view(%attn_output.5, %156), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.3 : Tensor = aten::transpose(%attn_output.7, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %159 : Tensor = aten::dropout(%input.3, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.5 : Tensor = aten::add(%src, %159, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.1)\n\t\t-   %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.1)\n\t\t-   %163 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1\n\t\t-   %input.7 : Tensor = aten::layer_norm(%input.5, %163, %weight.23, %bias.19, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.1)\n\t\t?         -                                               ^ ^\n\t\t+   %bias.19 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.7)\n\t\t?          +                                              ^ ^\n\t\t+   %linear2.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t-   %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.1)\n\t\t-   %input.9 : Tensor = aten::linear(%input.7, %weight.25, %bias.21), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.11 : Tensor = aten::relu(%input.9), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.13 : Tensor = aten::dropout(%input.11, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.1)\n\t\t-   %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.1)\n\t\t?            ^                                                  ^\n\t\t+   %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.5)\n\t\t?            ^                                                  ^\n\t\t+   %linear1.7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %input.15 : Tensor = aten::linear(%input.13, %weight.27, %bias.23), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %173 : Tensor = aten::dropout(%input.15, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.17 : Tensor = aten::add(%input.7, %173, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t-   %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.1)\n\t\t?         ^^                                         ^ ^^ ^\n\t\t+   %bias.17 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.7)\n\t\t?         ^^                                        ++ ^^ ^ ^\n\t\t+   %linear1.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.1)\n\t\t?            ^                                           ^ ^^ ^\n\t\t+   %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.5)\n\t\t?            ^                                          ++ ^^ ^ ^\n\t\t-   %177 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm2\n\t\t-   %query.3 : Tensor = aten::layer_norm(%input.17, %177, %weight.29, %bias.25, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.0/__module.transformer_encoder.layers.0.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %norm2.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?          ^\n\t\t+   %norm2.7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?          ^\n\t\t-   %dropout2.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_1)\n\t\t+   %bias.15 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.7)\n\t\t-   %linear2.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_1)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm2.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_1)\n\t\t?     ^ +  ^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.5)\n\t\t-   %dropout.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_1)\n\t\t-   %linear1.3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_1)\n\t\t-   %norm1.3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t?          ^\n\t\t+   %norm1.7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t?          ^\n\t\t-   %dropout1.3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_1)\n\t\t+   %bias.13 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.7)\n\t\t+   %norm1.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_1)\n\t\t+   %weight.17 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.5)\n\t\t+   %self_attn.15 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t+   %out_proj.7 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.15)\n\t\t+   %bias.11 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.7)\n\t\t-   %self_attn.3 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t+   %self_attn.13 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t?              +\n\t\t-   %out_proj.7 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.3)\n\t\t-   %bias.27 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.7)\n\t\t-   %out_proj.5 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.3)\n\t\t+   %out_proj.5 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.13)\n\t\t?                                                                                                                               +\n\t\t-   %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n\t\t?           -\n\t\t+   %weight.15 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.5)\n\t\t?            +\n\t\t+   %self_attn.11 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t-   %in_proj_bias.3 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.3)\n\t\t?                                                                            ^\n\t\t+   %in_proj_bias.3 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.11)\n\t\t?                                                                            ^^\n\t\t+   %self_attn.9 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n\t\t-   %in_proj_weight.3 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.3)\n\t\t?                                                                                ^\n\t\t+   %in_proj_weight.3 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.9)\n\t\t?                                                                                ^\n\t\t+   %src.7 : Tensor = aten::_transformer_encoder_layer_fwd(%src.5, %49, %50, %in_proj_weight.3, %in_proj_bias.3, %weight.15, %bias.11, %53, %53, %51, %weight.17, %bias.13, %weight.19, %bias.15, %weight.21, %bias.17, %weight.23, %bias.19, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2.11 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %query.5 : Tensor = aten::transpose(%query.3, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %194 : int = aten::size(%query.5, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.3 : Tensor = prim::NumToTensor(%194), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %196 : int = aten::size(%query.5, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.3 : Tensor = prim::NumToTensor(%196), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %198 : int = aten::size(%query.5, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.3 : Tensor = prim::NumToTensor(%198), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %head_dim.3 : Tensor = aten::div(%embed_dim.3, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %201 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %202 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %203 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %204 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %205 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %206 : int = aten::Int(%head_dim.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %207 : int = aten::size(%query.5, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %208 : Tensor = aten::linear(%query.5, %in_proj_weight.3, %in_proj_bias.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %209 : int[] = prim::ListConstruct(%48, %207), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %210 : Tensor = aten::unflatten(%208, %49, %209), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %211 : Tensor = aten::unsqueeze(%210, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %212 : Tensor = aten::transpose(%211, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %213 : Tensor = aten::squeeze(%212, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.3 : Tensor = aten::contiguous(%213, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.7 : Tensor = aten::select(%proj.3, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.7 : Tensor = aten::select(%proj.3, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.7 : Tensor = aten::select(%proj.3, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %218 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %219 : int = aten::Int(%218), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %220 : int[] = prim::ListConstruct(%194, %219, %206), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %221 : Tensor = aten::view(%q.7, %220), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.9 : Tensor = aten::transpose(%221, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %223 : int = aten::size(%k.7, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %224 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %225 : int = aten::Int(%224), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %226 : int[] = prim::ListConstruct(%223, %225, %205), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %227 : Tensor = aten::view(%k.7, %226), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.9 : Tensor = aten::transpose(%227, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %229 : int = aten::size(%v.7, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %230 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %231 : int = aten::Int(%230), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %232 : int[] = prim::ListConstruct(%229, %231, %204), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %233 : Tensor = aten::view(%v.7, %232), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.9 : Tensor = aten::transpose(%233, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %235 : int = aten::size(%k.9, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %236 : int[] = prim::ListConstruct(%196, %54, %54, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %237 : Tensor = aten::view(%src_key_padding_mask, %236), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %238 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %239 : Tensor = aten::expand(%237, %238, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %240 : Tensor = aten::mul(%bsz.3, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %241 : int = aten::Int(%240), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %242 : int[] = prim::ListConstruct(%241, %54, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %key_padding_mask.3 : Tensor = aten::reshape(%239, %242), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %244 : int[] = prim::ListConstruct(%196, %46, %49, %235), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_mask.3 : Tensor = aten::view(%key_padding_mask.3, %244), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %246 : int[] = prim::ListConstruct(%196, %46, %194, %203), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %q.11 : Tensor = aten::view(%q.9, %246), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %248 : int[] = prim::ListConstruct(%196, %46, %235, %202), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %k.11 : Tensor = aten::view(%k.9, %248), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %250 : int[] = prim::ListConstruct(%196, %46, %235, %201), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %v.11 : Tensor = aten::view(%v.9, %250), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.9 : Tensor = aten::scaled_dot_product_attention(%q.11, %k.11, %v.11, %attn_mask.3, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %253 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %254 : Tensor = aten::permute(%attn_output.9, %253), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %255 : Tensor = aten::contiguous(%254, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %256 : Tensor = aten::mul(%bsz.3, %tgt_len.3), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %257 : int = aten::Int(%256), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %258 : int[] = prim::ListConstruct(%257, %198), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_output.11 : Tensor = aten::view(%255, %258), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.13 : Tensor = aten::linear(%attn_output.11, %weight.31, %bias.27), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %261 : int = aten::size(%attn_output.13, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %262 : int[] = prim::ListConstruct(%194, %196, %261), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn\n\t\t-   %attn_output.15 : Tensor = aten::view(%attn_output.13, %262), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.19 : Tensor = aten::transpose(%attn_output.15, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %265 : Tensor = aten::dropout(%input.19, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.21 : Tensor = aten::add(%query.3, %265, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.29 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.3)\n\t\t-   %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.3)\n\t\t-   %269 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm1\n\t\t-   %input.23 : Tensor = aten::layer_norm(%input.21, %269, %weight.33, %bias.29, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.31 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.3)\n\t\t-   %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.3)\n\t\t-   %input.25 : Tensor = aten::linear(%input.23, %weight.35, %bias.31), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.27 : Tensor = aten::relu(%input.25), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.29 : Tensor = aten::dropout(%input.27, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.33 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.3)\n\t\t?         ^^                                                ^\n\t\t+   %bias.29 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.11)\n\t\t?         ^^                                                ^^\n\t\t+   %linear2.9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.3)\n\t\t?            ^                                                  ^\n\t\t+   %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.9)\n\t\t?            ^                                                  ^\n\t\t-   %input.31 : Tensor = aten::linear(%input.29, %weight.37, %bias.33), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %279 : Tensor = aten::dropout(%input.31, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.33 : Tensor = aten::add(%input.23, %279, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t+   %linear1.11 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t+   %bias.27 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.11)\n\t\t+   %linear1.9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t+   %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.9)\n\t\t+   %norm2.11 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t-   %bias.35 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.3)\n\t\t?         ^                                               ^\n\t\t+   %bias.25 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.11)\n\t\t?         ^                                               ^^\n\t\t-   %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.3)\n\t\t-   %283 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm2\n\t\t-   %query.7 : Tensor = aten::layer_norm(%input.33, %283, %weight.39, %bias.35, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.1/__module.transformer_encoder.layers.1.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %norm2.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t?          ^\n\t\t+   %norm2.9 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_2)\n\t\t?          ^\n\t\t+   %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.9)\n\t\t-   %dropout2.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_2)\n\t\t-   %linear2.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_2)\n\t\t-   %dropout.7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_2)\n\t\t-   %linear1.5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_2)\n\t\t?    -- ^^   ^                                 ---  ^^ -                        -- ^^\n\t\t+   %norm1.11 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?     ^ +  ^^                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %bias.23 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.11)\n\t\t-   %norm1.5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?          ^\n\t\t+   %norm1.9 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_2)\n\t\t?          ^\n\t\t-   %dropout1.5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_2)\n\t\t+   %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.9)\n\t\t-   %self_attn.5 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t?              ^\n\t\t+   %self_attn.23 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t?              ^^\n\t\t-   %out_proj.11 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t?                                                                                                                                ^\n\t\t+   %out_proj.11 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.23)\n\t\t?                                                                                                                                ^^\n\t\t-   %bias.37 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.11)\n\t\t?         ^^\n\t\t+   %bias.21 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj.11)\n\t\t?         ^^\n\t\t+   %self_attn.21 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %out_proj.9 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.5)\n\t\t?                                                                                                                               ^\n\t\t+   %out_proj.9 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.21)\n\t\t?                                                                                                                               ^^\n\t\t-   %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.9)\n\t\t?           ^^\n\t\t+   %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.9)\n\t\t?           ^^\n\t\t+   %self_attn.19 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %in_proj_bias.5 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.5)\n\t\t?                                                                            ^\n\t\t+   %in_proj_bias.5 : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.19)\n\t\t?                                                                            ^^\n\t\t+   %self_attn.17 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n\t\t-   %in_proj_weight.5 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.5)\n\t\t?                                                                                ^\n\t\t+   %in_proj_weight.5 : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.17)\n\t\t?                                                                                ^^\n\t\t+   %src : Tensor = aten::_transformer_encoder_layer_fwd(%src.7, %49, %50, %in_proj_weight.5, %in_proj_bias.5, %weight.25, %bias.21, %53, %53, %51, %weight.27, %bias.23, %weight.29, %bias.25, %weight.31, %bias.27, %weight.33, %bias.29, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %linear2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t-   %query.9 : Tensor = aten::transpose(%query.7, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %300 : int = aten::size(%query.9, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len.5 : Tensor = prim::NumToTensor(%300), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %302 : int = aten::size(%query.9, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz.5 : Tensor = prim::NumToTensor(%302), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %304 : int = aten::size(%query.9, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim.5 : Tensor = prim::NumToTensor(%304), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %head_dim.5 : Tensor = aten::div(%embed_dim.5, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %307 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %308 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %309 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %310 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %311 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %312 : int = aten::Int(%head_dim.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %313 : int = aten::size(%query.9, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %314 : Tensor = aten::linear(%query.9, %in_proj_weight.5, %in_proj_bias.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %315 : int[] = prim::ListConstruct(%48, %313), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %316 : Tensor = aten::unflatten(%314, %49, %315), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %317 : Tensor = aten::unsqueeze(%316, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %318 : Tensor = aten::transpose(%317, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %319 : Tensor = aten::squeeze(%318, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj.5 : Tensor = aten::contiguous(%319, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.13 : Tensor = aten::select(%proj.5, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.13 : Tensor = aten::select(%proj.5, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.13 : Tensor = aten::select(%proj.5, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %324 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %325 : int = aten::Int(%324), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %326 : int[] = prim::ListConstruct(%300, %325, %312), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %327 : Tensor = aten::view(%q.13, %326), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.15 : Tensor = aten::transpose(%327, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %329 : int = aten::size(%k.13, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %330 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %331 : int = aten::Int(%330), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %332 : int[] = prim::ListConstruct(%329, %331, %311), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %333 : Tensor = aten::view(%k.13, %332), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.15 : Tensor = aten::transpose(%333, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %335 : int = aten::size(%v.13, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %336 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %337 : int = aten::Int(%336), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %338 : int[] = prim::ListConstruct(%335, %337, %310), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %339 : Tensor = aten::view(%v.13, %338), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.15 : Tensor = aten::transpose(%339, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %341 : int = aten::size(%k.15, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %342 : int[] = prim::ListConstruct(%302, %54, %54, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %343 : Tensor = aten::view(%src_key_padding_mask, %342), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %344 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %345 : Tensor = aten::expand(%343, %344, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %346 : Tensor = aten::mul(%bsz.5, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %347 : int = aten::Int(%346), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %348 : int[] = prim::ListConstruct(%347, %54, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %key_padding_mask.5 : Tensor = aten::reshape(%345, %348), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %350 : int[] = prim::ListConstruct(%302, %46, %49, %341), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_mask.5 : Tensor = aten::view(%key_padding_mask.5, %350), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %352 : int[] = prim::ListConstruct(%302, %46, %300, %309), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %q.17 : Tensor = aten::view(%q.15, %352), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %354 : int[] = prim::ListConstruct(%302, %46, %341, %308), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %k.17 : Tensor = aten::view(%k.15, %354), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %356 : int[] = prim::ListConstruct(%302, %46, %341, %307), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %v.17 : Tensor = aten::view(%v.15, %356), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.17 : Tensor = aten::scaled_dot_product_attention(%q.17, %k.17, %v.17, %attn_mask.5, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %359 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %360 : Tensor = aten::permute(%attn_output.17, %359), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %361 : Tensor = aten::contiguous(%360, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %362 : Tensor = aten::mul(%bsz.5, %tgt_len.5), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %363 : int = aten::Int(%362), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %364 : int[] = prim::ListConstruct(%363, %304), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_output.19 : Tensor = aten::view(%361, %364), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.21 : Tensor = aten::linear(%attn_output.19, %weight.41, %bias.37), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %367 : int = aten::size(%attn_output.21, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %368 : int[] = prim::ListConstruct(%300, %302, %367), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn\n\t\t-   %attn_output.23 : Tensor = aten::view(%attn_output.21, %368), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.35 : Tensor = aten::transpose(%attn_output.23, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %371 : Tensor = aten::dropout(%input.35, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.37 : Tensor = aten::add(%query.7, %371, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t-   %bias.39 : Tensor = prim::GetAttr[name=\"bias\"](%norm1.5)\n\t\t-   %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.5)\n\t\t-   %375 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm1\n\t\t-   %input.39 : Tensor = aten::layer_norm(%input.37, %375, %weight.43, %bias.39, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.41 : Tensor = prim::GetAttr[name=\"bias\"](%linear1.5)\n\t\t-   %weight.45 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.5)\n\t\t-   %input.41 : Tensor = aten::linear(%input.39, %weight.45, %bias.41), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.43 : Tensor = aten::relu(%input.41), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.45 : Tensor = aten::dropout(%input.43, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.43 : Tensor = prim::GetAttr[name=\"bias\"](%linear2.5)\n\t\t?         -                                                --\n\t\t+   %bias.39 : Tensor = prim::GetAttr[name=\"bias\"](%linear2)\n\t\t?          +\n\t\t+   %linear2.13 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t-   %weight.47 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.5)\n\t\t?            ^                                                  ^\n\t\t+   %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%linear2.13)\n\t\t?            ^                                                  ^^\n\t\t+   %linear1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t-   %input.47 : Tensor = aten::linear(%input.45, %weight.47, %bias.43), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %385 : Tensor = aten::dropout(%input.47, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.49 : Tensor = aten::add(%input.39, %385, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t-   %bias.45 : Tensor = prim::GetAttr[name=\"bias\"](%norm2.5)\n\t\t?         ^^                                         ^ ^^^^\n\t\t+   %bias.37 : Tensor = prim::GetAttr[name=\"bias\"](%linear1)\n\t\t?         ^^                                        ++ ^^ ^\n\t\t+   %linear1.13 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t-   %weight.49 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.5)\n\t\t?            ^                                           ^ ^^ ^\n\t\t+   %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%linear1.13)\n\t\t?            ^                                          ++ ^^ ^ ^^\n\t\t-   %389 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm2\n\t\t-   %query.11 : Tensor = aten::layer_norm(%input.49, %389, %weight.49, %bias.45, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.2/__module.transformer_encoder.layers.2.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t    %norm2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_3)\n\t\t-   %dropout2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout2\"](%_3)\n\t\t+   %bias.35 : Tensor = prim::GetAttr[name=\"bias\"](%norm2)\n\t\t-   %linear2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear2\"](%_3)\n\t\t?    -- ^^                                   ---  ^^ -                        -- ^^\n\t\t+   %norm2.13 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm2\"](%_3)\n\t\t?     ^ + +++                              +++++  +++++   ^^  ++++                        ^ +\n\t\t+   %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%norm2.13)\n\t\t-   %dropout.9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout\"](%_3)\n\t\t-   %linear1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear1\"](%_3)\n\t\t    %norm1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_3)\n\t\t-   %dropout1 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name=\"dropout1\"](%_3)\n\t\t+   %bias.33 : Tensor = prim::GetAttr[name=\"bias\"](%norm1)\n\t\t+   %norm1.13 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"norm1\"](%_3)\n\t\t+   %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%norm1.13)\n\t\t    %self_attn : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t    %out_proj : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn)\n\t\t-   %bias.47 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n\t\t?         ^^\n\t\t+   %bias.31 : Tensor = prim::GetAttr[name=\"bias\"](%out_proj)\n\t\t?         ^^\n\t\t+   %self_attn.29 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %out_proj.13 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn)\n\t\t+   %out_proj.13 : __torch__.torch.nn.modules.linear.NonDynamicallyQuantizableLinear = prim::GetAttr[name=\"out_proj\"](%self_attn.29)\n\t\t?                                                                                                                               +++\n\t\t-   %weight.51 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.13)\n\t\t?            -\n\t\t+   %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%out_proj.13)\n\t\t?           +\n\t\t+   %self_attn.27 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn)\n\t\t+   %in_proj_bias : Tensor = prim::GetAttr[name=\"in_proj_bias\"](%self_attn.27)\n\t\t?                                                                         +++\n\t\t+   %self_attn.25 : __torch__.torch.nn.modules.activation.MultiheadAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n\t\t-   %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn)\n\t\t+   %in_proj_weight : Tensor = prim::GetAttr[name=\"in_proj_weight\"](%self_attn.25)\n\t\t?                                                                             +++\n\t\t+   %output : Tensor = aten::_transformer_encoder_layer_fwd(%src, %49, %50, %in_proj_weight, %in_proj_bias, %weight.35, %bias.31, %53, %53, %51, %weight.37, %bias.33, %weight.39, %bias.35, %weight.41, %bias.37, %weight.43, %bias.39, %54, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881:0\n\t\t+   %176 : int = aten::size(%src.1, %48), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t-   %query : Tensor = aten::transpose(%query.11, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339:0\n\t\t-   %406 : int = aten::size(%query, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %tgt_len : Tensor = prim::NumToTensor(%406), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %408 : int = aten::size(%query, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %bsz : Tensor = prim::NumToTensor(%408), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %410 : int = aten::size(%query, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6163:0\n\t\t-   %embed_dim : Tensor = prim::NumToTensor(%410), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %head_dim : Tensor = aten::div(%embed_dim, %51, %50), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6207:0\n\t\t-   %413 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %414 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %415 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %416 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %417 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %418 : int = aten::Int(%head_dim), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %419 : int = aten::size(%query, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5610:0\n\t\t-   %420 : Tensor = aten::linear(%query, %in_proj_weight, %in_proj_bias), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5614:0\n\t\t-   %421 : int[] = prim::ListConstruct(%48, %419), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %422 : Tensor = aten::unflatten(%420, %49, %421), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:1432:0\n\t\t-   %423 : Tensor = aten::unsqueeze(%422, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5618:0\n\t\t-   %424 : Tensor = aten::transpose(%423, %53, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5619:0\n\t\t-   %425 : Tensor = aten::squeeze(%424, %47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5620:0\n\t\t-   %proj : Tensor = aten::contiguous(%425, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5621:0\n\t\t-   %q.19 : Tensor = aten::select(%proj, %53, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %k.19 : Tensor = aten::select(%proj, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %v.19 : Tensor = aten::select(%proj, %53, %52), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:5623:0\n\t\t-   %430 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %431 : int = aten::Int(%430), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %432 : int[] = prim::ListConstruct(%406, %431, %418), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %433 : Tensor = aten::view(%q.19, %432), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %q.21 : Tensor = aten::transpose(%433, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6296:0\n\t\t-   %435 : int = aten::size(%k.19, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %436 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %437 : int = aten::Int(%436), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %438 : int[] = prim::ListConstruct(%435, %437, %417), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %439 : Tensor = aten::view(%k.19, %438), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %k.21 : Tensor = aten::transpose(%439, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6298:0\n\t\t-   %441 : int = aten::size(%v.19, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %442 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %443 : int = aten::Int(%442), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %444 : int[] = prim::ListConstruct(%441, %443, %416), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %445 : Tensor = aten::view(%v.19, %444), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %v.21 : Tensor = aten::transpose(%445, %53, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6309:0\n\t\t-   %447 : int = aten::size(%k.21, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6335:0\n\t\t-   %448 : int[] = prim::ListConstruct(%408, %54, %54, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %449 : Tensor = aten::view(%src_key_padding_mask, %448), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6343:0\n\t\t-   %450 : int[] = prim::ListConstruct(%49, %46, %49, %49), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %451 : Tensor = aten::expand(%449, %450, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6344:0\n\t\t-   %452 : Tensor = aten::mul(%bsz, %51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %453 : int = aten::Int(%452), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %454 : int[] = prim::ListConstruct(%453, %54, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %key_padding_mask : Tensor = aten::reshape(%451, %454), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6345:0\n\t\t-   %456 : int[] = prim::ListConstruct(%408, %46, %49, %447), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_mask : Tensor = aten::view(%key_padding_mask, %456), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6404:0\n\t\t-   %458 : int[] = prim::ListConstruct(%408, %46, %406, %415), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %q : Tensor = aten::view(%q.21, %458), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6406:0\n\t\t-   %460 : int[] = prim::ListConstruct(%408, %46, %447, %414), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %k : Tensor = aten::view(%k.21, %460), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6407:0\n\t\t-   %462 : int[] = prim::ListConstruct(%408, %46, %447, %413), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %v : Tensor = aten::view(%v.21, %462), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6408:0\n\t\t-   %attn_output.25 : Tensor = aten::scaled_dot_product_attention(%q, %k, %v, %attn_mask, %45, %60, %61, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6410:0\n\t\t-   %465 : int[] = prim::ListConstruct(%52, %53, %54, %48), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %466 : Tensor = aten::permute(%attn_output.25, %465), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %467 : Tensor = aten::contiguous(%466, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %468 : Tensor = aten::mul(%bsz, %tgt_len), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %469 : int = aten::Int(%468), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %470 : int[] = prim::ListConstruct(%469, %410), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_output.27 : Tensor = aten::view(%467, %470), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6414:0\n\t\t-   %attn_output.29 : Tensor = aten::linear(%attn_output.27, %weight.51, %bias.47), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6417:0\n\t\t-   %473 : int = aten::size(%attn_output.29, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %474 : int[] = prim::ListConstruct(%406, %408, %473), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn\n\t\t-   %attn_output : Tensor = aten::view(%attn_output.29, %474), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:6418:0\n\t\t-   %input.51 : Tensor = aten::transpose(%attn_output, %54, %53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.self_attn # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395:0\n\t\t-   %477 : Tensor = aten::dropout(%input.51, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.53 : Tensor = aten::add(%query.11, %477, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913:0\n\t\t?      -- ------------         ^^^  ^^^ ^  -     ------                                      --------------------------------------                                                                                                  ^^^\n\t\t+   %177 : int = aten::size(%src.1, %47), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t?    ++++++            ^^^^  ^ ^                                                                                                                                               ^^^\n\t\t-   %bias.49 : Tensor = prim::GetAttr[name=\"bias\"](%norm1)\n\t\t-   %weight.53 : Tensor = prim::GetAttr[name=\"weight\"](%norm1)\n\t\t-   %481 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm1\n\t\t-   %input.55 : Tensor = aten::layer_norm(%input.53, %481, %weight.53, %bias.49, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t-   %bias.51 : Tensor = prim::GetAttr[name=\"bias\"](%linear1)\n\t\t-   %weight.55 : Tensor = prim::GetAttr[name=\"weight\"](%linear1)\n\t\t-   %input.57 : Tensor = aten::linear(%input.55, %weight.55, %bias.51), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.linear1 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %input.59 : Tensor = aten::relu(%input.57), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n\t\t-   %input.61 : Tensor = aten::dropout(%input.59, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %bias.53 : Tensor = prim::GetAttr[name=\"bias\"](%linear2)\n\t\t-   %weight.57 : Tensor = prim::GetAttr[name=\"weight\"](%linear2)\n\t\t-   %input.63 : Tensor = aten::linear(%input.61, %weight.57, %bias.53), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.linear2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t-   %491 : Tensor = aten::dropout(%input.63, %55, %60), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.dropout2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t-   %input.65 : Tensor = aten::add(%input.55, %491, %54), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916:0\n\t\t?      -- ------------         ^^^  ^^^^^ ^^    ^^^^^^^                                      --------------------------------------                                                                                                  ^^^\n\t\t+   %178 : int = aten::size(%src.1, %46), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t?    ++++++            ^^^^  ^^^ ^    ^                                                                                                                                        ^^^\n\t\t+   %179 : int[] = prim::ListConstruct(%176, %177, %178), scope: __module.transformer_encoder\n\t\t+   %transformer_output : Tensor = aten::to_padded_tensor(%output, %45, %179), scope: __module.transformer_encoder # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:522:0\n\t\t-   %bias.55 : Tensor = prim::GetAttr[name=\"bias\"](%norm2)\n\t\t-   %weight.59 : Tensor = prim::GetAttr[name=\"weight\"](%norm2)\n\t\t-   %495 : int[] = prim::ListConstruct(%58), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm2\n\t\t-   %transformer_output : Tensor = aten::layer_norm(%input.65, %495, %weight.59, %bias.55, %57, %56), scope: __module.transformer_encoder/__module.transformer_encoder.layers.3/__module.transformer_encoder.layers.3.norm2 # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:2910:0\n\t\t    %13 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %14 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %15 : int = prim::Constant[value=9223372036854775807]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %16 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %17 : Tensor = aten::slice(%transformer_output, %13, %14, %15, %16) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %18 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %19 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %20 : Tensor = aten::select(%17, %18, %19) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %21 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %22 : int = prim::Constant[value=0]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %23 : int = prim::Constant[value=9223372036854775807]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t    %24 : int = prim::Constant[value=1]() # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t-   %input.67 : Tensor = aten::slice(%20, %21, %22, %23, %24) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t?          ^^\n\t\t+   %input.3 : Tensor = aten::slice(%20, %21, %22, %23, %24) # C:\\Users\\Administrator\\AI\\Sentiment-Analysis\\Model.py:50:0\n\t\t?          ^\n\t\t-   %497 : bool = prim::Constant[value=0](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^^\n\t\t+   %181 : bool = prim::Constant[value=0](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^^\n\t\t-   %498 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^^\n\t\t+   %182 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?    ^ +\n\t\t-   %input : Tensor = aten::dropout(%input.67, %498, %497), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?                                          ^^   ^^    ^^^\n\t\t+   %input : Tensor = aten::dropout(%input.3, %182, %181), scope: __module.dropout # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\functional.py:1425:0\n\t\t?                                          ^   ^ +   ^^^\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%fc_out)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%fc_out)\n\t\t-   %502 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.fc_out # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t?    ^^^\n\t\t+   %186 : Tensor = aten::linear(%input, %weight, %bias), scope: __module.fc_out # C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0\n\t\t?    ^^^\n\t\t-   return (%502)\n\t\t?            ^^^\n\t\t+   return (%186)\n\t\t?            ^^^\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %fc_out : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t?                                                             ^^\n\t\t+ %fc_out : __torch__.torch.nn.modules.linear.___torch_mangle_82.Linear = prim::GetAttr[name=\"fc_out\"](%self.1)\n\t\t?                                                             ^^\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Model import Transformer, PositionalEncoding\n",
    "\n",
    "vocab_size = 32000\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "output_dim = 7\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "max_len = 128\n",
    "pad_token_id = 0\n",
    "\n",
    "# 모델 인스턴스 생성 및 평가 모드 설정\n",
    "model = Transformer(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, n_heads, max_len=max_len, pad_token_id=pad_token_id)\n",
    "model.eval() # 모델을 평가 모드로 전환 (Dropout 등을 비활성화)\n",
    "\n",
    "# TensorBoard에 전달할 더미 입력 텐서 생성\n",
    "dummy_input_ids = torch.randint(0, vocab_size, (1, 10)).long()\n",
    "dummy_attention_mask = torch.ones((1, 10)).long()\n",
    "\n",
    "# TensorBoard SummaryWriter 설정\n",
    "# 'runs/transformer_model_graph' 경로에 로그 파일이 저장됩니다.\n",
    "writer = SummaryWriter('onnx/transformer_model_graph')\n",
    "\n",
    "# writer.add_graph를 사용하여 모델의 연산 그래프를 TensorBoard에 추가\n",
    "# 모델 객체와 모델의 forward 메서드에 전달될 더미 입력 튜플을 제공합니다.\n",
    "writer.add_graph(model, (dummy_input_ids, dummy_attention_mask))\n",
    "\n",
    "# writer를 닫아 로그 파일 작성을 완료합니다.\n",
    "writer.close()\n",
    "\n",
    "print(\"TensorBoard 그래프가 'onnx/transformer_model_graph' 경로에 저장되었습니다.\")\n",
    "print(\"TensorBoard를 실행하려면 다음 명령어를 터미널에서 실행하세요:\")\n",
    "print(\"tensorboard --logdir=onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6243998e-2fac-44d0-b39e-d53cddd62dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hiddenlayer in c:\\users\\administrator\\anaconda3\\envs\\tensor\\lib\\site-packages (0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade hiddenlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96c48c-d906-4c64-a04c-a07c67e03158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
