{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051e5a6d-f416-43d1-a8b7-6311dab86336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from Model import Transformer, PositionalEncoding\n",
    "from TrainDataset import prepare_classification_dataset, tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61aac92-20b7-4d5b-87f6-9b21419e9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 df 크기: 88110\n",
      "원본 df 감정 분포 (매핑 후): Counter({4: 48501, 2: 9265, 3: 7221, 5: 7054, 6: 5642, 0: 5565, 1: 4862})\n",
      "학습 데이터프레임 크기: 70488\n",
      "검증 데이터프레임 크기: 17622\n",
      "학습 데이터프레임 감정 분포: Counter({4: 38801, 2: 7412, 3: 5777, 5: 5643, 6: 4513, 0: 4452, 1: 3890})\n",
      "검증 데이터프레임 감정 분포: Counter({4: 9700, 2: 1853, 3: 1444, 5: 1411, 6: 1129, 0: 1113, 1: 972})\n",
      "학습 데이터 파싱 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e04fc1295564813ab32540de995865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/70488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 파싱 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f6867199554d709e4e44bc33db4f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/17622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 70488\n",
      "검증 데이터셋 크기: 17622\n",
      "학습 DataLoader 배치 수: 2203\n",
      "검증 DataLoader 배치 수: 551\n",
      "\n",
      "설정된 전역 변수:\n",
      "  VOCAB_SIZE: 32000\n",
      "  MAX_LEN: 128\n",
      "  OUTPUT_DIM: 7\n",
      "  PAD_TOKEN_ID: 0\n",
      "\n",
      "사용 가능한 장치: cuda\n",
      "\n",
      "Optuna 최적화 버전: v7 (버전 파일: saves\\optuna_version.txt)\n",
      "모든 Optuna Trial 로그 및 모델은 'optuna_runs\\v7' 아래에 저장됩니다.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'datasets/sentiment_train.csv', index_col=0)\n",
    "\n",
    "# '감정' 레이블을 숫자로 매핑\n",
    "df.loc[(df['감정'] == '불안'), '감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'), '감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'), '감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'), '감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'), '감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'), '감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'), '감정'] = 6\n",
    "\n",
    "print(f\"원본 df 크기: {len(df)}\")\n",
    "print(f\"원본 df 감정 분포 (매핑 후): {Counter(df['감정'])}\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, train_size=0.8, test_size=0.2, stratify=df['감정'], random_state=42) # 재현성을 위해 random_state 추가\n",
    "\n",
    "print(f\"학습 데이터프레임 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터프레임 크기: {len(val_df)}\")\n",
    "print(f\"학습 데이터프레임 감정 분포: {Counter(train_df['감정'])}\")\n",
    "print(f\"검증 데이터프레임 감정 분포: {Counter(val_df['감정'])}\")\n",
    "\n",
    "\n",
    "print(\"학습 데이터 파싱 중...\")\n",
    "train_datasets_dict = prepare_classification_dataset(train_df)\n",
    "print(\"검증 데이터 파싱 중...\")\n",
    "val_datasets_dict = prepare_classification_dataset(val_df)\n",
    "\n",
    "train_datasets = tensor_dataset(train_datasets_dict)\n",
    "val_datasets = tensor_dataset(val_datasets_dict)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {len(train_datasets)}\")\n",
    "print(f\"검증 데이터셋 크기: {len(val_datasets)}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"학습 DataLoader 배치 수: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"검증 DataLoader 배치 수: {len(val_loader)}\")\n",
    "\n",
    "VOCAB_SIZE = 32000\n",
    "MAX_LEN =128\n",
    "OUTPUT_DIM = int(df['감정'].nunique())\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "print(f\"\\n설정된 전역 변수:\")\n",
    "print(f\"  VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "print(f\"  MAX_LEN: {MAX_LEN}\")\n",
    "print(f\"  OUTPUT_DIM: {OUTPUT_DIM}\")\n",
    "print(f\"  PAD_TOKEN_ID: {PAD_TOKEN_ID}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n사용 가능한 장치: {device}\")\n",
    "\n",
    "VERSION_FILE_PATH = os.path.join('saves', 'optuna_version.txt')\n",
    "\n",
    "OPTUNA_LOG_MODEL_ROOT_DIR = 'optuna_runs'\n",
    "\n",
    "def get_next_version():\n",
    "    os.makedirs('saves', exist_ok=True) \n",
    "    current_version = 0\n",
    "    if os.path.exists(VERSION_FILE_PATH):\n",
    "        with open(VERSION_FILE_PATH, 'r') as f:\n",
    "            try:\n",
    "                current_version = int(f.read().strip())\n",
    "            except ValueError:\n",
    "                current_version = 0\n",
    "    next_version = current_version + 1\n",
    "    with open(VERSION_FILE_PATH, 'w') as f:\n",
    "        f.write(str(next_version))\n",
    "    return next_version\n",
    "\n",
    "CURRENT_OPTUNA_VERSION = get_next_version()\n",
    "print(f\"\\nOptuna 최적화 버전: v{CURRENT_OPTUNA_VERSION} (버전 파일: {VERSION_FILE_PATH})\")\n",
    "\n",
    "OPTUNA_VERSION_DIR = os.path.join(OPTUNA_LOG_MODEL_ROOT_DIR, f'v{CURRENT_OPTUNA_VERSION}')\n",
    "os.makedirs(OPTUNA_VERSION_DIR, exist_ok=True)\n",
    "print(f\"모든 Optuna Trial 로그 및 모델은 '{OPTUNA_VERSION_DIR}' 아래에 저장됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f581c3-96ac-42de-9c85-80590f798ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau,\n",
    "                      warmup_epochs, log_dir, save_path, patience, trial=None):\n",
    "    best_val_f1_weighted = 0.0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"  [Trial {trial.number}] TensorBoard 로그 디렉토리: {log_dir}\")\n",
    "\n",
    "    if warmup_epochs > 0:\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "        warmup_lr_schedule = lambda e: (e + 1) / warmup_epochs if e < warmup_epochs else 1.0\n",
    "\n",
    "    for e in range(1, epoch + 1):\n",
    "        NN.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        if warmup_epochs > 0 and e <= warmup_epochs:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = initial_lr * warmup_lr_schedule(e - 1)\n",
    "\n",
    "        for batch_idx, (data, attention_mask, labels) in enumerate(train_loader):\n",
    "            data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = NN(data, attention_mask)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        NN.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for data, attention_mask, labels in val_loader:\n",
    "                data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "                predictions = NN(data, attention_mask)\n",
    "                val_preds.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1_weighted = f1_score(val_true, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1_macro = f1_score(val_true, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "        if e > warmup_epochs:\n",
    "            scheduler_plateau.step(val_f1_weighted)\n",
    "\n",
    "        if val_f1_weighted > best_val_f1_weighted:\n",
    "            best_val_f1_weighted = val_f1_weighted\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        print(f\"  [Trial {trial.number}] Epoch {e}\\tTrain Loss: {avg_train_loss:.5f}\\tVal Acc: {val_acc:.4f}\\tVal F1 (Weighted): {val_f1_weighted:.4f}\\tVal F1 (Macro): {val_f1_macro:.4f}\\tNo Improve Epochs: {no_improve_epochs}\")\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, e)\n",
    "        writer.add_scalar('Metrics/Val_Accuracy', val_acc, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Weighted', val_f1_weighted, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Macro', val_f1_macro, e)\n",
    "        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], e)\n",
    "\n",
    "        if trial:\n",
    "            trial.report(val_f1_weighted, e)\n",
    "            if trial.should_prune():\n",
    "                print(f\"Trial {trial.number} is pruned at epoch {e}.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"조기 종료: {patience} 에포크 동안 성능 개선 없음. Trial {trial.number} 종료.\")\n",
    "            break\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return best_val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916700d2-0084-45d5-9fc1-60ea071e7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 1. 하이퍼파라미터 제안\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    n_heads = trial.suggest_categorical('n_heads', [4, 8, 16])\n",
    "    dropout_p = trial.suggest_float('dropout_p', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n",
    "    warmup_epochs = trial.suggest_int('warmup_epochs', 5, 15)\n",
    "    patience = trial.suggest_int('patience', 10, 20)\n",
    "    use_class_weights = trial.suggest_categorical('use_class_weights', [True, False])\n",
    "\n",
    "    if hidden_dim % n_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned(f\"hidden_dim ({hidden_dim}) is not divisible by n_heads ({n_heads})\")\n",
    "\n",
    "    # 2. 모델 인스턴스 생성 및 하이퍼파라미터 적용\n",
    "    NN = Transformer(vocab_size=VOCAB_SIZE, embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                     output_dim=OUTPUT_DIM, n_layers=n_layers, n_heads=n_heads, dropout_p=dropout_p,\n",
    "                     max_len=MAX_LEN, pad_token_id=PAD_TOKEN_ID).to(device)\n",
    "\n",
    "    class_weights = None\n",
    "    if use_class_weights:\n",
    "        all_train_labels_original = train_df['감정'].values.astype(int)\n",
    "        num_classes = NN.output_dim\n",
    "        label_counts_original = np.bincount(all_train_labels_original, minlength=num_classes)\n",
    "        class_counts_tensor = torch.tensor(label_counts_original, dtype=torch.float)\n",
    "        # 0으로 나누는 것을 방지 (매우 드물게 발생할 수 있는 0개 클래스 방지)\n",
    "        class_counts_tensor = torch.where(class_counts_tensor == 0, torch.tensor(1.0), class_counts_tensor) \n",
    "        class_weights = (class_counts_tensor.sum() / class_counts_tensor).to(device)\n",
    "        print(f\"Trial {trial.number}: 계산된 클래스 가중치: {class_weights.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial.number}: 클래스 가중치 미사용.\")\n",
    "\n",
    "\n",
    "    # 4. 손실 함수 (클래스 가중치 적용 여부에 따라 초기화)\n",
    "    if use_class_weights:\n",
    "        loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 5. 옵티마이저\n",
    "    optimizer = optim.AdamW(NN.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 6. 스케줄러\n",
    "    scheduler_plateau = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.8, patience=5, min_lr=1e-7)\n",
    "\n",
    "    # 7. 학습 실행 (Transformer_Train 함수 호출)\n",
    "    log_dir = os.path.join(OPTUNA_VERSION_DIR, f\"trial_{trial.number}_params_emb{embedding_dim}_heads{n_heads}_lr{learning_rate:.1e}_cw{use_class_weights}\")\n",
    "    save_path = os.path.join(OPTUNA_VERSION_DIR, f\"model_trial_{trial.number}.pt\")\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    val_f1_weighted = Transformer_Train(\n",
    "        epoch=10000,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        NN=NN,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        scheduler_plateau=scheduler_plateau,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        log_dir=log_dir,\n",
    "        save_path=save_path,\n",
    "        patience=patience,\n",
    "        trial=trial\n",
    "    )\n",
    "\n",
    "    return val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d693493-8cb2-4cb7-a3f6-4c3b41ed1434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 Study 'Optuna_Storage\\optuna_study.pkl'를 로드합니다...\n",
      "Study 로드 완료. 현재 20개의 트라이얼 기록이 있습니다.\n",
      "Optuna 최적화 시작...\n",
      "Trial 20: 계산된 클래스 가중치: [15.832883834838867, 18.12030792236328, 9.509984016418457, 12.201488494873047, 1.8166542053222656, 12.491228103637695, 15.618878364562988]\n",
      "  [Trial 20] TensorBoard 로그 디렉토리: optuna_runs\\v6\\trial_20_params_emb256_heads16_lr4.9e-05_cwTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16144\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 20] Epoch 1\tTrain Loss: 1.82967\tVal Acc: 0.2609\tVal F1 (Weighted): 0.2910\tVal F1 (Macro): 0.2197\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 2\tTrain Loss: 1.73152\tVal Acc: 0.2925\tVal F1 (Weighted): 0.3144\tVal F1 (Macro): 0.2487\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 3\tTrain Loss: 1.62947\tVal Acc: 0.3931\tVal F1 (Weighted): 0.4203\tVal F1 (Macro): 0.3187\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 4\tTrain Loss: 1.54278\tVal Acc: 0.3859\tVal F1 (Weighted): 0.4118\tVal F1 (Macro): 0.3357\tNo Improve Epochs: 1\n",
      "  [Trial 20] Epoch 5\tTrain Loss: 1.46882\tVal Acc: 0.3515\tVal F1 (Weighted): 0.3751\tVal F1 (Macro): 0.3273\tNo Improve Epochs: 2\n",
      "  [Trial 20] Epoch 6\tTrain Loss: 1.40667\tVal Acc: 0.3855\tVal F1 (Weighted): 0.4078\tVal F1 (Macro): 0.3463\tNo Improve Epochs: 3\n",
      "  [Trial 20] Epoch 7\tTrain Loss: 1.34306\tVal Acc: 0.4343\tVal F1 (Weighted): 0.4572\tVal F1 (Macro): 0.3693\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 8\tTrain Loss: 1.29837\tVal Acc: 0.4411\tVal F1 (Weighted): 0.4651\tVal F1 (Macro): 0.3868\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 9\tTrain Loss: 1.24964\tVal Acc: 0.4513\tVal F1 (Weighted): 0.4806\tVal F1 (Macro): 0.4019\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 10\tTrain Loss: 1.21149\tVal Acc: 0.4801\tVal F1 (Weighted): 0.5029\tVal F1 (Macro): 0.3989\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 11\tTrain Loss: 1.17399\tVal Acc: 0.5031\tVal F1 (Weighted): 0.5302\tVal F1 (Macro): 0.4260\tNo Improve Epochs: 0\n",
      "  [Trial 20] Epoch 12\tTrain Loss: 1.13496\tVal Acc: 0.4522\tVal F1 (Weighted): 0.4783\tVal F1 (Macro): 0.4047\tNo Improve Epochs: 1\n",
      "  [Trial 20] Epoch 13\tTrain Loss: 1.10620\tVal Acc: 0.4004\tVal F1 (Weighted): 0.4190\tVal F1 (Macro): 0.3807\tNo Improve Epochs: 2\n",
      "  [Trial 20] Epoch 14\tTrain Loss: 1.06670\tVal Acc: 0.4673\tVal F1 (Weighted): 0.4933\tVal F1 (Macro): 0.4133\tNo Improve Epochs: 3\n",
      "  [Trial 20] Epoch 15\tTrain Loss: 1.04369\tVal Acc: 0.4796\tVal F1 (Weighted): 0.5059\tVal F1 (Macro): 0.4148\tNo Improve Epochs: 4\n",
      "  [Trial 20] Epoch 16\tTrain Loss: 1.01140\tVal Acc: 0.4652\tVal F1 (Weighted): 0.4915\tVal F1 (Macro): 0.4066\tNo Improve Epochs: 5\n",
      "  [Trial 20] Epoch 17\tTrain Loss: 0.98377\tVal Acc: 0.4578\tVal F1 (Weighted): 0.4855\tVal F1 (Macro): 0.4112\tNo Improve Epochs: 6\n",
      "  [Trial 20] Epoch 18\tTrain Loss: 0.94091\tVal Acc: 0.4578\tVal F1 (Weighted): 0.4831\tVal F1 (Macro): 0.4088\tNo Improve Epochs: 7\n",
      "  [Trial 20] Epoch 19\tTrain Loss: 0.91390\tVal Acc: 0.4638\tVal F1 (Weighted): 0.4929\tVal F1 (Macro): 0.4103\tNo Improve Epochs: 8\n",
      "  [Trial 20] Epoch 20\tTrain Loss: 0.89336\tVal Acc: 0.4352\tVal F1 (Weighted): 0.4583\tVal F1 (Macro): 0.3966\tNo Improve Epochs: 9\n",
      "  [Trial 20] Epoch 21\tTrain Loss: 0.86874\tVal Acc: 0.4387\tVal F1 (Weighted): 0.4649\tVal F1 (Macro): 0.4015\tNo Improve Epochs: 10\n",
      "  [Trial 20] Epoch 22\tTrain Loss: 0.84656\tVal Acc: 0.4634\tVal F1 (Weighted): 0.4915\tVal F1 (Macro): 0.4139\tNo Improve Epochs: 11\n",
      "  [Trial 20] Epoch 23\tTrain Loss: 0.83037\tVal Acc: 0.4304\tVal F1 (Weighted): 0.4555\tVal F1 (Macro): 0.3951\tNo Improve Epochs: 12\n",
      "  [Trial 20] Epoch 24\tTrain Loss: 0.80068\tVal Acc: 0.4762\tVal F1 (Weighted): 0.5046\tVal F1 (Macro): 0.4186\tNo Improve Epochs: 13\n",
      "  [Trial 20] Epoch 25\tTrain Loss: 0.78319\tVal Acc: 0.4664\tVal F1 (Weighted): 0.4936\tVal F1 (Macro): 0.4102\tNo Improve Epochs: 14\n",
      "  [Trial 20] Epoch 26\tTrain Loss: 0.76341\tVal Acc: 0.4407\tVal F1 (Weighted): 0.4669\tVal F1 (Macro): 0.4020\tNo Improve Epochs: 15\n",
      "  [Trial 20] Epoch 27\tTrain Loss: 0.74585\tVal Acc: 0.4737\tVal F1 (Weighted): 0.5012\tVal F1 (Macro): 0.4150\tNo Improve Epochs: 16\n",
      "  [Trial 20] Epoch 28\tTrain Loss: 0.72705\tVal Acc: 0.4552\tVal F1 (Weighted): 0.4829\tVal F1 (Macro): 0.4042\tNo Improve Epochs: 17\n",
      "  [Trial 20] Epoch 29\tTrain Loss: 0.71970\tVal Acc: 0.4936\tVal F1 (Weighted): 0.5197\tVal F1 (Macro): 0.4220\tNo Improve Epochs: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 16:09:58,479] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 20] Epoch 30\tTrain Loss: 0.69455\tVal Acc: 0.4688\tVal F1 (Weighted): 0.4958\tVal F1 (Macro): 0.4123\tNo Improve Epochs: 19\n",
      "Trial 20 is pruned at epoch 30.\n",
      "Trial 21: 클래스 가중치 미사용.\n",
      "  [Trial 21] TensorBoard 로그 디렉토리: optuna_runs\\v6\\trial_21_params_emb256_heads4_lr6.2e-06_cwFalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16144\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 21] Epoch 1\tTrain Loss: 1.67873\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 2\tTrain Loss: 1.54572\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 3\tTrain Loss: 1.50010\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 2\n",
      "  [Trial 21] Epoch 4\tTrain Loss: 1.45851\tVal Acc: 0.5511\tVal F1 (Weighted): 0.4011\tVal F1 (Macro): 0.1178\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 5\tTrain Loss: 1.43469\tVal Acc: 0.5527\tVal F1 (Weighted): 0.4179\tVal F1 (Macro): 0.1436\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 6\tTrain Loss: 1.41743\tVal Acc: 0.5545\tVal F1 (Weighted): 0.4249\tVal F1 (Macro): 0.1538\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 7\tTrain Loss: 1.40801\tVal Acc: 0.5560\tVal F1 (Weighted): 0.4291\tVal F1 (Macro): 0.1582\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 8\tTrain Loss: 1.39424\tVal Acc: 0.5560\tVal F1 (Weighted): 0.4360\tVal F1 (Macro): 0.1662\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 9\tTrain Loss: 1.38232\tVal Acc: 0.5579\tVal F1 (Weighted): 0.4340\tVal F1 (Macro): 0.1653\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 10\tTrain Loss: 1.37011\tVal Acc: 0.5590\tVal F1 (Weighted): 0.4373\tVal F1 (Macro): 0.1707\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 11\tTrain Loss: 1.36086\tVal Acc: 0.5611\tVal F1 (Weighted): 0.4408\tVal F1 (Macro): 0.1743\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 12\tTrain Loss: 1.34936\tVal Acc: 0.5641\tVal F1 (Weighted): 0.4511\tVal F1 (Macro): 0.1911\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 13\tTrain Loss: 1.33926\tVal Acc: 0.5651\tVal F1 (Weighted): 0.4497\tVal F1 (Macro): 0.1892\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 14\tTrain Loss: 1.32822\tVal Acc: 0.5678\tVal F1 (Weighted): 0.4566\tVal F1 (Macro): 0.1978\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 15\tTrain Loss: 1.31919\tVal Acc: 0.5727\tVal F1 (Weighted): 0.4693\tVal F1 (Macro): 0.2164\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 16\tTrain Loss: 1.31275\tVal Acc: 0.5747\tVal F1 (Weighted): 0.4736\tVal F1 (Macro): 0.2227\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 17\tTrain Loss: 1.30627\tVal Acc: 0.5754\tVal F1 (Weighted): 0.4764\tVal F1 (Macro): 0.2282\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 18\tTrain Loss: 1.29798\tVal Acc: 0.5777\tVal F1 (Weighted): 0.4798\tVal F1 (Macro): 0.2330\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 19\tTrain Loss: 1.29135\tVal Acc: 0.5812\tVal F1 (Weighted): 0.4869\tVal F1 (Macro): 0.2432\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 20\tTrain Loss: 1.28346\tVal Acc: 0.5819\tVal F1 (Weighted): 0.4898\tVal F1 (Macro): 0.2476\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 21\tTrain Loss: 1.27628\tVal Acc: 0.5838\tVal F1 (Weighted): 0.4888\tVal F1 (Macro): 0.2481\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 22\tTrain Loss: 1.27259\tVal Acc: 0.5859\tVal F1 (Weighted): 0.4995\tVal F1 (Macro): 0.2639\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 23\tTrain Loss: 1.26886\tVal Acc: 0.5872\tVal F1 (Weighted): 0.5020\tVal F1 (Macro): 0.2674\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 24\tTrain Loss: 1.26224\tVal Acc: 0.5869\tVal F1 (Weighted): 0.4965\tVal F1 (Macro): 0.2602\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 25\tTrain Loss: 1.25857\tVal Acc: 0.5891\tVal F1 (Weighted): 0.5056\tVal F1 (Macro): 0.2735\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 26\tTrain Loss: 1.25229\tVal Acc: 0.5908\tVal F1 (Weighted): 0.5081\tVal F1 (Macro): 0.2771\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 27\tTrain Loss: 1.24916\tVal Acc: 0.5908\tVal F1 (Weighted): 0.5026\tVal F1 (Macro): 0.2695\tNo Improve Epochs: 1\n",
      "  [Trial 21] Epoch 28\tTrain Loss: 1.24500\tVal Acc: 0.5931\tVal F1 (Weighted): 0.5126\tVal F1 (Macro): 0.2827\tNo Improve Epochs: 0\n",
      "  [Trial 21] Epoch 29\tTrain Loss: 1.24067\tVal Acc: 0.5919\tVal F1 (Weighted): 0.5109\tVal F1 (Macro): 0.2812\tNo Improve Epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 16:28:10,121] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 21] Epoch 30\tTrain Loss: 1.23955\tVal Acc: 0.5951\tVal F1 (Weighted): 0.5154\tVal F1 (Macro): 0.2863\tNo Improve Epochs: 0\n",
      "Trial 21 is pruned at epoch 30.\n",
      "Trial 22: 클래스 가중치 미사용.\n",
      "  [Trial 22] TensorBoard 로그 디렉토리: optuna_runs\\v6\\trial_22_params_emb256_heads16_lr8.1e-06_cwFalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16144\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 22] Epoch 1\tTrain Loss: 1.77414\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 2\tTrain Loss: 1.60137\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 1\n",
      "  [Trial 22] Epoch 3\tTrain Loss: 1.57241\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 2\n",
      "  [Trial 22] Epoch 4\tTrain Loss: 1.53597\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3908\tVal F1 (Macro): 0.1014\tNo Improve Epochs: 3\n",
      "  [Trial 22] Epoch 5\tTrain Loss: 1.49092\tVal Acc: 0.5504\tVal F1 (Weighted): 0.3948\tVal F1 (Macro): 0.1068\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 6\tTrain Loss: 1.46383\tVal Acc: 0.5522\tVal F1 (Weighted): 0.4197\tVal F1 (Macro): 0.1406\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 7\tTrain Loss: 1.44205\tVal Acc: 0.5520\tVal F1 (Weighted): 0.4183\tVal F1 (Macro): 0.1420\tNo Improve Epochs: 1\n",
      "  [Trial 22] Epoch 8\tTrain Loss: 1.42671\tVal Acc: 0.5526\tVal F1 (Weighted): 0.4192\tVal F1 (Macro): 0.1436\tNo Improve Epochs: 2\n",
      "  [Trial 22] Epoch 9\tTrain Loss: 1.41579\tVal Acc: 0.5529\tVal F1 (Weighted): 0.4214\tVal F1 (Macro): 0.1462\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 10\tTrain Loss: 1.40112\tVal Acc: 0.5535\tVal F1 (Weighted): 0.4305\tVal F1 (Macro): 0.1574\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 11\tTrain Loss: 1.39146\tVal Acc: 0.5553\tVal F1 (Weighted): 0.4263\tVal F1 (Macro): 0.1537\tNo Improve Epochs: 1\n",
      "  [Trial 22] Epoch 12\tTrain Loss: 1.38440\tVal Acc: 0.5561\tVal F1 (Weighted): 0.4339\tVal F1 (Macro): 0.1629\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 13\tTrain Loss: 1.37166\tVal Acc: 0.5561\tVal F1 (Weighted): 0.4364\tVal F1 (Macro): 0.1656\tNo Improve Epochs: 0\n",
      "  [Trial 22] Epoch 14\tTrain Loss: 1.36559\tVal Acc: 0.5571\tVal F1 (Weighted): 0.4339\tVal F1 (Macro): 0.1636\tNo Improve Epochs: 1\n",
      "  [Trial 22] Epoch 15\tTrain Loss: 1.35523\tVal Acc: 0.5578\tVal F1 (Weighted): 0.4375\tVal F1 (Macro): 0.1684\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-11 16:38:45,181] Trial 22 failed with parameters: {'embedding_dim': 256, 'hidden_dim': 128, 'n_layers': 3, 'n_heads': 16, 'dropout_p': 0.4, 'learning_rate': 8.088945618561149e-06, 'warmup_epochs': 11, 'patience': 13, 'use_class_weights': False} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16144\\2319572601.py\", line 54, in objective\n",
      "    val_f1_weighted = Transformer_Train(\n",
      "                      ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16144\\2591231837.py\", line 28, in Transformer_Train\n",
      "    optimizer.step()\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 485, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 79, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py\", line 246, in step\n",
      "    adam(\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 147, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py\", line 933, in adam\n",
      "    func(\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py\", line 760, in _multi_tensor_adam\n",
      "    torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-11 16:38:45,193] Trial 22 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptuna 최적화 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 최적화 실행\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# n_trials는 추가로 실행할 횟수를 의미합니다.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 만약 이미 5개의 트라이얼을 실행했고, 총 10개를 하고 싶다면 n_trials=5로 다시 호출합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     51\u001b[39m os.makedirs(log_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     52\u001b[39m os.makedirs(os.path.dirname(save_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m val_f1_weighted = \u001b[43mTransformer_Train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m val_f1_weighted\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mTransformer_Train\u001b[39m\u001b[34m(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau, warmup_epochs, log_dir, save_path, patience, trial)\u001b[39m\n\u001b[32m     26\u001b[39m     loss = loss_function(predictions, labels)\n\u001b[32m     27\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     total_loss += loss.item()\n\u001b[32m     30\u001b[39m avg_train_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\optim\\adam.py:760\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    757\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n\u001b[32m    759\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m torch._foreach_addcdiv_(\n\u001b[32m    762\u001b[39m     device_params, device_exp_avgs, exp_avg_sq_sqrt, step_size  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    763\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Optuna Study 저장 폴더 및 파일 경로 설정\n",
    "OPTUNA_STORAGE_DIR = 'Optuna_Storage'\n",
    "STUDY_SAVE_PATH = os.path.join(OPTUNA_STORAGE_DIR, 'optuna_study.pkl')\n",
    "\n",
    "# Optuna 저장 폴더가 없다면 생성\n",
    "os.makedirs(OPTUNA_STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Study 객체를 저장할 변수 미리 선언\n",
    "study = None\n",
    "\n",
    "# 이전에 저장된 Study가 있는지 확인하고 로드\n",
    "if os.path.exists(STUDY_SAVE_PATH):\n",
    "    print(f\"이전 Study '{STUDY_SAVE_PATH}'를 로드합니다...\")\n",
    "    try:\n",
    "        study = joblib.load(STUDY_SAVE_PATH)\n",
    "        print(f\"Study 로드 완료. 현재 {len(study.trials)}개의 트라이얼 기록이 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Study 로드 중 오류 발생: {e}. 새로운 Study를 생성합니다.\")\n",
    "        # 로드 실패 시 새로운 Study 생성\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    "        )\n",
    "else:\n",
    "    print(f\"저장된 Study 파일이 없습니다. 새로운 Study를 생성합니다.\")\n",
    "    # 저장된 파일이 없다면 새로운 Study 생성\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    "    )\n",
    "\n",
    "print(\"Optuna 최적화 시작...\")\n",
    "# 최적화 실행\n",
    "# n_trials는 추가로 실행할 횟수를 의미합니다.\n",
    "# 만약 이미 5개의 트라이얼을 실행했고, 총 10개를 하고 싶다면 n_trials=5로 다시 호출합니다.\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb961e-a06f-4007-8406-2623fa7ac1c8",
   "metadata": {},
   "source": [
    "> tensorboard --logdir optuna_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805dd5cc-b52c-4ba3-a837-0a80e8d611f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STUDY_SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 최적화가 끝난 후 Study 객체 저장\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m최적화 결과를 \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mSTUDY_SAVE_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m에 저장합니다...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m joblib.dump(study, STUDY_SAVE_PATH)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStudy 저장 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'STUDY_SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# 최적화가 끝난 후 Study 객체 저장\n",
    "print(f\"최적화 결과를 '{STUDY_SAVE_PATH}'에 저장합니다...\")\n",
    "joblib.dump(study, STUDY_SAVE_PATH)\n",
    "print(\"Study 저장 완료.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Optuna 최적화 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터 조합 (Best Trial): {study.best_trial.params}\")\n",
    "print(f\"최적의 검증 Weighted F1 (Best Value): {study.best_trial.value:.4f}\")\n",
    "\n",
    "print(\"\\n--- 모든 시도 결과 ---\")\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f\"Trial {i}: Value={trial.value:.4f}, Params={trial.params}, State={trial.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88267bc4-97ca-4231-8a71-19ae59cf235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 Study 'Optuna_Storage\\optuna_study.pkl'를 로드합니다...\n",
      "Study 로드 완료. 현재 20개의 트라이얼 기록이 있습니다.\n",
      "\n",
      "--- Optuna 최적화 결과 ---\n",
      "최적의 하이퍼파라미터 조합 (Best Trial): {'embedding_dim': 256, 'hidden_dim': 128, 'n_layers': 4, 'n_heads': 8, 'dropout_p': 0.4, 'learning_rate': 4.565359362828951e-05, 'warmup_epochs': 5, 'patience': 20, 'use_class_weights': False}\n",
      "최적의 검증 Weighted F1 (Best Value): 0.6540\n",
      "\n",
      "--- 모든 시도 결과 ---\n",
      "Trial 0: Value=0.6028, Params={'embedding_dim': 128, 'hidden_dim': 128, 'n_layers': 2, 'n_heads': 4, 'dropout_p': 0.5, 'learning_rate': 1.1933217143832069e-05, 'warmup_epochs': 11, 'patience': 11, 'use_class_weights': False}, State=1\n",
      "Trial 1: Value=0.6395, Params={'embedding_dim': 128, 'hidden_dim': 256, 'n_layers': 2, 'n_heads': 16, 'dropout_p': 0.1, 'learning_rate': 2.9579485662059165e-05, 'warmup_epochs': 10, 'patience': 20, 'use_class_weights': False}, State=1\n",
      "Trial 2: Value=0.6394, Params={'embedding_dim': 128, 'hidden_dim': 128, 'n_layers': 2, 'n_heads': 16, 'dropout_p': 0.30000000000000004, 'learning_rate': 2.024947513625317e-05, 'warmup_epochs': 14, 'patience': 16, 'use_class_weights': False}, State=1\n",
      "Trial 3: Value=0.6471, Params={'embedding_dim': 256, 'hidden_dim': 256, 'n_layers': 4, 'n_heads': 16, 'dropout_p': 0.2, 'learning_rate': 4.267030055500867e-05, 'warmup_epochs': 13, 'patience': 19, 'use_class_weights': False}, State=1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 모든 시도 결과 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(study.trials):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Value=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m, Params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, State=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "OPTUNA_STORAGE_DIR = 'Optuna_Storage'\n",
    "STUDY_SAVE_PATH = os.path.join(OPTUNA_STORAGE_DIR, 'optuna_study.pkl')\n",
    "\n",
    "# Optuna 저장 폴더가 없다면 생성\n",
    "os.makedirs(OPTUNA_STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Study 객체를 저장할 변수 미리 선언\n",
    "study = None\n",
    "if os.path.exists(STUDY_SAVE_PATH):\n",
    "    print(f\"이전 Study '{STUDY_SAVE_PATH}'를 로드합니다...\")\n",
    "    try:\n",
    "        study = joblib.load(STUDY_SAVE_PATH)\n",
    "        print(f\"Study 로드 완료. 현재 {len(study.trials)}개의 트라이얼 기록이 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Study 로드 중 오류 발생: {e}. 새로운 Study를 생성합니다.\")\n",
    "        # 로드 실패 시 새로운 Study 생성\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    "        )\n",
    "print(\"\\n--- Optuna 최적화 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터 조합 (Best Trial): {study.best_trial.params}\")\n",
    "print(f\"최적의 검증 Weighted F1 (Best Value): {study.best_trial.value:.4f}\")\n",
    "\n",
    "print(\"\\n--- 모든 시도 결과 ---\")\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f\"Trial {i}: Value={trial.value:.4f}, Params={trial.params}, State={trial.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc027cbb-6222-4d31-85d9-67e196dbd26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
