{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e5a6d-f416-43d1-a8b7-6311dab86336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from Model import Transformer, PositionalEncoding\n",
    "from TrainDataset import prepare_classification_dataset, tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61aac92-20b7-4d5b-87f6-9b21419e9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'datasets/sentiment_train.csv', index_col=0)\n",
    "\n",
    "# '감정' 레이블을 숫자로 매핑\n",
    "df.loc[(df['감정'] == '불안'), '감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'), '감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'), '감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'), '감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'), '감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'), '감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'), '감정'] = 6\n",
    "\n",
    "print(f\"원본 df 크기: {len(df)}\")\n",
    "print(f\"원본 df 감정 분포 (매핑 후): {Counter(df['감정'])}\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, train_size=0.8, test_size=0.2, stratify=df['감정'], random_state=42) # 재현성을 위해 random_state 추가\n",
    "\n",
    "print(f\"학습 데이터프레임 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터프레임 크기: {len(val_df)}\")\n",
    "print(f\"학습 데이터프레임 감정 분포: {Counter(train_df['감정'])}\")\n",
    "print(f\"검증 데이터프레임 감정 분포: {Counter(val_df['감정'])}\")\n",
    "\n",
    "\n",
    "print(\"학습 데이터 파싱 중...\")\n",
    "train_datasets_dict = prepare_classification_dataset(train_df)\n",
    "print(\"검증 데이터 파싱 중...\")\n",
    "val_datasets_dict = prepare_classification_dataset(val_df)\n",
    "\n",
    "train_datasets = tensor_dataset(train_datasets_dict)\n",
    "val_datasets = tensor_dataset(val_datasets_dict)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {len(train_datasets)}\")\n",
    "print(f\"검증 데이터셋 크기: {len(val_datasets)}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"학습 DataLoader 배치 수: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"검증 DataLoader 배치 수: {len(val_loader)}\")\n",
    "\n",
    "VOCAB_SIZE = 32000\n",
    "MAX_LEN =128\n",
    "OUTPUT_DIM = df['감정'].unique()\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "print(f\"\\n설정된 전역 변수:\")\n",
    "print(f\"  VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "print(f\"  MAX_LEN: {MAX_LEN}\")\n",
    "print(f\"  OUTPUT_DIM: {OUTPUT_DIM}\")\n",
    "print(f\"  PAD_TOKEN_ID: {PAD_TOKEN_ID}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n사용 가능한 장치: {device}\")\n",
    "\n",
    "VERSION_FILE_PATH = os.path.join('saves', 'optuna_version.txt')\n",
    "\n",
    "OPTUNA_LOG_MODEL_ROOT_DIR = 'optuna_runs'\n",
    "\n",
    "def get_next_version():\n",
    "    os.makedirs('saves', exist_ok=True) \n",
    "    current_version = 0\n",
    "    if os.path.exists(VERSION_FILE_PATH):\n",
    "        with open(VERSION_FILE_PATH, 'r') as f:\n",
    "            try:\n",
    "                current_version = int(f.read().strip())\n",
    "            except ValueError:\n",
    "                current_version = 0\n",
    "    next_version = current_version + 1\n",
    "    with open(VERSION_FILE_PATH, 'w') as f:\n",
    "        f.write(str(next_version))\n",
    "    return next_version\n",
    "\n",
    "CURRENT_OPTUNA_VERSION = get_next_version()\n",
    "print(f\"\\nOptuna 최적화 버전: v{CURRENT_OPTUNA_VERSION} (버전 파일: {VERSION_FILE_PATH})\")\n",
    "\n",
    "OPTUNA_VERSION_DIR = os.path.join(OPTUNA_LOG_MODEL_ROOT_DIR, f'v{CURRENT_OPTUNA_VERSION}')\n",
    "os.makedirs(OPTUNA_VERSION_DIR, exist_ok=True)\n",
    "print(f\"모든 Optuna Trial 로그 및 모델은 '{OPTUNA_VERSION_DIR}' 아래에 저장됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f581c3-96ac-42de-9c85-80590f798ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau,\n",
    "                      warmup_epochs, log_dir, save_path, patience, trial=None):\n",
    "    best_val_f1_weighted = 0.0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"  [Trial {trial.number}] TensorBoard 로그 디렉토리: {log_dir}\")\n",
    "\n",
    "    if warmup_epochs > 0:\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "        warmup_lr_schedule = lambda e: (e + 1) / warmup_epochs if e < warmup_epochs else 1.0\n",
    "\n",
    "    for e in range(1, epoch + 1):\n",
    "        NN.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        if warmup_epochs > 0 and e <= warmup_epochs:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = initial_lr * warmup_lr_schedule(e - 1)\n",
    "\n",
    "        for batch_idx, (data, attention_mask, labels) in enumerate(train_loader):\n",
    "            data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = NN(data, attention_mask)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        NN.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for data, attention_mask, labels in val_loader:\n",
    "                data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "                predictions = NN(data, attention_mask)\n",
    "                val_preds.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1_weighted = f1_score(val_true, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1_macro = f1_score(val_true, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "        if e > warmup_epochs:\n",
    "            scheduler_plateau.step(val_f1_weighted)\n",
    "\n",
    "        if val_f1_weighted > best_val_f1_weighted:\n",
    "            best_val_f1_weighted = val_f1_weighted\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        print(f\"  [Trial {trial.number}] Epoch {e}\\tTrain Loss: {avg_train_loss:.5f}\\tVal Acc: {val_acc:.4f}\\tVal F1 (Weighted): {val_f1_weighted:.4f}\\tVal F1 (Macro): {val_f1_macro:.4f}\\tNo Improve Epochs: {no_improve_epochs}\")\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, e)\n",
    "        writer.add_scalar('Metrics/Val_Accuracy', val_acc, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Weighted', val_f1_weighted, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Macro', val_f1_macro, e)\n",
    "        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], e)\n",
    "\n",
    "        if trial:\n",
    "            trial.report(val_f1_weighted, e)\n",
    "            if trial.should_prune():\n",
    "                print(f\"Trial {trial.number} is pruned at epoch {e}.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"조기 종료: {patience} 에포크 동안 성능 개선 없음. Trial {trial.number} 종료.\")\n",
    "            break\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return best_val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916700d2-0084-45d5-9fc1-60ea071e7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 1. 하이퍼파라미터 제안\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    n_heads = trial.suggest_categorical('n_heads', [4, 8, 16])\n",
    "    dropout_p = trial.suggest_float('dropout_p', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n",
    "    warmup_epochs = trial.suggest_int('warmup_epochs', 5, 15)\n",
    "    patience = trial.suggest_int('patience', 10, 20)\n",
    "    use_class_weights = trial.suggest_categorical('use_class_weights', [True, False])\n",
    "\n",
    "    if hidden_dim % n_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned(f\"hidden_dim ({hidden_dim}) is not divisible by n_heads ({n_heads})\")\n",
    "\n",
    "    # 2. 모델 인스턴스 생성 및 하이퍼파라미터 적용\n",
    "    NN = Transformer(vocab_size=VOCAB_SIZE, embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                     output_dim=OUTPUT_DIM, n_layers=n_layers, n_heads=n_heads, dropout_p=dropout_p,\n",
    "                     max_len=MAX_LEN, pad_token_id=PAD_TOKEN_ID).to(device)\n",
    "\n",
    "    class_weights = None\n",
    "    if use_class_weights:\n",
    "        all_train_labels_original = train_df['감정'].values.astype(int)\n",
    "        num_classes = NN.output_dim\n",
    "        label_counts_original = np.bincount(all_train_labels_original, minlength=num_classes)\n",
    "        class_counts_tensor = torch.tensor(label_counts_original, dtype=torch.float)\n",
    "        # 0으로 나누는 것을 방지 (매우 드물게 발생할 수 있는 0개 클래스 방지)\n",
    "        class_counts_tensor = torch.where(class_counts_tensor == 0, torch.tensor(1.0), class_counts_tensor) \n",
    "        class_weights = (class_counts_tensor.sum() / class_counts_tensor).to(device)\n",
    "        print(f\"Trial {trial.number}: 계산된 클래스 가중치: {class_weights.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial.number}: 클래스 가중치 미사용.\")\n",
    "\n",
    "\n",
    "    # 4. 손실 함수 (클래스 가중치 적용 여부에 따라 초기화)\n",
    "    if use_class_weights:\n",
    "        loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 5. 옵티마이저\n",
    "    optimizer = optim.AdamW(NN.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 6. 스케줄러\n",
    "    scheduler_plateau = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.8, patience=5, min_lr=1e-7)\n",
    "\n",
    "    # 7. 학습 실행 (Transformer_Train 함수 호출)\n",
    "    log_dir = os.path.join(OPTUNA_VERSION_DIR, f\"trial_{trial.number}_params_emb{embedding_dim}_heads{n_heads}_lr{learning_rate:.1e}_cw{use_class_weights}\")\n",
    "    save_path = os.path.join(OPTUNA_VERSION_DIR, f\"model_trial_{trial.number}.pt\")\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    val_f1_weighted = Transformer_Train(\n",
    "        epoch=10000,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        NN=NN,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        scheduler_plateau=scheduler_plateau,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        log_dir=log_dir,\n",
    "        save_path=save_path,\n",
    "        patience=patience,\n",
    "        trial=trial\n",
    "    )\n",
    "\n",
    "    return val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d693493-8cb2-4cb7-a3f6-4c3b41ed1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    ")\n",
    "\n",
    "print(\"Optuna 최적화 시작...\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"\\n--- Optuna 최적화 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터 조합 (Best Trial): {study.best_trial.params}\")\n",
    "print(f\"최적의 검증 Weighted F1 (Best Value): {study.best_trial.value:.4f}\")\n",
    "\n",
    "print(\"\\n--- 모든 시도 결과 ---\")\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f\"Trial {i}: Value={trial.value:.4f}, Params={trial.params}, State={trial.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb961e-a06f-4007-8406-2623fa7ac1c8",
   "metadata": {},
   "source": [
    "> tensorboard --logdir optuna_runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
