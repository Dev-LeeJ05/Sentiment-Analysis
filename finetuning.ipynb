{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051e5a6d-f416-43d1-a8b7-6311dab86336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from Model import Transformer, PositionalEncoding\n",
    "from TrainDataset import prepare_classification_dataset, tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61aac92-20b7-4d5b-87f6-9b21419e9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 df 크기: 138664\n",
      "원본 df 감정 분포 (매핑 후): Counter({4: 48501, 3: 24748, 2: 18171, 0: 14651, 5: 13727, 1: 13224, 6: 5642})\n",
      "학습 데이터프레임 크기: 110931\n",
      "검증 데이터프레임 크기: 27733\n",
      "학습 데이터프레임 감정 분포: Counter({4: 38801, 3: 19798, 2: 14537, 0: 11721, 5: 10981, 1: 10579, 6: 4514})\n",
      "검증 데이터프레임 감정 분포: Counter({4: 9700, 3: 4950, 2: 3634, 0: 2930, 5: 2746, 1: 2645, 6: 1128})\n",
      "학습 데이터 파싱 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f394fb0134f388e6ac4bfaaee3662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/110931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 파싱 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8256b511ff7a44f8aa03e760c4069672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "데이터 파싱 중:   0%|          | 0/27733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 110931\n",
      "검증 데이터셋 크기: 27733\n",
      "학습 DataLoader 배치 수: 3467\n",
      "검증 DataLoader 배치 수: 867\n",
      "\n",
      "설정된 전역 변수:\n",
      "  VOCAB_SIZE: 32000\n",
      "  MAX_LEN: 128\n",
      "  OUTPUT_DIM: 7\n",
      "  PAD_TOKEN_ID: 0\n",
      "\n",
      "사용 가능한 장치: cuda\n",
      "\n",
      "Optuna 최적화 버전: v1 (버전 파일: saves\\optuna_version.txt)\n",
      "모든 Optuna Trial 로그 및 모델은 'optuna_runs\\v1' 아래에 저장됩니다.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'datasets/sentiment_train.csv', index_col=0)\n",
    "\n",
    "# '감정' 레이블을 숫자로 매핑\n",
    "df.loc[(df['감정'] == '불안'), '감정'] = 0\n",
    "df.loc[(df['감정'] == '당황'), '감정'] = 1\n",
    "df.loc[(df['감정'] == '분노'), '감정'] = 2\n",
    "df.loc[(df['감정'] == '슬픔'), '감정'] = 3\n",
    "df.loc[(df['감정'] == '중립'), '감정'] = 4\n",
    "df.loc[(df['감정'] == '행복'), '감정'] = 5\n",
    "df.loc[(df['감정'] == '혐오'), '감정'] = 6\n",
    "\n",
    "print(f\"원본 df 크기: {len(df)}\")\n",
    "print(f\"원본 df 감정 분포 (매핑 후): {Counter(df['감정'])}\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, train_size=0.8, test_size=0.2, stratify=df['감정'], random_state=42) # 재현성을 위해 random_state 추가\n",
    "\n",
    "print(f\"학습 데이터프레임 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터프레임 크기: {len(val_df)}\")\n",
    "print(f\"학습 데이터프레임 감정 분포: {Counter(train_df['감정'])}\")\n",
    "print(f\"검증 데이터프레임 감정 분포: {Counter(val_df['감정'])}\")\n",
    "\n",
    "\n",
    "print(\"학습 데이터 파싱 중...\")\n",
    "train_datasets_dict = prepare_classification_dataset(train_df)\n",
    "print(\"검증 데이터 파싱 중...\")\n",
    "val_datasets_dict = prepare_classification_dataset(val_df)\n",
    "\n",
    "train_datasets = tensor_dataset(train_datasets_dict)\n",
    "val_datasets = tensor_dataset(val_datasets_dict)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {len(train_datasets)}\")\n",
    "print(f\"검증 데이터셋 크기: {len(val_datasets)}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"학습 DataLoader 배치 수: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(f\"검증 DataLoader 배치 수: {len(val_loader)}\")\n",
    "\n",
    "VOCAB_SIZE = 32000\n",
    "MAX_LEN =128\n",
    "OUTPUT_DIM = int(df['감정'].nunique())\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "print(f\"\\n설정된 전역 변수:\")\n",
    "print(f\"  VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "print(f\"  MAX_LEN: {MAX_LEN}\")\n",
    "print(f\"  OUTPUT_DIM: {OUTPUT_DIM}\")\n",
    "print(f\"  PAD_TOKEN_ID: {PAD_TOKEN_ID}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n사용 가능한 장치: {device}\")\n",
    "\n",
    "VERSION_FILE_PATH = os.path.join('saves', 'optuna_version.txt')\n",
    "\n",
    "OPTUNA_LOG_MODEL_ROOT_DIR = 'optuna_runs'\n",
    "\n",
    "def get_next_version():\n",
    "    os.makedirs('saves', exist_ok=True) \n",
    "    current_version = 0\n",
    "    if os.path.exists(VERSION_FILE_PATH):\n",
    "        with open(VERSION_FILE_PATH, 'r') as f:\n",
    "            try:\n",
    "                current_version = int(f.read().strip())\n",
    "            except ValueError:\n",
    "                current_version = 0\n",
    "    next_version = current_version + 1\n",
    "    with open(VERSION_FILE_PATH, 'w') as f:\n",
    "        f.write(str(next_version))\n",
    "    return next_version\n",
    "\n",
    "CURRENT_OPTUNA_VERSION = get_next_version()\n",
    "print(f\"\\nOptuna 최적화 버전: v{CURRENT_OPTUNA_VERSION} (버전 파일: {VERSION_FILE_PATH})\")\n",
    "\n",
    "OPTUNA_VERSION_DIR = os.path.join(OPTUNA_LOG_MODEL_ROOT_DIR, f'v{CURRENT_OPTUNA_VERSION}')\n",
    "os.makedirs(OPTUNA_VERSION_DIR, exist_ok=True)\n",
    "print(f\"모든 Optuna Trial 로그 및 모델은 '{OPTUNA_VERSION_DIR}' 아래에 저장됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f581c3-96ac-42de-9c85-80590f798ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Train(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau,\n",
    "                      warmup_epochs, log_dir, save_path, patience, trial=None):\n",
    "    best_val_f1_weighted = 0.0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"  [Trial {trial.number}] TensorBoard 로그 디렉토리: {log_dir}\")\n",
    "\n",
    "    if warmup_epochs > 0:\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "        warmup_lr_schedule = lambda e: (e + 1) / warmup_epochs if e < warmup_epochs else 1.0\n",
    "\n",
    "    for e in range(1, epoch + 1):\n",
    "        NN.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        if warmup_epochs > 0 and e <= warmup_epochs:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = initial_lr * warmup_lr_schedule(e - 1)\n",
    "\n",
    "        for batch_idx, (data, attention_mask, labels) in enumerate(train_loader):\n",
    "            data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = NN(data, attention_mask)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        NN.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for data, attention_mask, labels in val_loader:\n",
    "                data, attention_mask, labels = data.to(device), attention_mask.to(device), labels.to(device)\n",
    "                predictions = NN(data, attention_mask)\n",
    "                val_preds.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1_weighted = f1_score(val_true, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1_macro = f1_score(val_true, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "        if e > warmup_epochs:\n",
    "            scheduler_plateau.step(val_f1_weighted)\n",
    "\n",
    "        if val_f1_weighted > best_val_f1_weighted:\n",
    "            best_val_f1_weighted = val_f1_weighted\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        print(f\"  [Trial {trial.number}] Epoch {e}\\tTrain Loss: {avg_train_loss:.5f}\\tVal Acc: {val_acc:.4f}\\tVal F1 (Weighted): {val_f1_weighted:.4f}\\tVal F1 (Macro): {val_f1_macro:.4f}\\tNo Improve Epochs: {no_improve_epochs}\")\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, e)\n",
    "        writer.add_scalar('Metrics/Val_Accuracy', val_acc, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Weighted', val_f1_weighted, e)\n",
    "        writer.add_scalar('Metrics/Val_F1_Macro', val_f1_macro, e)\n",
    "        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], e)\n",
    "\n",
    "        if trial:\n",
    "            trial.report(val_f1_weighted, e)\n",
    "            if trial.should_prune():\n",
    "                print(f\"Trial {trial.number} is pruned at epoch {e}.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"조기 종료: {patience} 에포크 동안 성능 개선 없음. Trial {trial.number} 종료.\")\n",
    "            break\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return best_val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916700d2-0084-45d5-9fc1-60ea071e7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 1. 하이퍼파라미터 제안\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    n_heads = trial.suggest_categorical('n_heads', [4, 8, 16])\n",
    "    dropout_p = trial.suggest_float('dropout_p', 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n",
    "    warmup_epochs = trial.suggest_int('warmup_epochs', 5, 15)\n",
    "    patience = trial.suggest_int('patience', 10, 20)\n",
    "    use_class_weights = trial.suggest_categorical('use_class_weights', [True, False])\n",
    "\n",
    "    if hidden_dim % n_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned(f\"hidden_dim ({hidden_dim}) is not divisible by n_heads ({n_heads})\")\n",
    "\n",
    "    # 2. 모델 인스턴스 생성 및 하이퍼파라미터 적용\n",
    "    NN = Transformer(vocab_size=VOCAB_SIZE, embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
    "                     output_dim=OUTPUT_DIM, n_layers=n_layers, n_heads=n_heads, dropout_p=dropout_p,\n",
    "                     max_len=MAX_LEN, pad_token_id=PAD_TOKEN_ID).to(device)\n",
    "\n",
    "    class_weights = None\n",
    "    if use_class_weights:\n",
    "        all_train_labels_original = train_df['감정'].values.astype(int)\n",
    "        num_classes = NN.output_dim\n",
    "        label_counts_original = np.bincount(all_train_labels_original, minlength=num_classes)\n",
    "        class_counts_tensor = torch.tensor(label_counts_original, dtype=torch.float)\n",
    "        # 0으로 나누는 것을 방지 (매우 드물게 발생할 수 있는 0개 클래스 방지)\n",
    "        class_counts_tensor = torch.where(class_counts_tensor == 0, torch.tensor(1.0), class_counts_tensor) \n",
    "        class_weights = (class_counts_tensor.sum() / class_counts_tensor).to(device)\n",
    "        print(f\"Trial {trial.number}: 계산된 클래스 가중치: {class_weights.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Trial {trial.number}: 클래스 가중치 미사용.\")\n",
    "\n",
    "\n",
    "    # 4. 손실 함수 (클래스 가중치 적용 여부에 따라 초기화)\n",
    "    if use_class_weights:\n",
    "        loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 5. 옵티마이저\n",
    "    optimizer = optim.AdamW(NN.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 6. 스케줄러\n",
    "    scheduler_plateau = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.8, patience=5, min_lr=1e-7)\n",
    "\n",
    "    # 7. 학습 실행 (Transformer_Train 함수 호출)\n",
    "    log_dir = os.path.join(OPTUNA_VERSION_DIR, f\"trial_{trial.number}_params_emb{embedding_dim}_heads{n_heads}_lr{learning_rate:.1e}_cw{use_class_weights}\")\n",
    "    save_path = os.path.join(OPTUNA_VERSION_DIR, f\"model_trial_{trial.number}.pt\")\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    val_f1_weighted = Transformer_Train(\n",
    "        epoch=10000,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        NN=NN,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        scheduler_plateau=scheduler_plateau,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        log_dir=log_dir,\n",
    "        save_path=save_path,\n",
    "        patience=patience,\n",
    "        trial=trial\n",
    "    )\n",
    "\n",
    "    return val_f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d693493-8cb2-4cb7-a3f6-4c3b41ed1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 Study 'Optuna_Storage\\optuna_study.pkl'를 로드합니다...\n",
      "Study 로드 완료. 현재 5개의 트라이얼 기록이 있습니다.\n",
      "Optuna 최적화 시작...\n",
      "Trial 5: 클래스 가중치 미사용.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 5] TensorBoard 로그 디렉토리: optuna_runs\\v1\\trial_5_params_emb256_heads8_lr1.1e-05_cwFalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 5] Epoch 1\tTrain Loss: 1.95684\tVal Acc: 0.3911\tVal F1 (Weighted): 0.2504\tVal F1 (Macro): 0.1222\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 2\tTrain Loss: 1.82171\tVal Acc: 0.4433\tVal F1 (Weighted): 0.3102\tVal F1 (Macro): 0.1568\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 3\tTrain Loss: 1.75396\tVal Acc: 0.4460\tVal F1 (Weighted): 0.3109\tVal F1 (Macro): 0.1572\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 4\tTrain Loss: 1.70369\tVal Acc: 0.4469\tVal F1 (Weighted): 0.3090\tVal F1 (Macro): 0.1563\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 5\tTrain Loss: 1.65584\tVal Acc: 0.4459\tVal F1 (Weighted): 0.3085\tVal F1 (Macro): 0.1571\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 6\tTrain Loss: 1.61937\tVal Acc: 0.4432\tVal F1 (Weighted): 0.3176\tVal F1 (Macro): 0.1700\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 7\tTrain Loss: 1.58955\tVal Acc: 0.4450\tVal F1 (Weighted): 0.3221\tVal F1 (Macro): 0.1766\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 8\tTrain Loss: 1.56519\tVal Acc: 0.4531\tVal F1 (Weighted): 0.3217\tVal F1 (Macro): 0.1721\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 9\tTrain Loss: 1.54576\tVal Acc: 0.4563\tVal F1 (Weighted): 0.3265\tVal F1 (Macro): 0.1778\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 10\tTrain Loss: 1.52973\tVal Acc: 0.4614\tVal F1 (Weighted): 0.3342\tVal F1 (Macro): 0.1925\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 11\tTrain Loss: 1.51609\tVal Acc: 0.4627\tVal F1 (Weighted): 0.3311\tVal F1 (Macro): 0.1878\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 12\tTrain Loss: 1.50581\tVal Acc: 0.4638\tVal F1 (Weighted): 0.3358\tVal F1 (Macro): 0.1932\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 13\tTrain Loss: 1.49846\tVal Acc: 0.4656\tVal F1 (Weighted): 0.3386\tVal F1 (Macro): 0.1949\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 14\tTrain Loss: 1.48840\tVal Acc: 0.4684\tVal F1 (Weighted): 0.3436\tVal F1 (Macro): 0.2043\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 15\tTrain Loss: 1.47355\tVal Acc: 0.4787\tVal F1 (Weighted): 0.3662\tVal F1 (Macro): 0.2330\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 16\tTrain Loss: 1.45976\tVal Acc: 0.4842\tVal F1 (Weighted): 0.3772\tVal F1 (Macro): 0.2475\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 17\tTrain Loss: 1.44873\tVal Acc: 0.4907\tVal F1 (Weighted): 0.3906\tVal F1 (Macro): 0.2646\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 18\tTrain Loss: 1.43467\tVal Acc: 0.5039\tVal F1 (Weighted): 0.4217\tVal F1 (Macro): 0.3061\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 19\tTrain Loss: 1.41978\tVal Acc: 0.5118\tVal F1 (Weighted): 0.4395\tVal F1 (Macro): 0.3272\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 20\tTrain Loss: 1.40708\tVal Acc: 0.5190\tVal F1 (Weighted): 0.4504\tVal F1 (Macro): 0.3403\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 21\tTrain Loss: 1.39877\tVal Acc: 0.5210\tVal F1 (Weighted): 0.4536\tVal F1 (Macro): 0.3448\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 22\tTrain Loss: 1.38948\tVal Acc: 0.5239\tVal F1 (Weighted): 0.4575\tVal F1 (Macro): 0.3496\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 23\tTrain Loss: 1.37951\tVal Acc: 0.5307\tVal F1 (Weighted): 0.4715\tVal F1 (Macro): 0.3675\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 24\tTrain Loss: 1.37049\tVal Acc: 0.5361\tVal F1 (Weighted): 0.4820\tVal F1 (Macro): 0.3823\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 25\tTrain Loss: 1.36182\tVal Acc: 0.5396\tVal F1 (Weighted): 0.4859\tVal F1 (Macro): 0.3879\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 26\tTrain Loss: 1.35363\tVal Acc: 0.5453\tVal F1 (Weighted): 0.4951\tVal F1 (Macro): 0.3994\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 27\tTrain Loss: 1.34820\tVal Acc: 0.5480\tVal F1 (Weighted): 0.4985\tVal F1 (Macro): 0.4039\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 28\tTrain Loss: 1.34100\tVal Acc: 0.5496\tVal F1 (Weighted): 0.5006\tVal F1 (Macro): 0.4069\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 29\tTrain Loss: 1.33019\tVal Acc: 0.5543\tVal F1 (Weighted): 0.5078\tVal F1 (Macro): 0.4151\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 30\tTrain Loss: 1.32276\tVal Acc: 0.5558\tVal F1 (Weighted): 0.5089\tVal F1 (Macro): 0.4153\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 31\tTrain Loss: 1.31573\tVal Acc: 0.5601\tVal F1 (Weighted): 0.5157\tVal F1 (Macro): 0.4231\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 32\tTrain Loss: 1.31046\tVal Acc: 0.5632\tVal F1 (Weighted): 0.5203\tVal F1 (Macro): 0.4306\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 33\tTrain Loss: 1.30329\tVal Acc: 0.5673\tVal F1 (Weighted): 0.5273\tVal F1 (Macro): 0.4405\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 34\tTrain Loss: 1.29666\tVal Acc: 0.5690\tVal F1 (Weighted): 0.5292\tVal F1 (Macro): 0.4423\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 35\tTrain Loss: 1.29127\tVal Acc: 0.5686\tVal F1 (Weighted): 0.5273\tVal F1 (Macro): 0.4398\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 36\tTrain Loss: 1.28447\tVal Acc: 0.5743\tVal F1 (Weighted): 0.5342\tVal F1 (Macro): 0.4470\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 37\tTrain Loss: 1.28060\tVal Acc: 0.5754\tVal F1 (Weighted): 0.5340\tVal F1 (Macro): 0.4485\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 38\tTrain Loss: 1.27769\tVal Acc: 0.5744\tVal F1 (Weighted): 0.5356\tVal F1 (Macro): 0.4491\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 39\tTrain Loss: 1.27218\tVal Acc: 0.5785\tVal F1 (Weighted): 0.5431\tVal F1 (Macro): 0.4579\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 40\tTrain Loss: 1.26670\tVal Acc: 0.5794\tVal F1 (Weighted): 0.5430\tVal F1 (Macro): 0.4594\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 41\tTrain Loss: 1.26302\tVal Acc: 0.5842\tVal F1 (Weighted): 0.5511\tVal F1 (Macro): 0.4655\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 42\tTrain Loss: 1.25942\tVal Acc: 0.5848\tVal F1 (Weighted): 0.5526\tVal F1 (Macro): 0.4686\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 43\tTrain Loss: 1.25577\tVal Acc: 0.5877\tVal F1 (Weighted): 0.5560\tVal F1 (Macro): 0.4734\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 44\tTrain Loss: 1.25056\tVal Acc: 0.5872\tVal F1 (Weighted): 0.5529\tVal F1 (Macro): 0.4700\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 45\tTrain Loss: 1.24783\tVal Acc: 0.5905\tVal F1 (Weighted): 0.5590\tVal F1 (Macro): 0.4751\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 46\tTrain Loss: 1.24443\tVal Acc: 0.5898\tVal F1 (Weighted): 0.5590\tVal F1 (Macro): 0.4763\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 47\tTrain Loss: 1.24237\tVal Acc: 0.5904\tVal F1 (Weighted): 0.5616\tVal F1 (Macro): 0.4801\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 48\tTrain Loss: 1.23865\tVal Acc: 0.5931\tVal F1 (Weighted): 0.5645\tVal F1 (Macro): 0.4817\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 49\tTrain Loss: 1.23447\tVal Acc: 0.5943\tVal F1 (Weighted): 0.5666\tVal F1 (Macro): 0.4838\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 50\tTrain Loss: 1.22862\tVal Acc: 0.5943\tVal F1 (Weighted): 0.5673\tVal F1 (Macro): 0.4845\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 51\tTrain Loss: 1.22710\tVal Acc: 0.5966\tVal F1 (Weighted): 0.5689\tVal F1 (Macro): 0.4878\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 52\tTrain Loss: 1.22365\tVal Acc: 0.5956\tVal F1 (Weighted): 0.5674\tVal F1 (Macro): 0.4864\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 53\tTrain Loss: 1.22133\tVal Acc: 0.5981\tVal F1 (Weighted): 0.5718\tVal F1 (Macro): 0.4893\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 54\tTrain Loss: 1.21769\tVal Acc: 0.6003\tVal F1 (Weighted): 0.5742\tVal F1 (Macro): 0.4914\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 55\tTrain Loss: 1.21663\tVal Acc: 0.5994\tVal F1 (Weighted): 0.5737\tVal F1 (Macro): 0.4909\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 56\tTrain Loss: 1.21359\tVal Acc: 0.6005\tVal F1 (Weighted): 0.5748\tVal F1 (Macro): 0.4919\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 57\tTrain Loss: 1.20942\tVal Acc: 0.6016\tVal F1 (Weighted): 0.5743\tVal F1 (Macro): 0.4939\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 58\tTrain Loss: 1.20899\tVal Acc: 0.6021\tVal F1 (Weighted): 0.5756\tVal F1 (Macro): 0.4918\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 59\tTrain Loss: 1.20471\tVal Acc: 0.6039\tVal F1 (Weighted): 0.5798\tVal F1 (Macro): 0.4981\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 60\tTrain Loss: 1.20042\tVal Acc: 0.6024\tVal F1 (Weighted): 0.5778\tVal F1 (Macro): 0.4980\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 61\tTrain Loss: 1.19794\tVal Acc: 0.6042\tVal F1 (Weighted): 0.5803\tVal F1 (Macro): 0.4995\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 62\tTrain Loss: 1.19731\tVal Acc: 0.6064\tVal F1 (Weighted): 0.5808\tVal F1 (Macro): 0.4956\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 63\tTrain Loss: 1.19323\tVal Acc: 0.6052\tVal F1 (Weighted): 0.5811\tVal F1 (Macro): 0.4963\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 64\tTrain Loss: 1.19105\tVal Acc: 0.6064\tVal F1 (Weighted): 0.5825\tVal F1 (Macro): 0.4991\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 65\tTrain Loss: 1.18850\tVal Acc: 0.6056\tVal F1 (Weighted): 0.5836\tVal F1 (Macro): 0.5040\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 66\tTrain Loss: 1.18610\tVal Acc: 0.6048\tVal F1 (Weighted): 0.5807\tVal F1 (Macro): 0.5004\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 67\tTrain Loss: 1.18208\tVal Acc: 0.6091\tVal F1 (Weighted): 0.5880\tVal F1 (Macro): 0.5062\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 68\tTrain Loss: 1.18238\tVal Acc: 0.6085\tVal F1 (Weighted): 0.5872\tVal F1 (Macro): 0.5075\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 69\tTrain Loss: 1.17915\tVal Acc: 0.6080\tVal F1 (Weighted): 0.5869\tVal F1 (Macro): 0.5102\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 70\tTrain Loss: 1.17796\tVal Acc: 0.6091\tVal F1 (Weighted): 0.5864\tVal F1 (Macro): 0.5005\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 71\tTrain Loss: 1.17429\tVal Acc: 0.6095\tVal F1 (Weighted): 0.5866\tVal F1 (Macro): 0.5022\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 72\tTrain Loss: 1.17345\tVal Acc: 0.6129\tVal F1 (Weighted): 0.5891\tVal F1 (Macro): 0.5084\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 73\tTrain Loss: 1.17098\tVal Acc: 0.6126\tVal F1 (Weighted): 0.5913\tVal F1 (Macro): 0.5124\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 74\tTrain Loss: 1.16949\tVal Acc: 0.6091\tVal F1 (Weighted): 0.5888\tVal F1 (Macro): 0.5057\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 75\tTrain Loss: 1.16614\tVal Acc: 0.6126\tVal F1 (Weighted): 0.5938\tVal F1 (Macro): 0.5163\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 76\tTrain Loss: 1.16386\tVal Acc: 0.6130\tVal F1 (Weighted): 0.5923\tVal F1 (Macro): 0.5127\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 77\tTrain Loss: 1.16445\tVal Acc: 0.6154\tVal F1 (Weighted): 0.5924\tVal F1 (Macro): 0.5064\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 78\tTrain Loss: 1.15939\tVal Acc: 0.6117\tVal F1 (Weighted): 0.5934\tVal F1 (Macro): 0.5151\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 79\tTrain Loss: 1.15786\tVal Acc: 0.6138\tVal F1 (Weighted): 0.5919\tVal F1 (Macro): 0.5077\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 80\tTrain Loss: 1.15914\tVal Acc: 0.6145\tVal F1 (Weighted): 0.5923\tVal F1 (Macro): 0.5124\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 81\tTrain Loss: 1.15453\tVal Acc: 0.6119\tVal F1 (Weighted): 0.5949\tVal F1 (Macro): 0.5154\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 82\tTrain Loss: 1.15279\tVal Acc: 0.6152\tVal F1 (Weighted): 0.5953\tVal F1 (Macro): 0.5128\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 83\tTrain Loss: 1.15050\tVal Acc: 0.6138\tVal F1 (Weighted): 0.5955\tVal F1 (Macro): 0.5121\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 84\tTrain Loss: 1.15295\tVal Acc: 0.6165\tVal F1 (Weighted): 0.5936\tVal F1 (Macro): 0.5112\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 85\tTrain Loss: 1.14911\tVal Acc: 0.6169\tVal F1 (Weighted): 0.5957\tVal F1 (Macro): 0.5137\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 86\tTrain Loss: 1.14577\tVal Acc: 0.6166\tVal F1 (Weighted): 0.5973\tVal F1 (Macro): 0.5189\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 87\tTrain Loss: 1.14549\tVal Acc: 0.6203\tVal F1 (Weighted): 0.6013\tVal F1 (Macro): 0.5198\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 88\tTrain Loss: 1.14175\tVal Acc: 0.6132\tVal F1 (Weighted): 0.5970\tVal F1 (Macro): 0.5180\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 89\tTrain Loss: 1.14143\tVal Acc: 0.6175\tVal F1 (Weighted): 0.5973\tVal F1 (Macro): 0.5181\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 90\tTrain Loss: 1.14106\tVal Acc: 0.6180\tVal F1 (Weighted): 0.5987\tVal F1 (Macro): 0.5192\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 91\tTrain Loss: 1.13866\tVal Acc: 0.6173\tVal F1 (Weighted): 0.5982\tVal F1 (Macro): 0.5149\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 92\tTrain Loss: 1.13731\tVal Acc: 0.6174\tVal F1 (Weighted): 0.6015\tVal F1 (Macro): 0.5247\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 93\tTrain Loss: 1.13425\tVal Acc: 0.6168\tVal F1 (Weighted): 0.5983\tVal F1 (Macro): 0.5164\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 94\tTrain Loss: 1.13465\tVal Acc: 0.6199\tVal F1 (Weighted): 0.6009\tVal F1 (Macro): 0.5202\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 95\tTrain Loss: 1.13162\tVal Acc: 0.6216\tVal F1 (Weighted): 0.6053\tVal F1 (Macro): 0.5292\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 96\tTrain Loss: 1.12906\tVal Acc: 0.6215\tVal F1 (Weighted): 0.6027\tVal F1 (Macro): 0.5258\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 97\tTrain Loss: 1.12737\tVal Acc: 0.6198\tVal F1 (Weighted): 0.6002\tVal F1 (Macro): 0.5162\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 98\tTrain Loss: 1.12626\tVal Acc: 0.6172\tVal F1 (Weighted): 0.6029\tVal F1 (Macro): 0.5258\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 99\tTrain Loss: 1.12510\tVal Acc: 0.6197\tVal F1 (Weighted): 0.6033\tVal F1 (Macro): 0.5252\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 100\tTrain Loss: 1.12276\tVal Acc: 0.6224\tVal F1 (Weighted): 0.6060\tVal F1 (Macro): 0.5294\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 101\tTrain Loss: 1.12184\tVal Acc: 0.6185\tVal F1 (Weighted): 0.6034\tVal F1 (Macro): 0.5259\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 102\tTrain Loss: 1.12034\tVal Acc: 0.6227\tVal F1 (Weighted): 0.6050\tVal F1 (Macro): 0.5278\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 103\tTrain Loss: 1.11834\tVal Acc: 0.6178\tVal F1 (Weighted): 0.6030\tVal F1 (Macro): 0.5259\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 104\tTrain Loss: 1.11756\tVal Acc: 0.6173\tVal F1 (Weighted): 0.6041\tVal F1 (Macro): 0.5264\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 105\tTrain Loss: 1.11556\tVal Acc: 0.6185\tVal F1 (Weighted): 0.6027\tVal F1 (Macro): 0.5250\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 106\tTrain Loss: 1.11216\tVal Acc: 0.6243\tVal F1 (Weighted): 0.6067\tVal F1 (Macro): 0.5279\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 107\tTrain Loss: 1.11306\tVal Acc: 0.6232\tVal F1 (Weighted): 0.6069\tVal F1 (Macro): 0.5268\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 108\tTrain Loss: 1.11237\tVal Acc: 0.6220\tVal F1 (Weighted): 0.6090\tVal F1 (Macro): 0.5362\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 109\tTrain Loss: 1.10763\tVal Acc: 0.6208\tVal F1 (Weighted): 0.6055\tVal F1 (Macro): 0.5313\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 110\tTrain Loss: 1.10737\tVal Acc: 0.6238\tVal F1 (Weighted): 0.6082\tVal F1 (Macro): 0.5311\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 111\tTrain Loss: 1.10576\tVal Acc: 0.6207\tVal F1 (Weighted): 0.6086\tVal F1 (Macro): 0.5348\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 112\tTrain Loss: 1.10581\tVal Acc: 0.6242\tVal F1 (Weighted): 0.6062\tVal F1 (Macro): 0.5250\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 113\tTrain Loss: 1.10383\tVal Acc: 0.6231\tVal F1 (Weighted): 0.6067\tVal F1 (Macro): 0.5261\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 114\tTrain Loss: 1.10398\tVal Acc: 0.6228\tVal F1 (Weighted): 0.6061\tVal F1 (Macro): 0.5271\tNo Improve Epochs: 6\n",
      "  [Trial 5] Epoch 115\tTrain Loss: 1.10139\tVal Acc: 0.6233\tVal F1 (Weighted): 0.6088\tVal F1 (Macro): 0.5330\tNo Improve Epochs: 7\n",
      "  [Trial 5] Epoch 116\tTrain Loss: 1.10126\tVal Acc: 0.6201\tVal F1 (Weighted): 0.6073\tVal F1 (Macro): 0.5292\tNo Improve Epochs: 8\n",
      "  [Trial 5] Epoch 117\tTrain Loss: 1.09776\tVal Acc: 0.6217\tVal F1 (Weighted): 0.6072\tVal F1 (Macro): 0.5275\tNo Improve Epochs: 9\n",
      "  [Trial 5] Epoch 118\tTrain Loss: 1.09749\tVal Acc: 0.6239\tVal F1 (Weighted): 0.6098\tVal F1 (Macro): 0.5316\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 119\tTrain Loss: 1.09757\tVal Acc: 0.6256\tVal F1 (Weighted): 0.6088\tVal F1 (Macro): 0.5297\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 120\tTrain Loss: 1.09514\tVal Acc: 0.6205\tVal F1 (Weighted): 0.6069\tVal F1 (Macro): 0.5284\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 121\tTrain Loss: 1.09535\tVal Acc: 0.6266\tVal F1 (Weighted): 0.6109\tVal F1 (Macro): 0.5322\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 122\tTrain Loss: 1.09368\tVal Acc: 0.6238\tVal F1 (Weighted): 0.6065\tVal F1 (Macro): 0.5252\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 123\tTrain Loss: 1.09175\tVal Acc: 0.6250\tVal F1 (Weighted): 0.6120\tVal F1 (Macro): 0.5383\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 124\tTrain Loss: 1.09015\tVal Acc: 0.6216\tVal F1 (Weighted): 0.6097\tVal F1 (Macro): 0.5350\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 125\tTrain Loss: 1.08878\tVal Acc: 0.6268\tVal F1 (Weighted): 0.6089\tVal F1 (Macro): 0.5212\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 126\tTrain Loss: 1.09083\tVal Acc: 0.6228\tVal F1 (Weighted): 0.6097\tVal F1 (Macro): 0.5328\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 127\tTrain Loss: 1.08854\tVal Acc: 0.6220\tVal F1 (Weighted): 0.6078\tVal F1 (Macro): 0.5314\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 128\tTrain Loss: 1.08664\tVal Acc: 0.6235\tVal F1 (Weighted): 0.6122\tVal F1 (Macro): 0.5392\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 129\tTrain Loss: 1.08423\tVal Acc: 0.6230\tVal F1 (Weighted): 0.6104\tVal F1 (Macro): 0.5354\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 130\tTrain Loss: 1.08396\tVal Acc: 0.6261\tVal F1 (Weighted): 0.6135\tVal F1 (Macro): 0.5346\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 131\tTrain Loss: 1.08592\tVal Acc: 0.6247\tVal F1 (Weighted): 0.6108\tVal F1 (Macro): 0.5305\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 132\tTrain Loss: 1.08357\tVal Acc: 0.6242\tVal F1 (Weighted): 0.6124\tVal F1 (Macro): 0.5356\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 133\tTrain Loss: 1.08324\tVal Acc: 0.6235\tVal F1 (Weighted): 0.6103\tVal F1 (Macro): 0.5312\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 134\tTrain Loss: 1.08126\tVal Acc: 0.6244\tVal F1 (Weighted): 0.6090\tVal F1 (Macro): 0.5285\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 135\tTrain Loss: 1.07883\tVal Acc: 0.6239\tVal F1 (Weighted): 0.6119\tVal F1 (Macro): 0.5382\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 136\tTrain Loss: 1.08021\tVal Acc: 0.6239\tVal F1 (Weighted): 0.6134\tVal F1 (Macro): 0.5405\tNo Improve Epochs: 6\n",
      "  [Trial 5] Epoch 137\tTrain Loss: 1.07781\tVal Acc: 0.6262\tVal F1 (Weighted): 0.6149\tVal F1 (Macro): 0.5371\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 138\tTrain Loss: 1.07810\tVal Acc: 0.6212\tVal F1 (Weighted): 0.6090\tVal F1 (Macro): 0.5299\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 139\tTrain Loss: 1.07737\tVal Acc: 0.6246\tVal F1 (Weighted): 0.6094\tVal F1 (Macro): 0.5270\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 140\tTrain Loss: 1.07833\tVal Acc: 0.6262\tVal F1 (Weighted): 0.6148\tVal F1 (Macro): 0.5386\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 141\tTrain Loss: 1.07464\tVal Acc: 0.6277\tVal F1 (Weighted): 0.6136\tVal F1 (Macro): 0.5366\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 142\tTrain Loss: 1.07397\tVal Acc: 0.6236\tVal F1 (Weighted): 0.6133\tVal F1 (Macro): 0.5399\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 143\tTrain Loss: 1.07489\tVal Acc: 0.6269\tVal F1 (Weighted): 0.6139\tVal F1 (Macro): 0.5376\tNo Improve Epochs: 6\n",
      "  [Trial 5] Epoch 144\tTrain Loss: 1.07337\tVal Acc: 0.6248\tVal F1 (Weighted): 0.6128\tVal F1 (Macro): 0.5357\tNo Improve Epochs: 7\n",
      "  [Trial 5] Epoch 145\tTrain Loss: 1.07145\tVal Acc: 0.6260\tVal F1 (Weighted): 0.6150\tVal F1 (Macro): 0.5408\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 146\tTrain Loss: 1.07136\tVal Acc: 0.6285\tVal F1 (Weighted): 0.6139\tVal F1 (Macro): 0.5371\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 147\tTrain Loss: 1.07117\tVal Acc: 0.6273\tVal F1 (Weighted): 0.6150\tVal F1 (Macro): 0.5380\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 148\tTrain Loss: 1.07077\tVal Acc: 0.6278\tVal F1 (Weighted): 0.6133\tVal F1 (Macro): 0.5337\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 149\tTrain Loss: 1.06979\tVal Acc: 0.6245\tVal F1 (Weighted): 0.6128\tVal F1 (Macro): 0.5357\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 150\tTrain Loss: 1.06940\tVal Acc: 0.6252\tVal F1 (Weighted): 0.6140\tVal F1 (Macro): 0.5382\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 151\tTrain Loss: 1.06971\tVal Acc: 0.6285\tVal F1 (Weighted): 0.6150\tVal F1 (Macro): 0.5358\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 152\tTrain Loss: 1.06664\tVal Acc: 0.6276\tVal F1 (Weighted): 0.6137\tVal F1 (Macro): 0.5359\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 153\tTrain Loss: 1.06557\tVal Acc: 0.6276\tVal F1 (Weighted): 0.6158\tVal F1 (Macro): 0.5399\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 154\tTrain Loss: 1.07103\tVal Acc: 0.6299\tVal F1 (Weighted): 0.6168\tVal F1 (Macro): 0.5391\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 155\tTrain Loss: 1.06529\tVal Acc: 0.6258\tVal F1 (Weighted): 0.6129\tVal F1 (Macro): 0.5357\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 156\tTrain Loss: 1.06489\tVal Acc: 0.6260\tVal F1 (Weighted): 0.6158\tVal F1 (Macro): 0.5389\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 157\tTrain Loss: 1.06298\tVal Acc: 0.6272\tVal F1 (Weighted): 0.6158\tVal F1 (Macro): 0.5385\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 158\tTrain Loss: 1.06551\tVal Acc: 0.6246\tVal F1 (Weighted): 0.6129\tVal F1 (Macro): 0.5367\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 159\tTrain Loss: 1.06546\tVal Acc: 0.6264\tVal F1 (Weighted): 0.6161\tVal F1 (Macro): 0.5413\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 160\tTrain Loss: 1.06228\tVal Acc: 0.6269\tVal F1 (Weighted): 0.6158\tVal F1 (Macro): 0.5392\tNo Improve Epochs: 6\n",
      "  [Trial 5] Epoch 161\tTrain Loss: 1.06244\tVal Acc: 0.6287\tVal F1 (Weighted): 0.6177\tVal F1 (Macro): 0.5412\tNo Improve Epochs: 0\n",
      "  [Trial 5] Epoch 162\tTrain Loss: 1.06518\tVal Acc: 0.6258\tVal F1 (Weighted): 0.6140\tVal F1 (Macro): 0.5379\tNo Improve Epochs: 1\n",
      "  [Trial 5] Epoch 163\tTrain Loss: 1.06331\tVal Acc: 0.6277\tVal F1 (Weighted): 0.6150\tVal F1 (Macro): 0.5381\tNo Improve Epochs: 2\n",
      "  [Trial 5] Epoch 164\tTrain Loss: 1.06243\tVal Acc: 0.6277\tVal F1 (Weighted): 0.6161\tVal F1 (Macro): 0.5395\tNo Improve Epochs: 3\n",
      "  [Trial 5] Epoch 165\tTrain Loss: 1.06030\tVal Acc: 0.6291\tVal F1 (Weighted): 0.6154\tVal F1 (Macro): 0.5380\tNo Improve Epochs: 4\n",
      "  [Trial 5] Epoch 166\tTrain Loss: 1.06006\tVal Acc: 0.6283\tVal F1 (Weighted): 0.6168\tVal F1 (Macro): 0.5417\tNo Improve Epochs: 5\n",
      "  [Trial 5] Epoch 167\tTrain Loss: 1.05995\tVal Acc: 0.6300\tVal F1 (Weighted): 0.6163\tVal F1 (Macro): 0.5373\tNo Improve Epochs: 6\n",
      "  [Trial 5] Epoch 168\tTrain Loss: 1.05814\tVal Acc: 0.6259\tVal F1 (Weighted): 0.6151\tVal F1 (Macro): 0.5399\tNo Improve Epochs: 7\n",
      "  [Trial 5] Epoch 169\tTrain Loss: 1.06010\tVal Acc: 0.6274\tVal F1 (Weighted): 0.6146\tVal F1 (Macro): 0.5380\tNo Improve Epochs: 8\n",
      "  [Trial 5] Epoch 170\tTrain Loss: 1.05804\tVal Acc: 0.6276\tVal F1 (Weighted): 0.6167\tVal F1 (Macro): 0.5416\tNo Improve Epochs: 9\n",
      "  [Trial 5] Epoch 171\tTrain Loss: 1.05835\tVal Acc: 0.6290\tVal F1 (Weighted): 0.6172\tVal F1 (Macro): 0.5397\tNo Improve Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 11:42:25,464] Trial 5 finished with value: 0.6176702581189648 and parameters: {'embedding_dim': 256, 'hidden_dim': 512, 'n_layers': 4, 'n_heads': 8, 'dropout_p': 0.5, 'learning_rate': 1.0965119432387575e-05, 'warmup_epochs': 10, 'patience': 11, 'use_class_weights': False}. Best is trial 3 with value: 0.6471016727207676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 5] Epoch 172\tTrain Loss: 1.05827\tVal Acc: 0.6296\tVal F1 (Weighted): 0.6174\tVal F1 (Macro): 0.5414\tNo Improve Epochs: 11\n",
      "조기 종료: 11 에포크 동안 성능 개선 없음. Trial 5 종료.\n",
      "Trial 6: 계산된 클래스 가중치: [9.46429443359375, 10.485962867736816, 7.630941867828369, 5.603141784667969, 2.8589727878570557, 10.10208511352539, 24.574878692626953]\n",
      "  [Trial 6] TensorBoard 로그 디렉토리: optuna_runs\\v1\\trial_6_params_emb128_heads4_lr4.1e-05_cwTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 6] Epoch 1\tTrain Loss: 2.02568\tVal Acc: 0.3751\tVal F1 (Weighted): 0.3354\tVal F1 (Macro): 0.2231\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 2\tTrain Loss: 1.90712\tVal Acc: 0.4053\tVal F1 (Weighted): 0.3384\tVal F1 (Macro): 0.2095\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 3\tTrain Loss: 1.82179\tVal Acc: 0.3844\tVal F1 (Weighted): 0.3250\tVal F1 (Macro): 0.2088\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 4\tTrain Loss: 1.77834\tVal Acc: 0.3930\tVal F1 (Weighted): 0.3308\tVal F1 (Macro): 0.2159\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 5\tTrain Loss: 1.74842\tVal Acc: 0.4041\tVal F1 (Weighted): 0.3471\tVal F1 (Macro): 0.2304\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 6\tTrain Loss: 1.72473\tVal Acc: 0.4183\tVal F1 (Weighted): 0.3587\tVal F1 (Macro): 0.2457\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 7\tTrain Loss: 1.69897\tVal Acc: 0.4308\tVal F1 (Weighted): 0.3855\tVal F1 (Macro): 0.2781\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 8\tTrain Loss: 1.66879\tVal Acc: 0.4620\tVal F1 (Weighted): 0.4329\tVal F1 (Macro): 0.3422\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 9\tTrain Loss: 1.62069\tVal Acc: 0.4649\tVal F1 (Weighted): 0.4587\tVal F1 (Macro): 0.3780\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 10\tTrain Loss: 1.57720\tVal Acc: 0.4830\tVal F1 (Weighted): 0.4740\tVal F1 (Macro): 0.4010\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 11\tTrain Loss: 1.53944\tVal Acc: 0.4915\tVal F1 (Weighted): 0.4880\tVal F1 (Macro): 0.4153\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 12\tTrain Loss: 1.50775\tVal Acc: 0.4936\tVal F1 (Weighted): 0.4964\tVal F1 (Macro): 0.4279\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 13\tTrain Loss: 1.47611\tVal Acc: 0.4907\tVal F1 (Weighted): 0.4977\tVal F1 (Macro): 0.4328\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 14\tTrain Loss: 1.45824\tVal Acc: 0.5177\tVal F1 (Weighted): 0.5179\tVal F1 (Macro): 0.4512\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 15\tTrain Loss: 1.43614\tVal Acc: 0.5215\tVal F1 (Weighted): 0.5250\tVal F1 (Macro): 0.4603\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 16\tTrain Loss: 1.41650\tVal Acc: 0.5212\tVal F1 (Weighted): 0.5258\tVal F1 (Macro): 0.4647\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 17\tTrain Loss: 1.40008\tVal Acc: 0.5229\tVal F1 (Weighted): 0.5278\tVal F1 (Macro): 0.4666\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 18\tTrain Loss: 1.38489\tVal Acc: 0.5288\tVal F1 (Weighted): 0.5354\tVal F1 (Macro): 0.4745\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 19\tTrain Loss: 1.36883\tVal Acc: 0.5377\tVal F1 (Weighted): 0.5456\tVal F1 (Macro): 0.4816\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 20\tTrain Loss: 1.35601\tVal Acc: 0.5243\tVal F1 (Weighted): 0.5401\tVal F1 (Macro): 0.4810\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 21\tTrain Loss: 1.34195\tVal Acc: 0.5428\tVal F1 (Weighted): 0.5501\tVal F1 (Macro): 0.4891\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 22\tTrain Loss: 1.33374\tVal Acc: 0.5595\tVal F1 (Weighted): 0.5588\tVal F1 (Macro): 0.4970\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 23\tTrain Loss: 1.32388\tVal Acc: 0.5523\tVal F1 (Weighted): 0.5557\tVal F1 (Macro): 0.4963\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 24\tTrain Loss: 1.31217\tVal Acc: 0.5508\tVal F1 (Weighted): 0.5569\tVal F1 (Macro): 0.4983\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 25\tTrain Loss: 1.30491\tVal Acc: 0.5577\tVal F1 (Weighted): 0.5610\tVal F1 (Macro): 0.5024\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 26\tTrain Loss: 1.29472\tVal Acc: 0.5550\tVal F1 (Weighted): 0.5611\tVal F1 (Macro): 0.5040\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 27\tTrain Loss: 1.28533\tVal Acc: 0.5484\tVal F1 (Weighted): 0.5593\tVal F1 (Macro): 0.5025\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 28\tTrain Loss: 1.27739\tVal Acc: 0.5515\tVal F1 (Weighted): 0.5617\tVal F1 (Macro): 0.5059\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 29\tTrain Loss: 1.26662\tVal Acc: 0.5606\tVal F1 (Weighted): 0.5673\tVal F1 (Macro): 0.5088\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 30\tTrain Loss: 1.26106\tVal Acc: 0.5616\tVal F1 (Weighted): 0.5670\tVal F1 (Macro): 0.5127\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 31\tTrain Loss: 1.25457\tVal Acc: 0.5628\tVal F1 (Weighted): 0.5722\tVal F1 (Macro): 0.5140\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 32\tTrain Loss: 1.24545\tVal Acc: 0.5552\tVal F1 (Weighted): 0.5626\tVal F1 (Macro): 0.5072\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 33\tTrain Loss: 1.23929\tVal Acc: 0.5703\tVal F1 (Weighted): 0.5752\tVal F1 (Macro): 0.5196\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 34\tTrain Loss: 1.23195\tVal Acc: 0.5557\tVal F1 (Weighted): 0.5662\tVal F1 (Macro): 0.5125\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 35\tTrain Loss: 1.22980\tVal Acc: 0.5657\tVal F1 (Weighted): 0.5741\tVal F1 (Macro): 0.5184\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 36\tTrain Loss: 1.21946\tVal Acc: 0.5658\tVal F1 (Weighted): 0.5753\tVal F1 (Macro): 0.5200\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 37\tTrain Loss: 1.21201\tVal Acc: 0.5634\tVal F1 (Weighted): 0.5743\tVal F1 (Macro): 0.5210\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 38\tTrain Loss: 1.21070\tVal Acc: 0.5588\tVal F1 (Weighted): 0.5729\tVal F1 (Macro): 0.5182\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 39\tTrain Loss: 1.20300\tVal Acc: 0.5723\tVal F1 (Weighted): 0.5794\tVal F1 (Macro): 0.5243\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 40\tTrain Loss: 1.19811\tVal Acc: 0.5548\tVal F1 (Weighted): 0.5671\tVal F1 (Macro): 0.5154\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 41\tTrain Loss: 1.19323\tVal Acc: 0.5706\tVal F1 (Weighted): 0.5777\tVal F1 (Macro): 0.5249\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 42\tTrain Loss: 1.18919\tVal Acc: 0.5648\tVal F1 (Weighted): 0.5780\tVal F1 (Macro): 0.5236\tNo Improve Epochs: 3\n",
      "  [Trial 6] Epoch 43\tTrain Loss: 1.18316\tVal Acc: 0.5694\tVal F1 (Weighted): 0.5786\tVal F1 (Macro): 0.5267\tNo Improve Epochs: 4\n",
      "  [Trial 6] Epoch 44\tTrain Loss: 1.17647\tVal Acc: 0.5711\tVal F1 (Weighted): 0.5793\tVal F1 (Macro): 0.5266\tNo Improve Epochs: 5\n",
      "  [Trial 6] Epoch 45\tTrain Loss: 1.17594\tVal Acc: 0.5763\tVal F1 (Weighted): 0.5817\tVal F1 (Macro): 0.5296\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 46\tTrain Loss: 1.16866\tVal Acc: 0.5948\tVal F1 (Weighted): 0.5989\tVal F1 (Macro): 0.5408\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 47\tTrain Loss: 1.16428\tVal Acc: 0.5806\tVal F1 (Weighted): 0.5881\tVal F1 (Macro): 0.5338\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 48\tTrain Loss: 1.16011\tVal Acc: 0.5809\tVal F1 (Weighted): 0.5893\tVal F1 (Macro): 0.5366\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 49\tTrain Loss: 1.15463\tVal Acc: 0.5829\tVal F1 (Weighted): 0.5923\tVal F1 (Macro): 0.5381\tNo Improve Epochs: 3\n",
      "  [Trial 6] Epoch 50\tTrain Loss: 1.15360\tVal Acc: 0.5980\tVal F1 (Weighted): 0.6015\tVal F1 (Macro): 0.5431\tNo Improve Epochs: 0\n",
      "  [Trial 6] Epoch 51\tTrain Loss: 1.15043\tVal Acc: 0.5842\tVal F1 (Weighted): 0.5905\tVal F1 (Macro): 0.5365\tNo Improve Epochs: 1\n",
      "  [Trial 6] Epoch 52\tTrain Loss: 1.14572\tVal Acc: 0.5775\tVal F1 (Weighted): 0.5864\tVal F1 (Macro): 0.5338\tNo Improve Epochs: 2\n",
      "  [Trial 6] Epoch 53\tTrain Loss: 1.14023\tVal Acc: 0.5793\tVal F1 (Weighted): 0.5890\tVal F1 (Macro): 0.5364\tNo Improve Epochs: 3\n",
      "  [Trial 6] Epoch 54\tTrain Loss: 1.13520\tVal Acc: 0.5859\tVal F1 (Weighted): 0.5920\tVal F1 (Macro): 0.5386\tNo Improve Epochs: 4\n",
      "  [Trial 6] Epoch 55\tTrain Loss: 1.13485\tVal Acc: 0.5827\tVal F1 (Weighted): 0.5893\tVal F1 (Macro): 0.5357\tNo Improve Epochs: 5\n",
      "  [Trial 6] Epoch 56\tTrain Loss: 1.12680\tVal Acc: 0.5795\tVal F1 (Weighted): 0.5899\tVal F1 (Macro): 0.5356\tNo Improve Epochs: 6\n",
      "  [Trial 6] Epoch 57\tTrain Loss: 1.12675\tVal Acc: 0.5861\tVal F1 (Weighted): 0.5945\tVal F1 (Macro): 0.5418\tNo Improve Epochs: 7\n",
      "  [Trial 6] Epoch 58\tTrain Loss: 1.11884\tVal Acc: 0.5821\tVal F1 (Weighted): 0.5912\tVal F1 (Macro): 0.5392\tNo Improve Epochs: 8\n",
      "  [Trial 6] Epoch 59\tTrain Loss: 1.12106\tVal Acc: 0.5902\tVal F1 (Weighted): 0.5971\tVal F1 (Macro): 0.5429\tNo Improve Epochs: 9\n",
      "  [Trial 6] Epoch 60\tTrain Loss: 1.11401\tVal Acc: 0.5872\tVal F1 (Weighted): 0.5939\tVal F1 (Macro): 0.5399\tNo Improve Epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 12:33:03,560] Trial 6 finished with value: 0.6014670269901999 and parameters: {'embedding_dim': 128, 'hidden_dim': 512, 'n_layers': 3, 'n_heads': 4, 'dropout_p': 0.30000000000000004, 'learning_rate': 4.0522106937852944e-05, 'warmup_epochs': 12, 'patience': 11, 'use_class_weights': True}. Best is trial 3 with value: 0.6471016727207676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 6] Epoch 61\tTrain Loss: 1.11248\tVal Acc: 0.5858\tVal F1 (Weighted): 0.5920\tVal F1 (Macro): 0.5402\tNo Improve Epochs: 11\n",
      "조기 종료: 11 에포크 동안 성능 개선 없음. Trial 6 종료.\n",
      "Trial 7: 계산된 클래스 가중치: [9.46429443359375, 10.485962867736816, 7.630941867828369, 5.603141784667969, 2.8589727878570557, 10.10208511352539, 24.574878692626953]\n",
      "  [Trial 7] TensorBoard 로그 디렉토리: optuna_runs\\v1\\trial_7_params_emb256_heads16_lr1.5e-05_cwTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 7] Epoch 1\tTrain Loss: 1.98876\tVal Acc: 0.3785\tVal F1 (Weighted): 0.3357\tVal F1 (Macro): 0.2192\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 2\tTrain Loss: 1.86319\tVal Acc: 0.3744\tVal F1 (Weighted): 0.3284\tVal F1 (Macro): 0.2132\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 3\tTrain Loss: 1.79846\tVal Acc: 0.3904\tVal F1 (Weighted): 0.3350\tVal F1 (Macro): 0.2163\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 4\tTrain Loss: 1.76567\tVal Acc: 0.4049\tVal F1 (Weighted): 0.3493\tVal F1 (Macro): 0.2328\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 5\tTrain Loss: 1.73068\tVal Acc: 0.4253\tVal F1 (Weighted): 0.3735\tVal F1 (Macro): 0.2625\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 6\tTrain Loss: 1.67952\tVal Acc: 0.4507\tVal F1 (Weighted): 0.4196\tVal F1 (Macro): 0.3252\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 7\tTrain Loss: 1.60946\tVal Acc: 0.4653\tVal F1 (Weighted): 0.4534\tVal F1 (Macro): 0.3750\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 8\tTrain Loss: 1.55483\tVal Acc: 0.4829\tVal F1 (Weighted): 0.4800\tVal F1 (Macro): 0.4076\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 9\tTrain Loss: 1.51299\tVal Acc: 0.4986\tVal F1 (Weighted): 0.4972\tVal F1 (Macro): 0.4235\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 10\tTrain Loss: 1.46924\tVal Acc: 0.5160\tVal F1 (Weighted): 0.5121\tVal F1 (Macro): 0.4429\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 11\tTrain Loss: 1.44159\tVal Acc: 0.5345\tVal F1 (Weighted): 0.5276\tVal F1 (Macro): 0.4570\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 12\tTrain Loss: 1.41491\tVal Acc: 0.5363\tVal F1 (Weighted): 0.5324\tVal F1 (Macro): 0.4662\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 13\tTrain Loss: 1.39337\tVal Acc: 0.5430\tVal F1 (Weighted): 0.5430\tVal F1 (Macro): 0.4777\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 14\tTrain Loss: 1.37765\tVal Acc: 0.5388\tVal F1 (Weighted): 0.5418\tVal F1 (Macro): 0.4790\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 15\tTrain Loss: 1.35953\tVal Acc: 0.5402\tVal F1 (Weighted): 0.5406\tVal F1 (Macro): 0.4799\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 16\tTrain Loss: 1.34624\tVal Acc: 0.5436\tVal F1 (Weighted): 0.5498\tVal F1 (Macro): 0.4886\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 17\tTrain Loss: 1.33147\tVal Acc: 0.5363\tVal F1 (Weighted): 0.5448\tVal F1 (Macro): 0.4885\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 18\tTrain Loss: 1.31667\tVal Acc: 0.5439\tVal F1 (Weighted): 0.5510\tVal F1 (Macro): 0.4931\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 19\tTrain Loss: 1.30555\tVal Acc: 0.5667\tVal F1 (Weighted): 0.5678\tVal F1 (Macro): 0.5068\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 20\tTrain Loss: 1.29400\tVal Acc: 0.5648\tVal F1 (Weighted): 0.5659\tVal F1 (Macro): 0.5052\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 21\tTrain Loss: 1.28500\tVal Acc: 0.5519\tVal F1 (Weighted): 0.5623\tVal F1 (Macro): 0.5049\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 22\tTrain Loss: 1.27696\tVal Acc: 0.5656\tVal F1 (Weighted): 0.5707\tVal F1 (Macro): 0.5125\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 23\tTrain Loss: 1.26346\tVal Acc: 0.5550\tVal F1 (Weighted): 0.5637\tVal F1 (Macro): 0.5091\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 24\tTrain Loss: 1.25689\tVal Acc: 0.5679\tVal F1 (Weighted): 0.5733\tVal F1 (Macro): 0.5167\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 25\tTrain Loss: 1.24715\tVal Acc: 0.5655\tVal F1 (Weighted): 0.5726\tVal F1 (Macro): 0.5172\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 26\tTrain Loss: 1.23933\tVal Acc: 0.5703\tVal F1 (Weighted): 0.5756\tVal F1 (Macro): 0.5192\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 27\tTrain Loss: 1.23220\tVal Acc: 0.5749\tVal F1 (Weighted): 0.5830\tVal F1 (Macro): 0.5262\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 28\tTrain Loss: 1.22710\tVal Acc: 0.5746\tVal F1 (Weighted): 0.5807\tVal F1 (Macro): 0.5245\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 29\tTrain Loss: 1.21590\tVal Acc: 0.5721\tVal F1 (Weighted): 0.5816\tVal F1 (Macro): 0.5257\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 30\tTrain Loss: 1.21044\tVal Acc: 0.5694\tVal F1 (Weighted): 0.5770\tVal F1 (Macro): 0.5224\tNo Improve Epochs: 3\n",
      "  [Trial 7] Epoch 31\tTrain Loss: 1.20887\tVal Acc: 0.5617\tVal F1 (Weighted): 0.5737\tVal F1 (Macro): 0.5209\tNo Improve Epochs: 4\n",
      "  [Trial 7] Epoch 32\tTrain Loss: 1.20154\tVal Acc: 0.5738\tVal F1 (Weighted): 0.5821\tVal F1 (Macro): 0.5268\tNo Improve Epochs: 5\n",
      "  [Trial 7] Epoch 33\tTrain Loss: 1.19699\tVal Acc: 0.5855\tVal F1 (Weighted): 0.5901\tVal F1 (Macro): 0.5345\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 34\tTrain Loss: 1.18735\tVal Acc: 0.5701\tVal F1 (Weighted): 0.5799\tVal F1 (Macro): 0.5275\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 35\tTrain Loss: 1.18246\tVal Acc: 0.5844\tVal F1 (Weighted): 0.5906\tVal F1 (Macro): 0.5348\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 36\tTrain Loss: 1.17794\tVal Acc: 0.5719\tVal F1 (Weighted): 0.5824\tVal F1 (Macro): 0.5306\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 37\tTrain Loss: 1.17306\tVal Acc: 0.5749\tVal F1 (Weighted): 0.5862\tVal F1 (Macro): 0.5324\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 38\tTrain Loss: 1.16931\tVal Acc: 0.5796\tVal F1 (Weighted): 0.5897\tVal F1 (Macro): 0.5350\tNo Improve Epochs: 3\n",
      "  [Trial 7] Epoch 39\tTrain Loss: 1.16538\tVal Acc: 0.5871\tVal F1 (Weighted): 0.5941\tVal F1 (Macro): 0.5391\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 40\tTrain Loss: 1.16004\tVal Acc: 0.5795\tVal F1 (Weighted): 0.5897\tVal F1 (Macro): 0.5360\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 41\tTrain Loss: 1.15230\tVal Acc: 0.5753\tVal F1 (Weighted): 0.5843\tVal F1 (Macro): 0.5307\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 42\tTrain Loss: 1.15104\tVal Acc: 0.5852\tVal F1 (Weighted): 0.5915\tVal F1 (Macro): 0.5390\tNo Improve Epochs: 3\n",
      "  [Trial 7] Epoch 43\tTrain Loss: 1.14774\tVal Acc: 0.5887\tVal F1 (Weighted): 0.5967\tVal F1 (Macro): 0.5418\tNo Improve Epochs: 0\n",
      "  [Trial 7] Epoch 44\tTrain Loss: 1.14203\tVal Acc: 0.5826\tVal F1 (Weighted): 0.5910\tVal F1 (Macro): 0.5365\tNo Improve Epochs: 1\n",
      "  [Trial 7] Epoch 45\tTrain Loss: 1.13860\tVal Acc: 0.5857\tVal F1 (Weighted): 0.5934\tVal F1 (Macro): 0.5393\tNo Improve Epochs: 2\n",
      "  [Trial 7] Epoch 46\tTrain Loss: 1.13642\tVal Acc: 0.5876\tVal F1 (Weighted): 0.5923\tVal F1 (Macro): 0.5391\tNo Improve Epochs: 3\n",
      "  [Trial 7] Epoch 47\tTrain Loss: 1.13105\tVal Acc: 0.5800\tVal F1 (Weighted): 0.5909\tVal F1 (Macro): 0.5387\tNo Improve Epochs: 4\n",
      "  [Trial 7] Epoch 48\tTrain Loss: 1.12732\tVal Acc: 0.5793\tVal F1 (Weighted): 0.5890\tVal F1 (Macro): 0.5402\tNo Improve Epochs: 5\n",
      "  [Trial 7] Epoch 49\tTrain Loss: 1.12398\tVal Acc: 0.5749\tVal F1 (Weighted): 0.5881\tVal F1 (Macro): 0.5367\tNo Improve Epochs: 6\n",
      "  [Trial 7] Epoch 50\tTrain Loss: 1.11687\tVal Acc: 0.5752\tVal F1 (Weighted): 0.5853\tVal F1 (Macro): 0.5374\tNo Improve Epochs: 7\n",
      "  [Trial 7] Epoch 51\tTrain Loss: 1.11528\tVal Acc: 0.5827\tVal F1 (Weighted): 0.5913\tVal F1 (Macro): 0.5410\tNo Improve Epochs: 8\n",
      "  [Trial 7] Epoch 52\tTrain Loss: 1.11075\tVal Acc: 0.5856\tVal F1 (Weighted): 0.5942\tVal F1 (Macro): 0.5408\tNo Improve Epochs: 9\n",
      "  [Trial 7] Epoch 53\tTrain Loss: 1.10669\tVal Acc: 0.5890\tVal F1 (Weighted): 0.5960\tVal F1 (Macro): 0.5464\tNo Improve Epochs: 10\n",
      "  [Trial 7] Epoch 54\tTrain Loss: 1.10534\tVal Acc: 0.5849\tVal F1 (Weighted): 0.5941\tVal F1 (Macro): 0.5429\tNo Improve Epochs: 11\n",
      "  [Trial 7] Epoch 55\tTrain Loss: 1.10165\tVal Acc: 0.5864\tVal F1 (Weighted): 0.5945\tVal F1 (Macro): 0.5424\tNo Improve Epochs: 12\n",
      "  [Trial 7] Epoch 56\tTrain Loss: 1.09878\tVal Acc: 0.5817\tVal F1 (Weighted): 0.5911\tVal F1 (Macro): 0.5401\tNo Improve Epochs: 13\n",
      "  [Trial 7] Epoch 57\tTrain Loss: 1.09585\tVal Acc: 0.5872\tVal F1 (Weighted): 0.5946\tVal F1 (Macro): 0.5440\tNo Improve Epochs: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 13:43:38,870] Trial 7 finished with value: 0.596743472167049 and parameters: {'embedding_dim': 256, 'hidden_dim': 512, 'n_layers': 3, 'n_heads': 16, 'dropout_p': 0.30000000000000004, 'learning_rate': 1.5271376674716724e-05, 'warmup_epochs': 7, 'patience': 15, 'use_class_weights': True}. Best is trial 3 with value: 0.6471016727207676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 7] Epoch 58\tTrain Loss: 1.09147\tVal Acc: 0.5793\tVal F1 (Weighted): 0.5919\tVal F1 (Macro): 0.5425\tNo Improve Epochs: 15\n",
      "조기 종료: 15 에포크 동안 성능 개선 없음. Trial 7 종료.\n",
      "Trial 8: 클래스 가중치 미사용.\n",
      "  [Trial 8] TensorBoard 로그 디렉토리: optuna_runs\\v1\\trial_8_params_emb256_heads8_lr4.6e-05_cwFalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 8] Epoch 1\tTrain Loss: 1.71969\tVal Acc: 0.4476\tVal F1 (Weighted): 0.3137\tVal F1 (Macro): 0.1599\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 2\tTrain Loss: 1.59096\tVal Acc: 0.4520\tVal F1 (Weighted): 0.3295\tVal F1 (Macro): 0.1888\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 3\tTrain Loss: 1.52462\tVal Acc: 0.4614\tVal F1 (Weighted): 0.3334\tVal F1 (Macro): 0.1893\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 4\tTrain Loss: 1.44463\tVal Acc: 0.5169\tVal F1 (Weighted): 0.4416\tVal F1 (Macro): 0.3295\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 5\tTrain Loss: 1.36945\tVal Acc: 0.5460\tVal F1 (Weighted): 0.4961\tVal F1 (Macro): 0.4056\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 6\tTrain Loss: 1.32069\tVal Acc: 0.5540\tVal F1 (Weighted): 0.5051\tVal F1 (Macro): 0.4125\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 7\tTrain Loss: 1.28187\tVal Acc: 0.5748\tVal F1 (Weighted): 0.5364\tVal F1 (Macro): 0.4524\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 8\tTrain Loss: 1.25690\tVal Acc: 0.5812\tVal F1 (Weighted): 0.5448\tVal F1 (Macro): 0.4583\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 9\tTrain Loss: 1.23197\tVal Acc: 0.5925\tVal F1 (Weighted): 0.5565\tVal F1 (Macro): 0.4722\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 10\tTrain Loss: 1.21337\tVal Acc: 0.5946\tVal F1 (Weighted): 0.5607\tVal F1 (Macro): 0.4734\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 11\tTrain Loss: 1.19596\tVal Acc: 0.5998\tVal F1 (Weighted): 0.5695\tVal F1 (Macro): 0.4821\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 12\tTrain Loss: 1.18365\tVal Acc: 0.6060\tVal F1 (Weighted): 0.5770\tVal F1 (Macro): 0.5016\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 13\tTrain Loss: 1.17146\tVal Acc: 0.6049\tVal F1 (Weighted): 0.5804\tVal F1 (Macro): 0.5011\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 14\tTrain Loss: 1.15922\tVal Acc: 0.6125\tVal F1 (Weighted): 0.5890\tVal F1 (Macro): 0.5087\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 15\tTrain Loss: 1.14660\tVal Acc: 0.6150\tVal F1 (Weighted): 0.5885\tVal F1 (Macro): 0.5070\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 16\tTrain Loss: 1.13843\tVal Acc: 0.6123\tVal F1 (Weighted): 0.5890\tVal F1 (Macro): 0.5143\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 17\tTrain Loss: 1.12819\tVal Acc: 0.6208\tVal F1 (Weighted): 0.5982\tVal F1 (Macro): 0.5152\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 18\tTrain Loss: 1.11931\tVal Acc: 0.6219\tVal F1 (Weighted): 0.5990\tVal F1 (Macro): 0.5166\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 19\tTrain Loss: 1.11237\tVal Acc: 0.6236\tVal F1 (Weighted): 0.6054\tVal F1 (Macro): 0.5338\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 20\tTrain Loss: 1.10202\tVal Acc: 0.6229\tVal F1 (Weighted): 0.6057\tVal F1 (Macro): 0.5208\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 21\tTrain Loss: 1.09547\tVal Acc: 0.6288\tVal F1 (Weighted): 0.6056\tVal F1 (Macro): 0.5141\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 22\tTrain Loss: 1.08790\tVal Acc: 0.6274\tVal F1 (Weighted): 0.6067\tVal F1 (Macro): 0.5292\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 23\tTrain Loss: 1.08279\tVal Acc: 0.6304\tVal F1 (Weighted): 0.6148\tVal F1 (Macro): 0.5408\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 24\tTrain Loss: 1.07492\tVal Acc: 0.6321\tVal F1 (Weighted): 0.6095\tVal F1 (Macro): 0.5289\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 25\tTrain Loss: 1.06893\tVal Acc: 0.6324\tVal F1 (Weighted): 0.6143\tVal F1 (Macro): 0.5257\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 26\tTrain Loss: 1.06467\tVal Acc: 0.6385\tVal F1 (Weighted): 0.6193\tVal F1 (Macro): 0.5384\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 27\tTrain Loss: 1.05786\tVal Acc: 0.6376\tVal F1 (Weighted): 0.6223\tVal F1 (Macro): 0.5522\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 28\tTrain Loss: 1.05329\tVal Acc: 0.6375\tVal F1 (Weighted): 0.6217\tVal F1 (Macro): 0.5500\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 29\tTrain Loss: 1.04527\tVal Acc: 0.6395\tVal F1 (Weighted): 0.6207\tVal F1 (Macro): 0.5434\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 30\tTrain Loss: 1.04313\tVal Acc: 0.6369\tVal F1 (Weighted): 0.6229\tVal F1 (Macro): 0.5452\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 31\tTrain Loss: 1.03762\tVal Acc: 0.6391\tVal F1 (Weighted): 0.6254\tVal F1 (Macro): 0.5512\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 32\tTrain Loss: 1.03179\tVal Acc: 0.6423\tVal F1 (Weighted): 0.6284\tVal F1 (Macro): 0.5542\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 33\tTrain Loss: 1.02905\tVal Acc: 0.6352\tVal F1 (Weighted): 0.6291\tVal F1 (Macro): 0.5598\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 34\tTrain Loss: 1.02196\tVal Acc: 0.6396\tVal F1 (Weighted): 0.6237\tVal F1 (Macro): 0.5400\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 35\tTrain Loss: 1.01977\tVal Acc: 0.6413\tVal F1 (Weighted): 0.6269\tVal F1 (Macro): 0.5511\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 36\tTrain Loss: 1.01484\tVal Acc: 0.6406\tVal F1 (Weighted): 0.6300\tVal F1 (Macro): 0.5584\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 37\tTrain Loss: 1.01033\tVal Acc: 0.6423\tVal F1 (Weighted): 0.6303\tVal F1 (Macro): 0.5568\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 38\tTrain Loss: 1.00881\tVal Acc: 0.6453\tVal F1 (Weighted): 0.6311\tVal F1 (Macro): 0.5509\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 39\tTrain Loss: 1.00139\tVal Acc: 0.6435\tVal F1 (Weighted): 0.6324\tVal F1 (Macro): 0.5603\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 40\tTrain Loss: 0.99839\tVal Acc: 0.6437\tVal F1 (Weighted): 0.6317\tVal F1 (Macro): 0.5566\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 41\tTrain Loss: 0.99636\tVal Acc: 0.6419\tVal F1 (Weighted): 0.6334\tVal F1 (Macro): 0.5628\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 42\tTrain Loss: 0.99040\tVal Acc: 0.6465\tVal F1 (Weighted): 0.6354\tVal F1 (Macro): 0.5614\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 43\tTrain Loss: 0.98770\tVal Acc: 0.6478\tVal F1 (Weighted): 0.6363\tVal F1 (Macro): 0.5623\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 44\tTrain Loss: 0.98468\tVal Acc: 0.6436\tVal F1 (Weighted): 0.6347\tVal F1 (Macro): 0.5638\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 45\tTrain Loss: 0.98297\tVal Acc: 0.6460\tVal F1 (Weighted): 0.6323\tVal F1 (Macro): 0.5493\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 46\tTrain Loss: 0.97814\tVal Acc: 0.6440\tVal F1 (Weighted): 0.6357\tVal F1 (Macro): 0.5612\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 47\tTrain Loss: 0.97703\tVal Acc: 0.6503\tVal F1 (Weighted): 0.6341\tVal F1 (Macro): 0.5564\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 48\tTrain Loss: 0.97219\tVal Acc: 0.6470\tVal F1 (Weighted): 0.6362\tVal F1 (Macro): 0.5655\tNo Improve Epochs: 5\n",
      "  [Trial 8] Epoch 49\tTrain Loss: 0.96654\tVal Acc: 0.6501\tVal F1 (Weighted): 0.6385\tVal F1 (Macro): 0.5632\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 50\tTrain Loss: 0.96803\tVal Acc: 0.6486\tVal F1 (Weighted): 0.6350\tVal F1 (Macro): 0.5548\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 51\tTrain Loss: 0.96361\tVal Acc: 0.6468\tVal F1 (Weighted): 0.6362\tVal F1 (Macro): 0.5649\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 52\tTrain Loss: 0.95975\tVal Acc: 0.6481\tVal F1 (Weighted): 0.6399\tVal F1 (Macro): 0.5725\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 53\tTrain Loss: 0.95692\tVal Acc: 0.6467\tVal F1 (Weighted): 0.6385\tVal F1 (Macro): 0.5726\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 54\tTrain Loss: 0.95183\tVal Acc: 0.6472\tVal F1 (Weighted): 0.6397\tVal F1 (Macro): 0.5709\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 55\tTrain Loss: 0.94928\tVal Acc: 0.6497\tVal F1 (Weighted): 0.6397\tVal F1 (Macro): 0.5648\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 56\tTrain Loss: 0.94810\tVal Acc: 0.6474\tVal F1 (Weighted): 0.6376\tVal F1 (Macro): 0.5573\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 57\tTrain Loss: 0.94400\tVal Acc: 0.6532\tVal F1 (Weighted): 0.6402\tVal F1 (Macro): 0.5650\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 58\tTrain Loss: 0.94205\tVal Acc: 0.6469\tVal F1 (Weighted): 0.6393\tVal F1 (Macro): 0.5722\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 59\tTrain Loss: 0.93859\tVal Acc: 0.6527\tVal F1 (Weighted): 0.6416\tVal F1 (Macro): 0.5673\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 60\tTrain Loss: 0.93586\tVal Acc: 0.6557\tVal F1 (Weighted): 0.6421\tVal F1 (Macro): 0.5640\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 61\tTrain Loss: 0.93382\tVal Acc: 0.6526\tVal F1 (Weighted): 0.6443\tVal F1 (Macro): 0.5740\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 62\tTrain Loss: 0.93114\tVal Acc: 0.6516\tVal F1 (Weighted): 0.6439\tVal F1 (Macro): 0.5730\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 63\tTrain Loss: 0.92867\tVal Acc: 0.6538\tVal F1 (Weighted): 0.6449\tVal F1 (Macro): 0.5727\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 64\tTrain Loss: 0.92391\tVal Acc: 0.6538\tVal F1 (Weighted): 0.6452\tVal F1 (Macro): 0.5761\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 65\tTrain Loss: 0.92394\tVal Acc: 0.6555\tVal F1 (Weighted): 0.6462\tVal F1 (Macro): 0.5744\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 66\tTrain Loss: 0.91758\tVal Acc: 0.6511\tVal F1 (Weighted): 0.6434\tVal F1 (Macro): 0.5733\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 67\tTrain Loss: 0.91696\tVal Acc: 0.6512\tVal F1 (Weighted): 0.6440\tVal F1 (Macro): 0.5756\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 68\tTrain Loss: 0.91312\tVal Acc: 0.6545\tVal F1 (Weighted): 0.6464\tVal F1 (Macro): 0.5837\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 69\tTrain Loss: 0.91123\tVal Acc: 0.6469\tVal F1 (Weighted): 0.6393\tVal F1 (Macro): 0.5641\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 70\tTrain Loss: 0.90929\tVal Acc: 0.6488\tVal F1 (Weighted): 0.6454\tVal F1 (Macro): 0.5782\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 71\tTrain Loss: 0.90703\tVal Acc: 0.6547\tVal F1 (Weighted): 0.6457\tVal F1 (Macro): 0.5708\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 72\tTrain Loss: 0.90493\tVal Acc: 0.6502\tVal F1 (Weighted): 0.6434\tVal F1 (Macro): 0.5698\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 73\tTrain Loss: 0.90229\tVal Acc: 0.6552\tVal F1 (Weighted): 0.6466\tVal F1 (Macro): 0.5740\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 74\tTrain Loss: 0.89970\tVal Acc: 0.6489\tVal F1 (Weighted): 0.6461\tVal F1 (Macro): 0.5805\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 75\tTrain Loss: 0.89935\tVal Acc: 0.6555\tVal F1 (Weighted): 0.6465\tVal F1 (Macro): 0.5747\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 76\tTrain Loss: 0.89637\tVal Acc: 0.6525\tVal F1 (Weighted): 0.6452\tVal F1 (Macro): 0.5720\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 77\tTrain Loss: 0.89336\tVal Acc: 0.6538\tVal F1 (Weighted): 0.6471\tVal F1 (Macro): 0.5764\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 78\tTrain Loss: 0.89280\tVal Acc: 0.6566\tVal F1 (Weighted): 0.6492\tVal F1 (Macro): 0.5776\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 79\tTrain Loss: 0.88647\tVal Acc: 0.6520\tVal F1 (Weighted): 0.6466\tVal F1 (Macro): 0.5767\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 80\tTrain Loss: 0.88556\tVal Acc: 0.6519\tVal F1 (Weighted): 0.6484\tVal F1 (Macro): 0.5831\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 81\tTrain Loss: 0.88469\tVal Acc: 0.6525\tVal F1 (Weighted): 0.6461\tVal F1 (Macro): 0.5763\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 82\tTrain Loss: 0.88096\tVal Acc: 0.6575\tVal F1 (Weighted): 0.6494\tVal F1 (Macro): 0.5787\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 83\tTrain Loss: 0.87974\tVal Acc: 0.6485\tVal F1 (Weighted): 0.6415\tVal F1 (Macro): 0.5647\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 84\tTrain Loss: 0.87770\tVal Acc: 0.6587\tVal F1 (Weighted): 0.6487\tVal F1 (Macro): 0.5784\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 85\tTrain Loss: 0.87589\tVal Acc: 0.6581\tVal F1 (Weighted): 0.6490\tVal F1 (Macro): 0.5773\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 86\tTrain Loss: 0.87220\tVal Acc: 0.6503\tVal F1 (Weighted): 0.6448\tVal F1 (Macro): 0.5727\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 87\tTrain Loss: 0.86788\tVal Acc: 0.6542\tVal F1 (Weighted): 0.6478\tVal F1 (Macro): 0.5817\tNo Improve Epochs: 5\n",
      "  [Trial 8] Epoch 88\tTrain Loss: 0.86667\tVal Acc: 0.6555\tVal F1 (Weighted): 0.6509\tVal F1 (Macro): 0.5856\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 89\tTrain Loss: 0.86754\tVal Acc: 0.6513\tVal F1 (Weighted): 0.6462\tVal F1 (Macro): 0.5782\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 90\tTrain Loss: 0.86536\tVal Acc: 0.6543\tVal F1 (Weighted): 0.6504\tVal F1 (Macro): 0.5813\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 91\tTrain Loss: 0.86211\tVal Acc: 0.6542\tVal F1 (Weighted): 0.6491\tVal F1 (Macro): 0.5842\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 92\tTrain Loss: 0.85795\tVal Acc: 0.6520\tVal F1 (Weighted): 0.6478\tVal F1 (Macro): 0.5802\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 93\tTrain Loss: 0.85845\tVal Acc: 0.6554\tVal F1 (Weighted): 0.6480\tVal F1 (Macro): 0.5736\tNo Improve Epochs: 5\n",
      "  [Trial 8] Epoch 94\tTrain Loss: 0.85544\tVal Acc: 0.6432\tVal F1 (Weighted): 0.6417\tVal F1 (Macro): 0.5776\tNo Improve Epochs: 6\n",
      "  [Trial 8] Epoch 95\tTrain Loss: 0.84785\tVal Acc: 0.6589\tVal F1 (Weighted): 0.6526\tVal F1 (Macro): 0.5869\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 96\tTrain Loss: 0.84851\tVal Acc: 0.6599\tVal F1 (Weighted): 0.6507\tVal F1 (Macro): 0.5819\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 97\tTrain Loss: 0.84753\tVal Acc: 0.6537\tVal F1 (Weighted): 0.6510\tVal F1 (Macro): 0.5891\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 98\tTrain Loss: 0.84248\tVal Acc: 0.6576\tVal F1 (Weighted): 0.6523\tVal F1 (Macro): 0.5866\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 99\tTrain Loss: 0.84357\tVal Acc: 0.6543\tVal F1 (Weighted): 0.6477\tVal F1 (Macro): 0.5741\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 100\tTrain Loss: 0.84092\tVal Acc: 0.6576\tVal F1 (Weighted): 0.6525\tVal F1 (Macro): 0.5873\tNo Improve Epochs: 5\n",
      "  [Trial 8] Epoch 101\tTrain Loss: 0.83945\tVal Acc: 0.6526\tVal F1 (Weighted): 0.6477\tVal F1 (Macro): 0.5785\tNo Improve Epochs: 6\n",
      "  [Trial 8] Epoch 102\tTrain Loss: 0.83588\tVal Acc: 0.6593\tVal F1 (Weighted): 0.6540\tVal F1 (Macro): 0.5879\tNo Improve Epochs: 0\n",
      "  [Trial 8] Epoch 103\tTrain Loss: 0.82914\tVal Acc: 0.6523\tVal F1 (Weighted): 0.6487\tVal F1 (Macro): 0.5787\tNo Improve Epochs: 1\n",
      "  [Trial 8] Epoch 104\tTrain Loss: 0.83010\tVal Acc: 0.6576\tVal F1 (Weighted): 0.6515\tVal F1 (Macro): 0.5832\tNo Improve Epochs: 2\n",
      "  [Trial 8] Epoch 105\tTrain Loss: 0.82774\tVal Acc: 0.6574\tVal F1 (Weighted): 0.6512\tVal F1 (Macro): 0.5837\tNo Improve Epochs: 3\n",
      "  [Trial 8] Epoch 106\tTrain Loss: 0.82534\tVal Acc: 0.6551\tVal F1 (Weighted): 0.6500\tVal F1 (Macro): 0.5822\tNo Improve Epochs: 4\n",
      "  [Trial 8] Epoch 107\tTrain Loss: 0.82930\tVal Acc: 0.6513\tVal F1 (Weighted): 0.6480\tVal F1 (Macro): 0.5814\tNo Improve Epochs: 5\n",
      "  [Trial 8] Epoch 108\tTrain Loss: 0.82354\tVal Acc: 0.6490\tVal F1 (Weighted): 0.6460\tVal F1 (Macro): 0.5837\tNo Improve Epochs: 6\n",
      "  [Trial 8] Epoch 109\tTrain Loss: 0.81822\tVal Acc: 0.6542\tVal F1 (Weighted): 0.6498\tVal F1 (Macro): 0.5815\tNo Improve Epochs: 7\n",
      "  [Trial 8] Epoch 110\tTrain Loss: 0.81610\tVal Acc: 0.6557\tVal F1 (Weighted): 0.6506\tVal F1 (Macro): 0.5819\tNo Improve Epochs: 8\n",
      "  [Trial 8] Epoch 111\tTrain Loss: 0.81584\tVal Acc: 0.6532\tVal F1 (Weighted): 0.6497\tVal F1 (Macro): 0.5843\tNo Improve Epochs: 9\n",
      "  [Trial 8] Epoch 112\tTrain Loss: 0.81646\tVal Acc: 0.6535\tVal F1 (Weighted): 0.6493\tVal F1 (Macro): 0.5839\tNo Improve Epochs: 10\n",
      "  [Trial 8] Epoch 113\tTrain Loss: 0.81569\tVal Acc: 0.6538\tVal F1 (Weighted): 0.6491\tVal F1 (Macro): 0.5824\tNo Improve Epochs: 11\n",
      "  [Trial 8] Epoch 114\tTrain Loss: 0.81597\tVal Acc: 0.6542\tVal F1 (Weighted): 0.6501\tVal F1 (Macro): 0.5836\tNo Improve Epochs: 12\n",
      "  [Trial 8] Epoch 115\tTrain Loss: 0.81038\tVal Acc: 0.6552\tVal F1 (Weighted): 0.6513\tVal F1 (Macro): 0.5858\tNo Improve Epochs: 13\n",
      "  [Trial 8] Epoch 116\tTrain Loss: 0.80874\tVal Acc: 0.6526\tVal F1 (Weighted): 0.6478\tVal F1 (Macro): 0.5798\tNo Improve Epochs: 14\n",
      "  [Trial 8] Epoch 117\tTrain Loss: 0.80999\tVal Acc: 0.6519\tVal F1 (Weighted): 0.6480\tVal F1 (Macro): 0.5784\tNo Improve Epochs: 15\n",
      "  [Trial 8] Epoch 118\tTrain Loss: 0.80714\tVal Acc: 0.6516\tVal F1 (Weighted): 0.6482\tVal F1 (Macro): 0.5791\tNo Improve Epochs: 16\n",
      "  [Trial 8] Epoch 119\tTrain Loss: 0.80577\tVal Acc: 0.6552\tVal F1 (Weighted): 0.6495\tVal F1 (Macro): 0.5784\tNo Improve Epochs: 17\n",
      "  [Trial 8] Epoch 120\tTrain Loss: 0.80447\tVal Acc: 0.6512\tVal F1 (Weighted): 0.6480\tVal F1 (Macro): 0.5806\tNo Improve Epochs: 18\n",
      "  [Trial 8] Epoch 121\tTrain Loss: 0.80260\tVal Acc: 0.6526\tVal F1 (Weighted): 0.6491\tVal F1 (Macro): 0.5824\tNo Improve Epochs: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-10 16:17:30,549] Trial 8 finished with value: 0.6540150125607246 and parameters: {'embedding_dim': 256, 'hidden_dim': 128, 'n_layers': 4, 'n_heads': 8, 'dropout_p': 0.4, 'learning_rate': 4.565359362828951e-05, 'warmup_epochs': 5, 'patience': 20, 'use_class_weights': False}. Best is trial 8 with value: 0.6540150125607246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 8] Epoch 122\tTrain Loss: 0.80160\tVal Acc: 0.6530\tVal F1 (Weighted): 0.6482\tVal F1 (Macro): 0.5794\tNo Improve Epochs: 20\n",
      "조기 종료: 20 에포크 동안 성능 개선 없음. Trial 8 종료.\n",
      "Trial 9: 계산된 클래스 가중치: [9.46429443359375, 10.485962867736816, 7.630941867828369, 5.603141784667969, 2.8589727878570557, 10.10208511352539, 24.574878692626953]\n",
      "  [Trial 9] TensorBoard 로그 디렉토리: optuna_runs\\v1\\trial_9_params_emb256_heads16_lr7.4e-06_cwTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-6, 5e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Trial 9] Epoch 1\tTrain Loss: 2.08432\tVal Acc: 0.3883\tVal F1 (Weighted): 0.3175\tVal F1 (Macro): 0.1859\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 2\tTrain Loss: 2.00786\tVal Acc: 0.4032\tVal F1 (Weighted): 0.3251\tVal F1 (Macro): 0.1951\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 3\tTrain Loss: 1.93402\tVal Acc: 0.4000\tVal F1 (Weighted): 0.3307\tVal F1 (Macro): 0.2019\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 4\tTrain Loss: 1.87662\tVal Acc: 0.3889\tVal F1 (Weighted): 0.3385\tVal F1 (Macro): 0.2157\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 5\tTrain Loss: 1.83918\tVal Acc: 0.3860\tVal F1 (Weighted): 0.3394\tVal F1 (Macro): 0.2201\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 6\tTrain Loss: 1.81544\tVal Acc: 0.3949\tVal F1 (Weighted): 0.3452\tVal F1 (Macro): 0.2241\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 7\tTrain Loss: 1.79041\tVal Acc: 0.3932\tVal F1 (Weighted): 0.3508\tVal F1 (Macro): 0.2328\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 8\tTrain Loss: 1.76986\tVal Acc: 0.4000\tVal F1 (Weighted): 0.3524\tVal F1 (Macro): 0.2350\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 9\tTrain Loss: 1.74792\tVal Acc: 0.4066\tVal F1 (Weighted): 0.3709\tVal F1 (Macro): 0.2570\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 10\tTrain Loss: 1.72601\tVal Acc: 0.4063\tVal F1 (Weighted): 0.3745\tVal F1 (Macro): 0.2688\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 11\tTrain Loss: 1.70602\tVal Acc: 0.4126\tVal F1 (Weighted): 0.3840\tVal F1 (Macro): 0.2800\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 12\tTrain Loss: 1.68920\tVal Acc: 0.4314\tVal F1 (Weighted): 0.4085\tVal F1 (Macro): 0.3092\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 13\tTrain Loss: 1.66379\tVal Acc: 0.4302\tVal F1 (Weighted): 0.4093\tVal F1 (Macro): 0.3141\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 14\tTrain Loss: 1.64208\tVal Acc: 0.4460\tVal F1 (Weighted): 0.4281\tVal F1 (Macro): 0.3325\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 15\tTrain Loss: 1.62221\tVal Acc: 0.4558\tVal F1 (Weighted): 0.4436\tVal F1 (Macro): 0.3532\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 16\tTrain Loss: 1.60357\tVal Acc: 0.4676\tVal F1 (Weighted): 0.4568\tVal F1 (Macro): 0.3722\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 17\tTrain Loss: 1.58646\tVal Acc: 0.4786\tVal F1 (Weighted): 0.4717\tVal F1 (Macro): 0.3895\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 18\tTrain Loss: 1.56870\tVal Acc: 0.4761\tVal F1 (Weighted): 0.4731\tVal F1 (Macro): 0.3952\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 19\tTrain Loss: 1.55087\tVal Acc: 0.4867\tVal F1 (Weighted): 0.4836\tVal F1 (Macro): 0.4056\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 20\tTrain Loss: 1.53744\tVal Acc: 0.4858\tVal F1 (Weighted): 0.4854\tVal F1 (Macro): 0.4103\tNo Improve Epochs: 0\n",
      "  [Trial 9] Epoch 21\tTrain Loss: 1.52496\tVal Acc: 0.4902\tVal F1 (Weighted): 0.4906\tVal F1 (Macro): 0.4124\tNo Improve Epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-07-10 16:46:56,136] Trial 9 failed with parameters: {'embedding_dim': 256, 'hidden_dim': 128, 'n_layers': 4, 'n_heads': 16, 'dropout_p': 0.4, 'learning_rate': 7.411661454943252e-06, 'warmup_epochs': 8, 'patience': 15, 'use_class_weights': True} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2319572601.py\", line 54, in objective\n",
      "    val_f1_weighted = Transformer_Train(\n",
      "                      ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2164\\2591231837.py\", line 27, in Transformer_Train\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\Users\\Administrator\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-10 16:46:56,155] Trial 9 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptuna 최적화 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 최적화 실행\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# n_trials는 추가로 실행할 횟수를 의미합니다.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 만약 이미 5개의 트라이얼을 실행했고, 총 10개를 하고 싶다면 n_trials=5로 다시 호출합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 최적화가 끝난 후 Study 객체 저장\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m최적화 결과를 \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTUDY_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m에 저장합니다...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     51\u001b[39m os.makedirs(log_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     52\u001b[39m os.makedirs(os.path.dirname(save_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m val_f1_weighted = \u001b[43mTransformer_Train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_plateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m val_f1_weighted\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mTransformer_Train\u001b[39m\u001b[34m(epoch, device, train_loader, val_loader, NN, loss_function, optimizer, scheduler_plateau, warmup_epochs, log_dir, save_path, patience, trial)\u001b[39m\n\u001b[32m     25\u001b[39m predictions = NN(data, attention_mask)\n\u001b[32m     26\u001b[39m loss = loss_function(predictions, labels)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m optimizer.step()\n\u001b[32m     29\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\tensor\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Optuna Study 저장 폴더 및 파일 경로 설정\n",
    "OPTUNA_STORAGE_DIR = 'Optuna_Storage'\n",
    "STUDY_SAVE_PATH = os.path.join(OPTUNA_STORAGE_DIR, 'optuna_study.pkl')\n",
    "\n",
    "# Optuna 저장 폴더가 없다면 생성\n",
    "os.makedirs(OPTUNA_STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Study 객체를 저장할 변수 미리 선언\n",
    "study = None\n",
    "\n",
    "# 이전에 저장된 Study가 있는지 확인하고 로드\n",
    "if os.path.exists(STUDY_SAVE_PATH):\n",
    "    print(f\"이전 Study '{STUDY_SAVE_PATH}'를 로드합니다...\")\n",
    "    try:\n",
    "        study = joblib.load(STUDY_SAVE_PATH)\n",
    "        print(f\"Study 로드 완료. 현재 {len(study.trials)}개의 트라이얼 기록이 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Study 로드 중 오류 발생: {e}. 새로운 Study를 생성합니다.\")\n",
    "        # 로드 실패 시 새로운 Study 생성\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    "        )\n",
    "else:\n",
    "    print(f\"저장된 Study 파일이 없습니다. 새로운 Study를 생성합니다.\")\n",
    "    # 저장된 파일이 없다면 새로운 Study 생성\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30, interval_steps=10)\n",
    "    )\n",
    "\n",
    "print(\"Optuna 최적화 시작...\")\n",
    "# 최적화 실행\n",
    "# n_trials는 추가로 실행할 횟수를 의미합니다.\n",
    "# 만약 이미 5개의 트라이얼을 실행했고, 총 10개를 하고 싶다면 n_trials=5로 다시 호출합니다.\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb961e-a06f-4007-8406-2623fa7ac1c8",
   "metadata": {},
   "source": [
    "> tensorboard --logdir optuna_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805dd5cc-b52c-4ba3-a837-0a80e8d611f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적화 결과를 'Optuna_Storage\\optuna_study.pkl'에 저장합니다...\n",
      "Study 저장 완료.\n",
      "\n",
      "--- Optuna 최적화 결과 ---\n",
      "최적의 하이퍼파라미터 조합 (Best Trial): {'embedding_dim': 256, 'hidden_dim': 128, 'n_layers': 4, 'n_heads': 8, 'dropout_p': 0.4, 'learning_rate': 4.565359362828951e-05, 'warmup_epochs': 5, 'patience': 20, 'use_class_weights': False}\n",
      "최적의 검증 Weighted F1 (Best Value): 0.6540\n",
      "\n",
      "--- 모든 시도 결과 ---\n",
      "Trial 0: Value=0.6028, Params={'embedding_dim': 128, 'hidden_dim': 128, 'n_layers': 2, 'n_heads': 4, 'dropout_p': 0.5, 'learning_rate': 1.1933217143832069e-05, 'warmup_epochs': 11, 'patience': 11, 'use_class_weights': False}, State=1\n",
      "Trial 1: Value=0.6395, Params={'embedding_dim': 128, 'hidden_dim': 256, 'n_layers': 2, 'n_heads': 16, 'dropout_p': 0.1, 'learning_rate': 2.9579485662059165e-05, 'warmup_epochs': 10, 'patience': 20, 'use_class_weights': False}, State=1\n",
      "Trial 2: Value=0.6394, Params={'embedding_dim': 128, 'hidden_dim': 128, 'n_layers': 2, 'n_heads': 16, 'dropout_p': 0.30000000000000004, 'learning_rate': 2.024947513625317e-05, 'warmup_epochs': 14, 'patience': 16, 'use_class_weights': False}, State=1\n",
      "Trial 3: Value=0.6471, Params={'embedding_dim': 256, 'hidden_dim': 256, 'n_layers': 4, 'n_heads': 16, 'dropout_p': 0.2, 'learning_rate': 4.267030055500867e-05, 'warmup_epochs': 13, 'patience': 19, 'use_class_weights': False}, State=1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 모든 시도 결과 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(study.trials):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Value=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m, Params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, State=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# 최적화가 끝난 후 Study 객체 저장\n",
    "print(f\"최적화 결과를 '{STUDY_SAVE_PATH}'에 저장합니다...\")\n",
    "joblib.dump(study, STUDY_SAVE_PATH)\n",
    "print(\"Study 저장 완료.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Optuna 최적화 결과 ---\")\n",
    "print(f\"최적의 하이퍼파라미터 조합 (Best Trial): {study.best_trial.params}\")\n",
    "print(f\"최적의 검증 Weighted F1 (Best Value): {study.best_trial.value:.4f}\")\n",
    "\n",
    "print(\"\\n--- 모든 시도 결과 ---\")\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f\"Trial {i}: Value={trial.value:.4f}, Params={trial.params}, State={trial.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88267bc4-97ca-4231-8a71-19ae59cf235c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
